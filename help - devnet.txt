
Certification 350-901 DEVCOR
----------------------------
Developing Applications using Cisco Core Platforms and APIs v1.0 (350-901)
Developing Applications using Cisco Core Platforms and APIs v1.0 (DEVCOR 350-901) is a 120-minute exam associated with the DevNet Professional Certification. This exam tests a candidate's knowledge of software development and design including using APIs, Cisco platforms, application deployment and security, and infrastructure and automation. The course, Developing Applications using Cisco Core Platforms and APIs helps candidates to prepare for this exam.

https://learningnetwork.cisco.com/s/devcor-exam-topics
https://developer.cisco.com/certification/exam-topic-core/

Exam overview
1.0 Software Development and Design
1.1 Describe distributed applications related to the concepts of front-end, back-end, and load balancing
1.2 Evaluate an application design considering scalability and modularity
1.3 Evaluate an application design considering high-availability and resiliency (including on-premises, hybrid, and cloud)
1.4 Evaluate an application design considering latency and rate limiting
1.5 Evaluate an application design and implementation considering maintainability
1.6 Evaluate an application design and implementation considering observability
1.7 Diagnose problems with an application given logs related to an event
1.8 Evaluate choice of database types with respect to application requirements (such as relational, document, graph, columnar, and Time Series)
1.9 Explain architectural patterns (monolithic, services oriented, microservices, and event driven)
1.10 Utilize advanced version control operations with Git
1.10.a Merge a branch
1.10.b Resolve conflicts
1.10.c git reset
1.10.d git checkout
1.10.e git revert
1.11 Explain the concepts of release packaging and dependency management
1.12 Construct a sequence diagram that includes API calls

Study Material
Setting up your Linux (Ubuntu) workstation as a development environment
Setting up your Windows workstation as a development environment
Setting up your macOS workstation as a development environment
What is a Development Environment and why do you need one?
A brief introduction to Git
Intro to Python Part 1
Intro to Python Part 2
Coding 202: Parsing JSON using Python
Introduction to XML
Introduction to the Guest Shell
Chat-Ops with Webex Teams and Python
Firepower Management Center (FMC) REST API token-based authentication
Exploring the 'webexteamssdk' Webex Teams Python Library
Git 100: Basics of the git version control system
Git 101: Branching
Git 102: Using git with servers
Introduction to Webex Teams Apps
Modern Application Development
Modern Application Development
Understanding the OAuth Flow of a Webex Teams Integration
Webex Teams Security and Access: Tokens, OAuth, Scopes, and Roles


Exam overview
2.0 Using APIs
2.1 Implement robust REST API error handling for time outs and rate limits
2.2 Implement control flow of consumer code for unrecoverable REST API errors
2.3 Identify ways to optimize API usage through HTTP cache controls
2.4 Construct an application that consumes a REST API that supports pagination
2.5 Describe the steps in the OAuth2 three-legged authorization code grant flow

Study Material
What is REST? What are APIs
Intro to Coding and APIs
Getting started with REST APIs
Hands On: Use Postman to interact with REST APIs
Prime Infrastructure API 101: REST Basics
Invoke Webex REST APIs from the interactive documentation
Building Python Requests to Read and Create Webex Teams API Items
Introduction to XML
Introductory UCS Director REST API, Custom Tasks and Workflow Creation Part I
Meraki Dashboard API Using Postman
Meraki Location Scanning API Python
Run a Webex Teams Bot Locally


3.0 Cisco Platforms
3.1 Construct API requests to implement chatops with Webex Teams API
3.2 Construct API requests to create and delete objects using Firepower device management (FDM)
3.3 Construct API requests using the Meraki platform to accomplish these tasks
3.3.a Use Meraki Dashboard APIs to enable an SSID
3.3.b Use Meraki location APIs to retrieve location data
3.4 Construct API calls to retrieve data from Intersight
3.5 Construct a Python script using the UCS APIs to provision a new UCS server given a template
3.6 Construct a Python script using the Cisco DNA center APIs to retrieve and display wireless health information
3.7 Describe the capabilities of AppDynamics when instrumenting an application
3.8 Describe steps to build a custom dashboard to present data collected from Cisco APIs

Study Material
Exploring Firepower Management Center (FMC) REST APIs
Introduction to Meraki Integrations
Introduction to Cisco DNA Center Northbound APIs
Cisco DNA Center API Overview
Cisco DNA Center Northbound API Modules
What and Why of Model Driven Programmability
Introducing YANG Data Modeling for the Network


Exam overview
4.0 Application Deployment and Security
4.1 Diagnose a CI/CD pipeline failure (such as missing dependency, incompatible versions of components, and failed tests)
4.2 Integrate an application into a prebuilt CD environment leveraging Docker and Kubernetes
4.3 Describe the benefits of continuous testing and static code analysis in a CI pipeline
4.4 Utilize Docker to containerize an application
4.5 Describe the tenets of the "12-factor app"
4.6 Describe an effective logging strategy for an application
4.7 Explain data privacy concerns related to storage and transmission of data
4.8 Identify the secret storage approach relevant to a given scenario
4.9 Configure application specific SSL certificates
4.10 Implement mitigation strategies for OWASP threats (such as XSS, CSRF, and SQL injection)
4.11 Describe how end-to-end encryption principles apply to APIs

Study Material
Introducing Containers
Docker 101
Microservices overview


Exam overview
5.0 Infrastructure and Automation
5.1 Explain considerations of model-driven telemetry (including data consumption and data storage)
5.2 Utilize RESTCONF to configure a network device including interfaces, static routes, and VLANs (IOS XE only)
5.3 Construct a workflow to configure network parameters with:
5.3.a Ansible playbook
5.3.b Puppet manifest
5.4 Identify a configuration management solution to achieve technical and business requirements
5.5 Describe how to host an application on a network device (including Catalyst 9000 and Cisco IOx-enabled devices)

Study Material
Introduction to Meraki integrations
Using the Meraki Dashboard API with postman
Meraki location scanning API Python
A hands-on introduction to the Cisco Container Platform v3.1
Advanced Docker features
Building an IOx Application with Docker
Introduction to Ansible
Managing Cisco Compute with Ansible
Microservices overview


=============================================


Network programmability basics
------------------------------
https://developer.cisco.com/video/net-prog-basics/
https://github.com/CiscoDevNet/netprog_basics			(code, slides and sandboxes!)


MODULE 1 - Introduction
1	How to be a Network Engineer in a Programmable Age				16:04	DONE

MODULE 2 - Programming Fundamentals
1	Data Formats: Understanding and using JSON, XML and YAML 		21:21	DONE
2	APIs are Everywhere... but what are they? 						13:00	DONE
3	REST APIs Part 1: HTTP is for more than Web Browsing 			32:07	DONE
4	REST APIs Part 2: Making REST API Calls with Postman 			19:43	DONE
5	Python Part 1: Python Language and Script Basics 				33:17	-
6	Python Part 2: Working with Libraries and Virtual Environments 	21:45	-
7	Python Part 3: Useful Python Libraries for Network Engineers 	38:55	DONE

MODULE 3 - Network Device APIs
1	Getting the "YANG" of it with Standard Data Models				18:41	DONE
2	Goodbye SNMP <hello> NETCONF!									27:00	DONE
3	Learn to CRUD with GET, POST and DELETE using RESTCONF			22:41	DONE
4	NX-API Part 1: Get Started with APIs and Nexus					19:14	DONE
5	NX-API Part 2: Dive into the Nexus Object Model					23:31	DONE

MODULE 4 - Network Controllers
1	Introducing Cisco DNA Center Platform APIs and Programmability 	15:44	DONE
2	Got SDN? Understanding the ACI Programmability Options 			28:56	DONE
3	Network Control in the Cloud - Developing with Cisco Meraki 	16:16	DONE
4	Cisco DNA Center Platform APIs Part 1: Exploring Apps and Tools 19:39	DONE
5	Cisco DNA Center Platform APIs Part 2: Network Troubleshooting 	23:47	DONE
6	ACI Programmability Part 1: The ACI Object Model 				19:08	-
7	ACI Programmability Part 2: Using the ACI Toolkit 				29:24	-

MODULE 5 - Application Hosting and the Network
1	Cloud to Fog: Why Host Apps in the Network 						09:44	DONE
2	Linux at the Edge: Introduction to Guest Shell 					21:39	DONE
3	Python at the Edge: Super Charged Network Event Management 		23:48	DONE
4	Package, Deploy and Run Applications in the Network with IOx 	20:31	DONE

MODULE 6 - NetDevOps
1	Configuration Management and the Network 						11:28	DONE
2	Ansible Part 1: What you need to Get Started 					24:27	DONE
3	Ansible Part 2: Using Ansible for Network Configuration 		14:21	DONE
4	Ansible Part 3: Your Network As Code 							20:02	DONE


=============================================


Cisco Live Presentations
------------------------
https://developer.cisco.com/learning/lab/why-mdp/step/7

-	Device APIs: The Force Awakens - BRKSDN-1119
-	Model Driven Approach to SDN with NETCONF - BRKSDN-1903
-	YANG Data Modeling and NETFCONF: Cisco and Industry Developments - BRKNMS- 2032
-	Network programming using Yang service models with Cisco Network Services Orchestrator - DEVNET-1177
-	Introduction to Data Models and Cisco's NextGen Device Level APIs - DEVNET-1082
-	Introduction to RESTCONF - DEVNET-1081


=============================================


Functional requirements:
- Specify what the software should do.
- Can be measured in "yes" or "no" terms.
- Usually include the words "can" and "shall".
- Often derived from user stories.
- Use cases.

Non-functional requirements:
- Tell how a system should perform those actions.
- Technical requirements of the system.
- Quality attributes.
- Often use the words "must" and "should".
- Commonly measured over an interval or a range.


Modular software design enforces the following software qualities:
- Composability
- Decomposability
- Understandability
- Continuity
- Protection

Module:
- Has a distinct purpose.
- Encapsulates data and functionality.
- Is reusable.
- Has dependencies.
- Has an interface and a body.
- High cohesion and low coupling.

Module design rules:
- Direct Mapping Rule
- Few Interfaces Rule
- Small Interfaces Rule
- Explicit Interfaces Rule
- Information Hiding Rule

Microservice:
- Single functionality inside a business domain.
- Covers a bounded context inside the domain. Logical boundary between sub-domains.
- Has a distinct purpose.
- Encapsulates the underlying functionality.
- Is reusable.
- Can depend on other microservices.
- Communicates via API or events.


SOLID:
- Single Responsibility Principle (SRP)
- Open-Closed Principle (OCP)
- Liskov's Substitution Principle (LSP)
- Interface Segregation Principle (ISP)
- Dependency Inversion Principle (DIP)


Dependency injection roles:
- Service
- Client 	(uses the service)
- Interface
- Injector 	(constructs service and injects into client)

Dependency injection methods:
- Constructor injection
- Setter injection
- Interface injection

Pros DI: 													Cons DI:
- Looser coupling.							 				 - Code is more difficult to trace and navigate.
- Allows the client to be configurable. 					 - Increases complexity.
- No knowledge of implementation. 							 - Code is tightly coupled to the implementation of dependency injection pattern.
- Makes client code easier to test by mocking dependencies.  - Requires more knowledge and time to implement.
- Creates reusable code.


=============================================


Observability in Application Design:
- Logging
- Monitoring
- Documentation


Log4j log levels (DIWEF / DIWEC; compare to DINWECAE):
- OFF	-	NOTSET. The highest possible rank and is intended to turn off logging.
- FATAL	C	CRITICAL. Severe errors that cause premature termination.
- ERROR	E	Other runtime errors or unexpected conditions.
- WARN	W	There might be a problem in the application. Use of deprecated APIs, poor use of API, 'almost' errors, runtime situations that are undesirable or unexpected, but not necessarily "wrong".
- INFO	I	Normal application behavior. Interesting runtime events (startup/shutdown).
- DEBUG	D	Granular information about the execution; detailed information on the flow through the system.
- TRACE	-	Extremely detailed information on the flow of the system.
- ALL 	-	Log everything.

DINWECAE Logging levels:
0 — emergency: 		System unusable
1 — alert: 			Immediate action needed
2 — critical: 		Critical condition—default level
3 — error: 			Error condition
4 — warning: 		Warning condition
5 — notification: 	Normal but significant condition
6 — informational: 	Informational message only
7 — debugging: 		Appears during debugging only

Example Python:

	import logging
	logging.basicConfig(level=logging.DEBUG)
	logging.info('Info. Blah blah')
	logging.warning('Warning. Meh mien lepke')


Scalability dimensions:
- Functional scalability: 		Do more without disrupting existing services.
- Geographical scalability: 	Expand geographically.
- Load scalability: 			Increase load on individual components.
- Administrative scalability: 	More users or tenants.
- Generation scalability: 		Adapt to newer components.
- Heterogenous scalability: 	Adapt to different vendors.

Scalability:
- Vertical 	 (scale up/down)
- Horizontal (scale in/out)
- Hybrid

Loosely coupled applications.
Stateless vs. Stateful.

Infrastructure sharing options:
- Share nothing architecture
- Shared disk/database architecture
- Shared everything architecture


Availability 	Downtime/year:
90.0%			36.5 days		Three days/month
99.9%			8.76 hours		Three Nines
99.999%			5.26 minutes	Five Nines

Three principles to achieve high availability:
- No single points of failure (redundancy).
- Detection and handling of failures.
- Reliable failover/crossover.

Improving resilience:
- Circuit breaker
- Fallback (e.g. to default value)
- Retry loops
- Timeouts


Rate limiting: HTTP code 429: Too Many Requests


To get a feeling about latency:
- Theoretical speed of light:	 3.3 microseconds/km
- Actual speed in fiber:		 5  microseconds/km
RTT between two points on the opposite sides of the globe: 200 ms.
In practice much higher, because only fiber is considered in calculation...


SOA
---
SOA  	Service-Oriented Architecture
SOAP 	Simple Object Access Protocol
WSDL 	Web-Service Description Language
BPEL 	Business Process Execution Language.

ESB  	Enterprise Service Bus
EDA  	Event-driven Architecture
MQ 	 	Message Queue

SOA service types:
- Business
- Enterprise
- Application
- Infrastructure

Microservice service types:
- Functional
- Infrastructure

Microservice is more granular.
SOA uses a shared communication channel, while microservices communicate independently...


Event-Driven Architecture:
- Event Broker: Events arrive at (distributed) brokers, placed in a queue, and dispatched to event processors.
- Event Mediator: Events require coordination by a central mediator component.

Message queues
Publish-subscribe pattern


ECB Design Pattern
------------------
ECB  Entity Control Boundary
MVC  Model View Controller

Entity   	-  Model
Control 	-  Controller
Boundary  	-  View


=============================================


Cisco Webex Teams
-----------------
https://developer.webex.com/docs/api/getting-started.
https://developer.webex.com/login

Secure collaboration platform.
REST API.
Integrate chatbot into your workflow.
Chat room is called a space in Webex Teams.
Space has a roomId.

Bot creation:
- Provide: Name, Username, Icon, Description.
- Properties assigned to your bot: Bot ID and Bot Webex Access Token.

To respond to events register a webhook (e.g. to external ChatOps Gateway).
Registered targetUrl gets called after a specific event occurs inside Webex Teams.
Do a callback to Webex Teams REST API to get more detailed information.

Webhooks that do not respond with the HTTP status code 2xx (100 times/five minutes) are disabled. Manually re-enable.

Example webhook:
	{
	      "name": "My_Nice_Webhook",
	      "filter": "roomId=101",
	      "resource": "messages,"
	      "event": "created,"
	      "targetUrl": "https://my.apigateway/events"
	}


ChatOps Benefits:
- Centralization 	Centralized infrastructure management
- Conversation 		Conversational approach to operations
- Collaboration 	Tighter collaboration
- Transparency 		Transparent workflow

Chatbot Frameworks:
- Botkit
- Errbot
- Hubot
- Lita
- Wobot
- Robut

Chatbot APIs:
- Cisco Webex Teams
- Microsoft Teams
- Google Hangouts
- Slack
- Hipchat

Chatbots types:
- Notifier bots			Monitor specific changes and send notifications to your chat room.
- Controller bot 		Listen for phrases or keywords in your chat room and execute pre-defined tasks.
- Personal assistants 	Most sophisticated. Use advanced algorithms and natural language processing to respond.


Examples of possible call flow directions:
- ChatOps Gateway to Webex Teams REST API.
- Webex Teams ChatBot App to ChatOps Gateway.
- ChatOps Gateway to Meraki REST API.

Example of 'components' of ChatOps Gateway:
- Webex Access Token to call Webex Teams REST API.
- Python flask Web module to serve webhooks.
- Meraki API Key to call Meraki REST API.
- Python requests module to submit HTTP requests.


Webex REST API
--------------
https://developer.webex.com/docs/api/basics

# Webex Teams REST API:
# Set Authorization HTTP header to "Bearer ACCESS_TOKEN" where ACCESS_TOKEN is your Webex Access Token
#
import requests

headers = {
    "Authorization": "Bearer ACCESS_TOKEN",
    # "Content-Type": "application/json"
}

requests.get('https://api.ciscospark.com/v1/messages', headers=headers)

curl -X GET https://api.ciscospark.com/v1/messages -H "Authorization:Bearer <ACCESS_TOKEN>"



Meraki
------
https://developer.cisco.com/meraki/api/
https://developer.cisco.com/meraki 			Credentials always-on public sandbox
https://dashboard.meraki.com/				Manage your Meraki devices from the cloud


Cisco Meraki APIs:
- Dashboard API
- Location/Scanning API
- Captive Portal API


# Meraki REST API:
# Set X-Cisco-Meraki-API-Key HTTP header to the Meraki API Key string that has been generated
# via https://dashboard.meraki.com/
#
import requests
requests.get('https://api.meraki.com/api/v0', headers={'X-Cisco-Meraki-API-Key': 'MERAKI-API-KEY'})

	+++ add some examples like requests.get(url, headers=headers).json()  +++

Note: missing/invalid access token results in HTTP 404 (Not Found).


Dashboard API:
- To use the Meraki Dashboard API:
	- Enable the API
	- Generate an API key
- Only two API keys are valid at the same time.
- API hierarchy mirrors the structure of the Meraki Dashboard:
	organizations
	  networks
	  - devices
	  - SSIDs
	  - clients
-Base URI: https://api.meraki.com/api/v0


Steps to update an SSID:
- Get the API key from the Meraki Dashboard
- GET /organizations
- GET /organizations/{{organizationId}}/networks
- GET /networks/{{networkId}}/ssids
- PUT /networks/{{networkId}}/ssids



Get Network Ssid:	GET /networks/{networkId}/ssids/{number}

	curl -X GET \
	  --url 'https://api.meraki.com/api/v0/networks/networkId3/ssids/number2' \
	  -H 'X-Cisco-Meraki-API-Key: X-Cisco-Meraki-API-Key'\
	  -H 'Accept: application/json'


Update the attributes of an SSID: 	PUT /networks/{networkId}/ssids/{number}

	For example, to enable SSID:

	curl -X PUT \
	  --url 'https://api.meraki.com/api/v0/networks/networkId4/ssids/number2' \
	  -H 'X-Cisco-Meraki-API-Key: X-Cisco-Meraki-API-Key'\
	  -H 'Accept: application/json' \
	  -H 'Content-type: application/json' \
	  --data-raw '{
	  "name": "My SSID",
	  "enabled": true
	}'


Location/Scanning API:
- https://developer.cisco.com/meraki/scanning-api/#!introduction
- To use the API:
	- Enable the API:
	  Network Wide > General > Location and scanning > Scanning API enabled
	- Define a secret.
	- Copy the Validator string (generated by the Meraki Dashboard).
	- Specify a post URL for your destination server.
- Meraki cloud performs a single HTTP GET to destination server.
- Destination server must return the validator string as a response.
- The Meraki cloud will then begin performing JSON posts (using the defined secret).


UML
---
Unified Modeling Language

UML tools:
- PlantUML
- GitUML
- Lucidchart
- Smartdraw
- WebSequenceDiagrams


Example using PlantUML text language:

	@startuml

	actor writer
	participant CMS
	actor reviewer
	participant Web

	-> writer : Get topic

	activate writer
	writer -> writer : Research topic
	writer -> writer : Create draft

	writer -> CMS : Submit draft

	activate CMS
	CMS -> reviewer : Send draft

	activate reviewer
	reviewer -> reviewer : Review draft
	reviewer -> CMS : Provide feedback
	deactivate reviewer

	CMS -> writer : Send feedback
	deactivate CMS

	alt Draft not OK
	   writer -> writer : Implement feedback
	   writer -> CMS : Submit draft

	   activate CMS
	   CMS -> reviewer : Send draft

	   activate reviewer
	   reviewer -> reviewer : Review draft
	else Draft OK
	   reviewer -> CMS : Approve draft
	   deactivate reviewer
	   CMS -> writer : Send approval
	   deactivate CMS
	end

	writer -\ CMS : Publish document
	deactivate writer

	activate CMS
	      CMS -> Web : Publish document
	      activate Web
	        CMS <- Web : Document published
	      deactivate Web
	deactivate CMS

	Web -\ : Notify Subscribers

	@enduml



=============================================


Advanced REST API Integration
=============================
Reasons for using pagination
- Save resources
- Improve response times
- Improve end user experience


Some pagination Methods:
- Page-based pagination (client provides page number; optionally the page length)
  GET /devices?page=3
  GET /devices?per_page=25&page=3	(#items per page, page number)

- Offset based pagination (kind of page-based pagination)
  GET /devices?limit=25&offset=50 	(#records per page, #records to be skipped; used with SQL DB)

- Cursor-Based Pagination (keyset-based)
  beforeId cursor:
    GET /devices?beforeId=6&max=3
  afterDate cursor:
    GET /devices?afterDate=2020-03-19T11:11:11&max=3

- HATEOAS (Hypermedia Links)

- HTTP Link Header

- Metadata in HTTP Response Payload


Limitations paged-based/offset-limit:
- Adding new entries can cause confusion, known as page drift.
- Adding items. Some result items could appear multiple times on different pages.
- Deleting items. Subsequent results shift forward. When requesting next page, results might be skipped.


HATEOAS
-------
- Hypermedia Links
- Hypermedia as the Engine of Application State
- Engine of Application State: What actions are possible varies as the state of the resource varies.
- REST Client interacts with a network application whose application servers provide information
  dynamically through hypermedia.
- A REST client needs little to no prior knowledge about how to interact with an application or server
  beyond a generic understanding of hypermedia.

GET request:

	GET /accounts/12345 HTTP/1.1
	Host: bank.example.com
	Accept: application/vnd.acme.account+json
	...

Response:

	HTTP/1.1 200 OK
	Content-Type: application/vnd.acme.account+json
	Content-Length: ...

	{
	    "account": {
	        "account_number": 12345,
	        "balance": {
	            "currency": "usd",
	            "value": 100.00
	        },
	        "links": { 										<<< hypermedia links
	            "deposit": "/accounts/12345/deposit",
	            "withdraw": "/accounts/12345/withdraw",
	            "transfer": "/accounts/12345/transfer",
	            "close": "/accounts/12345/close"
	        }
	    }
	}

Response (account is overdrawn):

	HTTP/1.1 200 OK
	Content-Type: application/vnd.acme.account+json
	Content-Length: ...

	{
	    "account": {
	        "account_number": 12345,
	        "balance": {
	            "currency": "usd",
	            "value": -25.00
	        },
	        "links": { 										<<< Only one hypermedia link
	            "deposit": "/accounts/12345/deposit"
	        }
	    }
	}


Another example:

	GET http://example.com/api/item/11

	HTTP/1.1 200 OK
	<"... snip "...>
	{
	 "itemId": 11,
	 "category": "book",
	 "author": "Mai Link",
	 "links": [ 							<<< links
	    {
	      "href":"11/cart, 					<<< Hypertext REFerence / link target
	      "rel": "addCart", 				<<< RELation attribute  / relation type
	      "method": "POST"
	    },
	    {
	     "href":"11/wishlist,
	      "rel": "addWish",
	      "method": "POST"
	    }
	  ]
	}

The rel attribute specifies the relationship between the current document and the linked document.



HTTP Link Header
----------------
- RFC 5988, RFC 8288 (Web Linking).
- Response HTTP header.
- Resources on the web (presented as web links). URI.
- HTTP "Link" for navigating pages.
- "rel" parameter: Defines type of relationships (relation type) between the resources.
- Common relation values: first, last, previous, next.
- Client should use the URL provided in the Link header.
- Link can be parsed for example with Python requests utility method: requests.utils.parse_header_links

Pagination example:
	HTTP/1.1 200 OK
	<"... snip "...>

	Link: <https://api.github.com/user/repos?page=3&per_page=100>; rel="next",
	      <https://api.github.com/user/repos?page=50&per_page=100>; rel="last"

	<"... snip "...>



Metadata in HTTP Response Payload
---------------------------------
- Construct new Requests from Response Payload.
- Metadata can include:
  - Values like: current page number, #pages, #records/page, #records.
  - Hypermedia links to the first/last/previous/next record.


Example Request:

	from requests import request

	first_page = request('GET', '/devices?page=3')
	last_page_link = first_page.json()['_metadata']['links']['last']
	last_page = request('GET',last_page_link)


Example Metadata:

	{ "_metadata":
	  { "page": 3,
	    "per_page": 10,
	    "page_count": 20,
	    "record_count": 196,
	    "links": [ {"first": "/devices?page=1},
	                {"prev": "/devices?page=2"},
	                {"next": "/devices?page=4},
	                {"last": "/devices?page=20} ]
	  },
	...
	}


---
WebEx Teams API https://developer.webex.com/docs/api/basics
WebEx Teams API implements the Web Linking standard for pagination.

VS Code:
- Ctrl+j to display the terminal.
- pipenv install: to install the required packages in the virtual environment.
- pipenv shell:   to activate the virtual environment.
---


Network errors:
- Connection timeout
- Destination host unreachable
- Destination port unreachable
- Protocol error (using HTTP when HTTPS required)

API errors, HTTP response codes:
1xx		Informational
2xx		Success
3xx		Redirect
4xx		Client error
5xx		Server Error

429 	Too Many Requests; throttling

Example implementation rate-limiting:

	HTTP/1.1 200 OK
	Server: nginx
	Date: Thu, 71 Nov 2039 12:54:51 GMT
	RateLimit-Limit: 600								<<< #requests/min
	RateLimit-Observed: 4								<<< #requests issued
	RateLimit-Remaining: 596							<<< #requests remaining
	RateLimit-ResetTime: Thu, 71 Nov 2039 12:55:51 GMT	<<< back-off until (in case of violation)


REST API Unrecoverable Errors:
- 400 Bad Request
- 401 Unauthorized
- 403 Forbidden

REST API Retryable Errors:
- 405 Method Not Allowed 	(HTTP "Allow" header: lists valid methods)
- 408 Request Timeout
- 411 Length Required 		(Content-Length header missing)
- 429 Too Many Requests 	(Throttling, rate-limiting; Webex responds with HTTP "Retry-After" header, in secs)

Are 405 and 411 really retryable ?? ==> inform you how to successfully complete your request...
										adapt your curl request or change your code...


Retry-After code:

	while True:
	    response = requests.post(url)
	    if response.status_code == 429:
	        wait = response.headers.get('Retry-After', 99)
	        time.sleep(int(wait))
	    else:
	        break


Timeout and Throttling example:

	try:
	    response = requests.get(api_url, headers=headers)

	    if response:
	        # OK
	        return response

	    # Rate limiting
	    if response.status_code == 429 and tries < REQUEST_RETRIES:
	        try:
	            retry_after = int(response.headers.get('Retry-After'))
	        except Exception:
	            retry_after = 1

	        sleep(retry_after)
	        continue

	    # Throw if not OK (2xx)
	    response.raise_for_status()

	except ConnectTimeout:
		# Network timeout, retry.
	    if tries < REQUEST_RETRIES:
	        continue
	    else:
	        raise


Reasons for optimizing API performance:
- Response time
- Required bandwidth
- Processing time and resources

Optimize API calls by:
- Caching
- Compression
- Pagination

HTTP Caching:
- Client-side caching in browser
- Web proxy servers / Caching proxy server
- Server-side caching (reserve proxy, e.g. Varnish Cache; API gateway)

- Caching systems (Memcached, redis)
- CDN

HTTP mechanisms to invalidate stale cached data:
1. Time-based period (static resources)
2. Mandatory revalidation with origin server to verify that cached entry is still valid.


Cache-Control header specifies caching behaviour of HTTP responses:
- No-store: 	Disable caching
- No-cache: 	Always verify if resource is still valid
- Max-age: 		Validity time of cached copy (in secs)
- Public: 		Anyone may cache
- Private: 		Only browser/client may cache


HTTP ETag Validation
-------------------
- ETag = Entity Tag.
- HTTP header.
- Kind of a fingerprint.
- Used for web cache validation.
- Can also be used for concurrent resource modification control.
- Web server does not send a full response if the content has not changed.
- Used in conjunction with If-Match and If-None-Match HTTP request headers.

- Clients use conditional IF-Match HTTP header in PUT or DELETE (If-Match: <etag_value>).
- Server uses strong or weak validation:
	- Strong validation: Byte-by-byte comparison.
	- Weak validation: 	 Small diffs allowed (e.g. ads/banners).
						 ETag prefixed with W/ , for example, W/"09j12ALMpo73a8."

- Used for web cache validation


Cache control:
- HTTP 304 Not Modified header
- Cached copy is still up-to-date.
- Client redirection response code indicates that there is no need to retransmit the requested resources.
- Implicit redirection to a cached resource.

Concurrent resource control
- HTTP 412 Precondition Failed.
- Client error response code indicates that access to the target resource has been denied.


HTTP HEAD Method:
- Similar to GET method.
- Server only returns HTTP headers (instead of a full response).
- Used to check if resource exists.
- Used to inspect HTTP headers only.

When timing curl request, you'll see the fast response times of HEAD:
time curl -silent --output /tmp/myFile.gz  http://dev.web.local/myFile.gz
time curl --head http://dev.web.local/myFile.gz


HTTP Conditions
---------------
If-Modified-Since:
- Server sends back the requested resource, with a 200 status, only if it has been modified after the given date.
- If the request has not been modified, the response is a 304 Not Modified without any body.
- Note: Client uses the "Last-Modified" response header of a previous request, because it contains the date of last modification.


Compressing Data
----------------
- Compression may lead to errors due to:
	- Antivirus programs on client.
	- Proxies & company policies.

- Client may include the "Accept-Encoding" header. Lists supported compression algorithms.
- Server can include the "Content-Encoding" entity header. Indicates which encoding was applied.

Client side:
	Accept-Encoding: gzip, deflate
	Accept-Encoding: identity			<<< Identity function indicates no compression (default)

Server side:
	Content-Encoding: gzip

Compression algorithms:
- compress
- deflate 			<<< Most common
- gzip 	    		<<< Most common
- bzip2
- lzma
- xz


=============================================


Security
--------
PKI		Public Key Infrastructure
MTLS 	Mutual TLS

GDPR	General Data Protection Regulation
PII 	Personal Identifiable Information data protection

OCSP	Online Certificate Status Protocol
SCEP 	Simple Certificate Enrollment Protocol

FDE 	Full Disk Encryption
HSM 	Hardware Security Module

Pseudonymization:
- Anonymization
- Deidentification

RFC 8446	TLS version 1.3

TLS_RSA_WITH_AES_128_CBC_SHA:
- RSA  key exchange & authentication
- AES  confidentiality
- SHA  integrity; digital signatures.

Create a Public Key Certificate:
- Generate Private Key: 					openssl genrsa -aes192 -out PRIVATEKEY.key 4096
- Generate Certificate Signing Request: 	openssl req -new -key PRIVATEKEY.key -out CSR.csr
- Check the CSR:							cat CSR.csr; Digicert CSR Check: https://ssltools.digicert.com/checker/views/csrCheck.jsp
- Enroll CSR with CA
- Install certificate

Note: Public/private-key encryption is one-way encryption. Only the server can decrypt with private key.

Certificate Signing Request: PKCS #10

Certificate revocation check methods:
- CRL
- OCSP


Protect east-west traffic:
- TLS or MTLS between services
- Service mesh between containers
- Dedicated container firewall

Service meshes using TLS (inter-container):
- Linkerd
- Istio
Note: TCP only (ICMP and UDP are not supported).


Secret/key handling by applications:
- Directly in code
- Encrypted in code
- In environment variables
- In Database
- API key syncing services
- Cloud-based secret services (direct request)
- Cloud-based secret services accessed through an API gateway


=============================================


OWASP	Open Web Application Security Project
XXE		XML External Entities
XSS		Cross-Site Scripting
CSRF	Cross-Site Request Forgery (sometimes pronounced sea-surf)
XSRF	Cross-Site Request Forgery
SSRF	Server-Side Request Forgery


OWASP Risk Rating Methodology:

	Risk = Likelyhood x Impact

	Likelyhood is an average of:
	- Exploitability
	- Weakness Prevalence
	- Weakness Detectability


OWASP top 10 (2017)
1.  Injection
2.  Broken Authentication
3.  Sensitive Data Exposure			<== most prevalent and impactful
4.  XML External Entities (XXE)
5.  Broken Access Control
6.  Security Misconfiguration
7.  Cross-Site Scripting (XSS)		<== 2nd most prevalent
8.  Insecure Deserialization
9.  Vulnerable Components
10. Insufficient Logging & Monitoring


Mitigation SQL injection:
- Blacklist / whitelist characters (filter malicious characters).
- Fortify SQL statements (Parameterized queries or prepared statements; escape dangerous characters).
- Catch/remove SQL error messages (don't show to user).
- IPS
- Firewall (layer 7 inspection).


Python library to validate user data:
Cerberus data validator:	https://docs.python-cerberus.org/en/stable/

	>>> schema = {'name': {'type': 'string'}}
	>>> v = Validator(schema)

	>>> document = {'name': 'john doe'}
	>>> v.validate(document)
	True


Cross-site scripting (XSS):
- Injection attack.
- Typically via persistent storage.
- XSS enables attackers to inject client-side scripts into web pages viewed by other users.
- XSS exploits the trust a user has for a particular web site.

Mitigation XSS:
- Escape any untrusted input.
- Escape data to and from user.
- HTML escaping.
- Input sanitization.
- Separate untrusted data from active browser content.


Python:
- HTML.Escape to escape HTML (HTML Core library)
- Sanitize library for HTML input sanitization

	form = cgi.FieldStorage()
	comment = html.escape(form.getvalue(“COMMENT”)

	import sanitize
	sanitize.HTML('safe<meta http-equiv="Refresh" content="0; URL=/">')


Cross-Site Request Forgery (CSRF/XSRF):
- Attack on state-changing requests.
- Also known as one-click attack or session riding.
- Malicious exploit of a website where unauthorized commands are transmitted from a user (without the user's interaction or knowledge).
- CSRF exploits the trust that a web site has in a user's browser.

Mitigation CSRF:
- Use of CSRF token (unique, unpredictable value)

Python frameworks supporting CSRF tokens:
- Django
- Flask-WTF
- Flask-SeaSurf
- Pyramid
- Sanic-WTF

WTF = WTForms


OAuth
-----
RFC6749: OAuth 2.0 Authorization Framework

OAuth roles:
- Resource owner 			(typically the end user who owns a protected resource; who grants access)
- Client 					(the application)
- Authorisation server 		(verifies identity; issues autho. code and/or access token to the client)
- Resource server 			(holds the protected resource)

Authorization and resource server are often co-located.

OAuth interaction models:
- Client Credentials flow 						(two-legged)
- Authorization Code Grant flow 				(three-legged; autho. code & access token)
- Implicit flow 								(three-legged; only access token, no autho. code)
- Resource Owner Password Credentials flow

OAuth 2.0 grant types (the way an application gets an access token):
- Authorization Code grant 		(three-legged)
- Client Credentials grant 		(two-legged)
- Refresh Token grant
- Device Code grant
- Password grant (Legacy)

Two-Legged authorization flow:
- Two flows
- Client application, authorization server and a resource server
- Two-Legged authorization flow is mostly used for APIs

Three-Legged authorization flow:
- Tree flows
- Resource owner, client application, authorization server and a resource server
- Three-Legged authorization flow is mostly used for end-user authentication.

Three-Legged authorization steps:
- Retrieve authorization code.
- Exchange authorization code for an access token.
- Use the access token.

Complete OAuth sequence: refer text. +++ What kind of Oauth flow is this ? +++


Grant type used in the two-legged authorization flow: client_credentials

	POST /token HTTP/1.1
	Host: authorization-server.com
	Content-Type: application/x-www-form-urlencoded

	grant_type=client_credentials
	client_id=xxxxxxxxxx
	client_secret=yyyyyyy


	HTTP/1.1 200 OK
	Content-Type: application/json;charset=UTF-8
	Cache-Control: no-store
	Pragma: no-cache

	{
	 "access_token":"zzzzzzzzzzzzzzzzz",
 	 "token_type":"bearer",
	 "expires_in":3600,
	 ...
	 }


Revoke access token:

	POST /revoke HTTP/1.1
	Host: authorization-server.com
	Content-Type: application/x-www-form-urlencoded
	Authorization: Bearer <access_token>

	token=<access_token>


Access token:  lifetime 60 minutes
Refresh token: lifetime 60 days


Authentication (?) protocols authorization server:
- OAuth
- OpenID


=============================================


Python
------
PyPi 	Python Package Index Repository
Wheel 	A Built Distribution format (intended to replace the Egg format).


Installing Python Packages:
pip install <packageName>
pip install -r requirements.txt

pip show <packageName>
pip list

pip freeze > requirements.txt 			<<< Simple dependency management
cat requirements.txt
pip install -r requirements.txt


# Create a virtual environment
# python3 -m venv <virtualenv>
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate

# Deactivate virtual environment
deactivate


Packaging
---------
Creating packages with setuptools library. Example setup.py script:

	from setuptools import setup, find_packages
	setup(
	    name='my_package',
	    version='1.0.0',
	    packages=find_packages()
	)


# Make sure you have the latest versions of setuptools and wheel installed:
#
python3 -m pip install --user --upgrade setuptools wheel

# Generate distribution archive(s):
# - .tar.gz file is a source archive
# - .whl file is a built distribution (a wheel)
#
python3 setup.py sdist bdist_wheel


Dependency management
---------------------
Pipenv: built on top of pip and virtualenv.

Pipenv dependency manager uses two files:
- Pipfile 			(project libraries minimum requirements; populated by installs)
- Pipfile.lock 		(final snapshot of currently installed versions; after lock cmd)

To see the dependency tree and resolve the conflicts:
 pipenv graph

Pipenv also enhances security by using verification hashes.


Python Package Versioning
-------------------------
- Ensure compliancy with public version scheme in PEP 440 (https://www.python.org/dev/peps/pep-0440).
- Serial versioning, YEAR.SERIAL (for example 20.01, 19.88).
- Date based versioning: YEAR.MONTH (for example, 20.04, 19.12).
- Preferred one is the semantic versioning 2.0.0.

Semantic Versioning:
- MAJOR.MINOR.PATCH
- MAJOR version when you make incompatible API changes.
- MINOR version when you add functionality in a backwards compatible manner.
- PATCH version when you make backwards compatible bug fixes.
- Option to append additional identification strings that define non-final releases (dev, beta, release candidate).
  For example: 2.3.0dev1, 2.2.0_b1, 2.2.0.rc1

Changelog:
- Added 		New features
- Changed 		Changed functionalities
- Deprecated    Soon-to be removed features
- Removed 		Removed features
- Fixed 		Bug fixes
- Security 		Vulnerability fixes



Git
---
https://linuxacademy.com/blog/linux/git-terms-explained

VCS	Version Control Software

Main git concepts:
- Remote repository
- Local repository
- Staging area
- Working directory


Origin: The original remote repository (default name).

Master: Is a local branch (default branch that git creates when first creating a repository).

Origin/master: Is a remote branch (it's a local copy of the "master" branch on the remote "origin").

	# Since origin/master is a branch, you can merge it. Here's a pull in two steps:
	#
	# Fetch master from the remote origin. The master branch on origin will be fetched and the
	# local copy will be named origin/master.
	#
	git fetch origin master

	# Then you merge origin/master into master.
	#
	git merge origin/master

	# Then you can push your new changes in master back to origin:
	#
	git push origin master


Master: The primary branch of all repositories. All committed and accepted changes should be on the master branch. You can work directly from the master branch, or create other branches.

Cherry-picking a commit:
- Taking an older commit, and rerunning it at a defined location.
- Git copies the changes from the original commit, and then adds them to the current location.

HEAD: This is a reference variable used to denote the most current commit of the repository in which you are working. When you add a new commit, HEAD will then become that new commit.

Detached HEAD: HEAD pointing to a commit rather than branch (detached state).

Nice to now:
- Git commit --amend can be used only with the last commit.
- Git revert does not rewrite the Git history. It adds a new commit...
- Git checkout can be used to copy a desired version of a file into the working directory.


# Initialize a git repository (creates .git directory)
git init

# Cloning a remote repository
git clone <url> <local dir>
git clone <url> .


# Get the status (local working directory, staging area, etc.)
git status

# Check diffs with staging area
git diff
git diff --staged

# Add a file to staging area
git add <file>
git add .

# Remove a file from staging area
git reset <file>

# Remove all file from staging area
git reset

# Commit files in staging area
git commit -m "My first commit"

# Combine: add to staging area & commmit
git commit -am "Caching"

# Push changes
git push


# History log of commits
git log
git log --oneline


# Create a local branch called my_test_branch
git branch my_test_branch

# List the branches
git branch
git branch -all

# Jump to branch (git put files in your working directory).
# Switch branches.
#
git checkout my_test_branch


# Merge my_test_branch with master
git checkout master
git merge my_test_branch


# If you don't want to merge and prefer one file over the other, choose
# one of the versions: either "ours" (HEAD) or "theirs" (branch).
#
git checkout --ours my_network.py

# Abort the merge, for example in case of too many conflicts.
#
git merge --abort


# Stash away unfinished branch
git stash push -m "Store temporary"

# Restore unfinished branch
git stash pop


# Other git 'merge' options:
- git rebase <dev branch> 				(Note: This forges history)
- git cherry-pick <commit-hash>


# Revert a specific commit.
# Does not discard any commits that came after that one.
# It creates a new revision that reverts the effects of a specified commit.
#
git revert <hash>


# Reset
git reset --hard 	(working directory and staging area are overwritten/synchronized)
git reset --mixed	(preserves staging area)
git reset --soft 	(preserves staging area and working directory)


# View information about remote repository
git remote -v

git remote add origin https://github.com/Tisipi/Help-Files.git

+++
git pull: combines git fetch and git merge
git push -u <remote> <branch name> com
+++


# Check the content of a git object
#
git cat-file -p master
git cat-file -p <SHA1>

# Check the type of a git object
#
git cat-file -t <SHA1>


# List files in git working directory
#
git ls-files --modified

# List files in git staging area
#
git ls-files --stage



Branching
---------
Two opposite collaboration approaches:
1. Centralized workflow (small projects, document oriented projects e.g. infrastructure as code).
2. Feature branch workflow (separate dedicated branch for each feature).

GitFlow (Workflow) is a branching strategy that covers CI/CD operations:
- Master branch 	Only Release and Hotfix/maintenance branches can be merged into master.
- Develop branch 	Derived from the Master branch; integration branch for all features.
					NOT merged into Master.
- Feature branches 	Created from Develop; never allowed to interact directly with the Master branch.
					Merged back into Develop.
- Release branch 	Created from Develop. Used to integrate all the features
					since the last release, and fix any arising bugs.
					Merged back into Develop and Master.
- Hotfix branch 	Hotfix/maintenance branch is created from the Master.    <<< !!
					Merged back into Develop and Master.

Master branch
	Develop branch
		Feature branch
		Release branch
	Hotfix/maintenance branch



CI/CD Pipelines
---------------
CI 	Continuous Integration
CD 	Continuous Delivery		(manual deploy)
CD	Continuous Deployment 	(automatic deploy)

Automated (Unit) Testing

Static Code Analysis:
- Lint/linter
- Style checker (PEP8)
- Code complexity, unused code
- Security issues (OWASP, credential leaking)

Python packages
- Potential bugs and common errors:		prospector, pyflakes
- Code complexity:				 		mccabe
- Unused code:				 			vulture
- Security vulnerabilities:				bandit
- Leaking credentials:				 	dodgy
- Incorrect type usage:				 	mypy
- Code style inconsistencies:			pylint, pycodestyle


Elements of CI/CD system:
- Pipeline. Has multiple stages.
- Stages. Consists of jobs running in parallel.
- Jobs. If a job fails, the entire stage is considered failed.


Example, pipeline containerized application:
- Build: Create application image.
- Test: Run tests against the image.
- Push image to registry.
- Deploy image on server.

Security testing:
- SAST. Static Application Security Testing. Source/compiled code analysis.
- DAST. Dynamic Application Security Testing. Vulnerability scanning of application.


Examples of CI/CD pipeline services:
- Tools on GitHub, Gitlab and Bitbucket.
- Jenkins, Travis CI, and Semaphore.


Common pipeline failures:
- Pipeline configuration (file)
- Missing dependencies
- Static code analysis errors
- Incompatible versions
- Unit and integration tests
- Deployment failures


GitLab pipeline consists for example of:
- Commit: 	A change in the code.
- Pipeline: A group of jobs divided into different stages.
- Stages
- Job: 		Runner instructions.
- Runner

GitLab CI pipeline file: .gitlab-ci.yml


When a test fails in the Test stage of a Build/Test CI pipeline, will it then restart with the first job in the Build stage (after fix)? Is this specific for Build/Test only??


=============================================


Deploying Applications
----------------------

12-Factor App
-------------
Adam Wiggins, 2012, Heroku; https://12factor.net

The Twelve Factors:
 1. Codebase				One codebase tracked in revision control, many deploys
 2. Dependencies			Explicitly declare and isolate dependencies
 3. Config					Store configuration in the environment
 4. Backing services		Treat backing services as attached resources
 5. Build, release, run		Strictly separate build and run stages
 6. Processes				Execute the app as one or more stateless processes
 7. Port binding			Export services via port binding
 8. Concurrency				Scale out via the process model
 9. Disposability			Maximize robustness with fast startup and graceful shutdown
10. Dev/prod parity			Keep development, staging, and production as similar as possible
11. Logs					Treat logs as event streams
12. Admin processes			Run admin/management tasks as one-off processes


Version control system (VCS): Git, Subversion (SVN), Mercurial

Package managers:
- NuGet (.NET). Dependency manifest file in json.
- Maven or Gradle (Java).
- npm package manager (Node Javascript). Json dependency declaration file (npm install).
- Pip (Python). Dependency manifest text file (pip install -r requirements.txt).

Examples backing services:
- Data stores (Mysql, PostgreSQL)
- Messaging systems
- SMTP services (Postfix)
- Caching systems (Memcached, redis)
- External authentication providers (Auth0).

Example Web Servers:
- Apache, NGINX (open source)
- Tomcat and JBoss (Java)
- Microsoft Internet Information Server (IIS)  (.NET)

A 12-factor app should not rely on runtime injection of web servers. A self-contained application declares web server libraries as dependencies, e.g.:
- Tornado (Python)
- Jetty (Java)
Export services via port binding.

Beanstalkd: background processing queueing software.

Adapters: Abstraction to different backing services. e.g. databases (SQLite, PostgreSQL) in dev, staging and prod. Example adapter: ActiveRecord.
Con: Might cause problems, e.g. tiny incompatibilities between dev and prod.

Read Eval Print Loop (REPL) shell.


Hypervisors
-----------
- Type-1, native or bare-metal hypervisors. Run directly on the host's hardware.
  Examples: Xen, Oracle VM Server, Microsoft Hyper-V, VMware ESXi.
- Type-2 or hosted hypervisors. Run on OS. Runs as a process on the host.
  Examples: VMware Workstation/Player, VirtualBox, Parallels Desktop for Mac, QEMU.

The distinction between these two types is not always clear.


Docker
------
Apache 2.0 open-source license.

Docker editions:
- Engine community
- Engine enterprise
- Enterprise with full management support

Linux xommand "ps aux" lists all processes (incl. docker containers)

Docker client-server architecture:
- Client using CLI
- Server, Docker daemon

Docker also supports a REST API
Docker daemon manages containers, images, networks and volumes.

Docker registry:
- Public: Docker Hub (https://hub.docker.com),
- Self-hosted: Harbor or GitLab Container Registry

Docker uses Union File System (unionFS) to layer Docker images.
- unionFS allows files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system.
- Layers or intermediate images.
- Image can start with a blank layer called scratch (base image).
- Image can take an existing parent image as base.


Dockerfile example:

	FROM 	ubuntu:18.04
	COPY 	. /app 					<<< Copies from host to container location.
	RUN 	make /app
	CMD 	python /app/app.py


Dockerfile example:

	FROM 	python:3.7
	ENV 	MESSAGE HELLO-WORLD

	COPY 	. /app
	WORKDIR /app

	RUN 	pip install -r requirements.txt
	EXPOSE 	7777

	ENTRYPOINT	["python3"]			<< This is the command
	CMD 		["myApp.py"]		<< This is the argument


Dockerfile example:

	FROM 	python:3.7
	ENV 	MESSAGE HELLO-WORLD

	COPY 	. /app
	WORKDIR /app

	RUN 	pip install -r requirements.txt
	EXPOSE 	7777

	CMD 	python3 myApp.py


FROM 		Creates a layer from the ubuntu:18.04 Docker image.
COPY 		Adds files from your Docker client’s current directory.
RUN 		Builds your application with make.
CMD 		Specifies what command to run within the container.

WORKDIR		Changes working directory for any consecutive commands.
EXPOSE 		Exposed port (only for documentation; must use --publish)
ENV 		Environment variables (key value)
ENTRYPOINT

If you don't use RUN either ENTRYPOINT or CMD must be defined.
Two modes: shell and exec.
Exec format: Arguments are inside square brackets.

Exec mode is preferred. It starts the specified command process as PID 1. Ensures that Unix signals sent to the container are sent to your spawned process.



# Build an image from the dockerfile
#
docker build -t <image-tag> .
docker build -t <image-tag> -f <path to Dockerfile>

# Push image to registry
#
docker push <image-tag>



# Create and run a container (fetches image automatically, if needed)
#
# docker run -dit --name <name container> -p <host port>:<container port> <image:version>
#
# 	-d (--detach)		detached mode (in background)
# 	-i (--interactive)	interactive session
# 	-t (--tty)			standard input terminal
# 	-p (--publish) 		Publish a port outside the container
#	--name 				Optional name of container
#

# Run busybox:
#
docker run busybox echo hello, world
docker run busybox pwd
docker run busybox ping 8.8.8.8

# Run Apache httpd web server
#
docker run -dit -p 8080:80 httpd
docker run -dit -p 8080:80 httpd:2.4

# Run Apache httpd web server
#
docker run -dit --name MyApacheWeb -p 8080:80 httpd:2.4


# Instead of running Apache httpd web server
# jump into the bash shell (for example for troubleshooting).
#
# You now need to start the container manually from the bash shell...
# Can be used to run one-off containers with diagnostic or maintenance commands...
# Containers stops when you exit bash shell...
#
docker run -dit --name MyApacheWeb -p 8080:80 httpd:2.4 bash


# Access the bash shell after starting the container.
#
docker run -dit --name MyApacheWeb -p 8080:80 httpd:2.4 bash
docker exec -it <containerid> bash



# List running containers:
#
docker ps
docker container ls

# List all containers that have run:
#
docker ps --all
docker container ls -a

# Check logs of container
#
docker log <container id>

# Remove stopped containers, docker cache, etc.:
#
docker system prune



# Fetch image explicitly from registry
#
# docker pull [OPTIONS] NAME[:TAG|@DIGEST]
#
docker pull debian
docker pull debian:jessie


# Check which images are present locally
#
docker images
docker image ls


#  Create a container without starting it
#
docker create hello-world

# Create and start a container.
#
docker create hello-world
docker start -a <container reference>

# Restart a stopped container.
# First three characters of id suffice.
#
docker start <container id>

# Restart a stopped container; attach to STDOUT/STDERR.
#
docker start -a <container id>


# Stop a running container
#
docker stop <container id>
docker kill <container id>


# Help
#
docker --help.


# New docker syntax available:
# docker container run
# docker image build



Docker networking
-----------------
Network drivers:
- Bridge (default)
- Host
- Overlay
- Macvlan
- None

Built-in DNS resolution:
- Default bridge network: Container can only access other container by IP addres.
- Custom bridge network: Automatic DNS resolution between containers.

Containers connected to default bridge network, can not speak to external hosts.
To enable forwarding:
1. Allow IP forwarding on host:
   	sysctl net.ipv4.conf.all.forwarding=1
2. Update iptables FORWARD policy to ACCEPT traffic:
   	iptables -P FORWARD ACCEPT



# List Docker networks
#
docker network ls


# Create a container network
#
# docker network create [OPTIONS] NETWORK
#
docker network create my-bridge-network
docker network create -d bridge my-bridge-network

docker network create --driver=bridge --subnet=192.168.0.0/16 br0

docker network create \
  --driver=bridge \
  --subnet=172.28.0.0/16 \
  --ip-range=172.28.5.0/24 \
  --gateway=172.28.5.254 \
  br0


docker network create -d overlay my-multihost-network

docker network create -d overlay \
  --subnet=192.168.10.0/25 \
  --subnet=192.168.20.0/25 \
  --gateway=192.168.10.100 \
  --gateway=192.168.20.100 \
  --aux-address="my-router=192.168.10.5" --aux-address="my-switch=192.168.10.6" \
  --aux-address="my-printer=192.168.20.5" --aux-address="my-nas=192.168.20.6" \
  my-multihost-network


# Attach container to a network upon creation
#
docker run -it -p 5000:5000 --network multi-host-network container1


# Connect a container to a network
#
# docker network connect [OPTIONS] NETWORK CONTAINER
# docker network connect [OPTIONS] <net-id> <container-id>
#
docker network connect multi-host-network container1
docker network connect --ip 10.10.36.122 multi-host-network container2


# Disconnect a container from a network
#
# docker network disconnect [OPTIONS] NETWORK CONTAINER
# docker network disconnect [OPTIONS] <net-id> <container-id>
#
docker network disconnect multi-host-network container1


Persistent Data
---------------
Restarting a container: Filesystem data is not lost
Remove a container:		Filesystem data is lost

Persist data with:
- Volume: 		Location outside of container union FS: /var/lib/docker/volumes
				Not accessible to non-Docker processes.
- Bind mount:	Any location on host. Accessible to non-Docker processes.
- tmpfs mount:	System memory on host.

Volumes are the best way.
A volume can be mounted into multiple containers simultaneously.


# List the volumes
#
docker volume ls

# Inspect a volume
#
docker volume inspect <volume name>


# Create a volume
#
# docker volume create [OPTIONS] [VOLUME]
#
docker volume create <name volume>


# Create a volume when starting the container
#
# volume:						my-volume
# maps host local path 			/var/lib/docker/volumes/my-volume/
# into the container at 	 	/var/lib/app-data
# container:					my-app
#
docker run -d -v my-volume:/var/lib/app-data my-app


# Bind mount can only be specified with the docker run command.
# The path always starts with the forward slash (/)
#
docker run -d -v /home/cisco/app-folder:/var/lib/app-data my-app


Docker Exercise
---------------
app/main.py:

	from flask import Flask

	app = Flask(__name__)

	@app.route('/')
	def home():
	    out = (
	        f'Welcome My Friend.<br>'
	    )
	    return out

	if __name__ == '__main__':
	    app.run(debug=True, host='0.0.0.0')


Access the application:
	http://localhost:5000


app/requirements.txt:

 Flask==1.1.1


app/Dockerfile:

	FROM python:3.7

	COPY . /app
	WORKDIR /app

	RUN pip install -r requirements.txt
	EXPOSE 5000

	CMD ["python3", "main.py"]

Build the container:
	docker build -t app .

Start the container:
	docker run -it -p 5000:5000 app


lb/nginx.conf:

	events {}
	http {

	  upstream myapp {
	    server 172.20.0.100:5000;
	    server 172.20.0.101:5000;
	  }

	  server {
	    listen 8080;
	    server_name localhost;

	    location / {
	      proxy_pass http://myapp;
	      proxy_set_header Host $host;
	    }
	  }

	}


lb/Dockerfile:

	FROM nginx

	COPY nginx.conf /etc/nginx/nginx.conf

	EXPOSE 8080

	CMD ["nginx", "-g", "daemon off;"]


Build the container:
	docker build -t lb .


app/main.py:

	from flask import Flask
	import socket

	ip = socket.gethostbyname(socket.gethostname())

	app = Flask(__name__)

	@app.route('/')
	def home():
	    out = (
	        f'Welcome My Dear Friends.<br>'
	        f'IP address of the server is {ip}.<br><br>'
	    )
	    return out

	if __name__ == '__main__':
	    app.run(debug=True, host='0.0.0.0')


Create and inspect docker network appnet:
	docker network create --subnet=172.20.0.0/24 --gateway=172.20.0.1 appnet
	docker network inspect appnet

Run the app container on the appnet network with .100 IP address:
	docker run --net appnet --ip 172.20.0.100 -it -d -p 5000:5000 app

Start the lb container on the appnet network with .10 IP address:
	docker run --net appnet --ip 172.20.0.10 -it -d -p 8080:8080 lb

Access the application via the LB:
	http://localhost:8080


+++ #TODO: 8.4 Containerize Application Using Docker, LAB  +++


Kubernetes
----------
https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/
https://kubernetes.io/docs/tutorials/
https://kubernetes.io/docs/reference/kubectl/cheatsheet/

K8s: K + 8 characters + s ("kates", "keights")

Kubernetes supports Docker, containerd, CRI-O, rktlet, and any implementation of the Kubernetes Container Runtime Interface (CRI).
(LXC Linux Containers, rkt)

Kubernetes:
Open-source system for automating deployment, scaling, and management of containerized applications.
It groups containers that make up an application into logical units for easy management and discovery.

Kubernetes features:
- Service discovery and load balancing
- Storage orchestration
- Automated rollouts and rollbacks
- Automatic bin packing
- Self-healing
- Secret and configuration management

- EndpointSlices
- Service Topology
- Batch execution
- IPv4/IPv6 dual-stack
- Horizontal scaling


Pods:
- Smallest deployment units on K8s.
- Typically one container per pod.
- If multiple containers per pod, then assigned to the same worker node.
- When you scale on K8s, you scale the pods.
- Replicated pods (traffic is load balanced between them).
- Each Pod gets its own IP address. Address is internal to the cluster.


Service object expose the application to other pods (internally) or to the outside world.
Types of Service objects:
- ClusterIP 	(internal; default service)
- NodePort  	(external; opens a specific IP address and a port on all the Nodes (VMs); traffic sent to this port is forwarded to the service)
- LoadBalancer 	(external; external --> LB --> NodePort --?--> ClusterIP)
- ExternalName 	(external; maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record)


ClusterIP service:

	apiVersion: v1
	kind: Service 						<<<<
	metadata:
	  name: my-internal-service
	spec:
	  selector:
	    app: my-app
	  type: ClusterIP 					<<<<
	  ports:
	  - name: http
	    port: 80
	    targetPort: 80
	    protocol: TCP

	Acces to ClusterIP Service via Kubernetes proxy:
	  http://localhost:8080/api/v1/proxy/namespaces/<NAMESPACE>/services/<SERVICE-NAME>:<PORT-NAME>/
	  http://localhost:8080/api/v1/proxy/namespaces/default/services/my-internal-service:http/


NodePort Service:

	apiVersion: v1
	kind: Service 						<<<<
	metadata:
	  name: my-nodeport-service
	spec:
	  selector:
	    app: my-app
	  type: NodePort 					<<<<
	  ports:
	  - name: http
	    port: 80
	    targetPort: 80
	    nodePort: 30036 				<<<<
	    protocol: TCP


LoadBalancer Service:

	apiVersion: v1
	kind: Service 						<<<<
	metadata:
	  name: my-service
	spec:
	  type: LoadBalancer 				<<<<
	  selector:
	    app: MyApp
	  ports:
	    - port: 8765
	      targetPort: 9376


	apiVersion: v1
	kind: Service 						<<<<
	metadata:
	  name: my-service
	spec:
	  type: LoadBalancer 				<<<<
	  selector:
	    app: MyApp
	  ports:
	    - protocol: TCP
	      port: 80
	      targetPort: 9376
	  clusterIP: 10.0.171.239
	status:
	  loadBalancer:
	    ingress:
	    - ip: 192.0.2.127



Ingress. Is actually NOT a type of service. Kind of HTTP Load Balancer

	apiVersion: extensions/v1beta1
	kind: Ingress 						<<<<
	metadata:
	  name: my-ingress
	spec:
	  backend:
	    serviceName: other
	    servicePort: 8080
	  rules:
	  - host: foo.mydomain.com
	    http:
	      paths:
	      - backend:
	          serviceName: foo
	          servicePort: 8080
	  - host: mydomain.com
	    http:
	      paths:
	      - path: /bar/*
	        backend:
	          serviceName: bar
	          servicePort: 8080



---

Two types of nodes:
- Master node(s) (control plane)
- Worker nodes

Master components:
- API server (kube-apiserver)
- etcd
- Scheduler (kube-scheduler)
- Controller Manager (kube-controller-manager)
- Cloud-controller-manager (NEW)

Worker components:
- Kubelet
- Kube-proxy
- Container Runtime

Addons:
- DNS (CoreDNS)
- Web UI (Dashboard)
- Container Resource Monitoring
- Cluster-level Logging


Ways to create K8s cluster:
- Minikube tool
- Cisco Container Platform
- Google Kubernetes Engine (GKE)
- Amazon Elastic Kubernetes Service (EKS)

Kubectl CLI client.

Deployment Descriptor
Deployment Controller
ReplicaSet Controller

Note: ReplicationController is getting deprecated and replaced by the ReplicaSet.

Type of kubectl commands:
- Imperative.  Using kubectl create, run, scale and expose.
- Declarative. Using the kubectl apply command.


kubectl run my-example --image=gcr.io/devnet-756111/my-flask-example:latest --port=4200 --replicas=3


(kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml)
kubectl get deployments

kubectl get pods
kubectl get rs


# Expose deployed application using public IP address of a load balancer
#
kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

# get the state of the service
#
kubectl get service

# Get more information about the deployment
#
kubectl describe pod devnet-example-6af1d76bdb-5knbx



# Export a YAML descriptor of a running pod:
#
kubectl get pod <pod> -o yaml

# Output a json formatted pod object
#
kubectl get pod <pod> -o json



# Deploy YAML manifest for pod
#
kubectl create -f devnet-pod.yaml


+++ add example with kind:pod +++



# Delete a Pod.
#
kubectl delete pod <pod>
kubectl get pod <pod>



# The Deployment controller, sitting on top of ReplicaSet, simplifies application updates
#
# kubectl set image deployment ...
#
# E.g. use my-nginx deployment to update container my-nginx to nginx image version 1.12.0
#
kubectl set image deployment my-nginx my-nginx=nginx:1.12.0
kubectl set image deployment/my-nginx my-nginx=nginx:1.12.0

# Inspect the nginx Deployment's rollout
#
kubectl rollout status deployment my-nginx
kubectl rollout status deployment/my-nginx


# Rollback to the previous version
#
kubectl rollout undo deployment my-nginx



Deployment Example from Kubernetes docs
---------------------------------------
https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/

Yaml manifest file, load-balancer-example.yaml:

	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  labels:
	    app.kubernetes.io/name: load-balancer-example
	  name: hello-world
	spec:
	  replicas: 5
	  selector:
	    matchLabels:
	      app.kubernetes.io/name: load-balancer-example
	  template:
	    metadata:
	      labels:
	        app.kubernetes.io/name: load-balancer-example
	    spec:
	      containers:
	      - image: gcr.io/google-samples/node-hello:1.0
	        name: hello-world
	        ports:
	        - containerPort: 8080

# Deploy.
# Creates a Deployment object and an associated ReplicaSet object.
#
kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml

# Display information about the Deployment
#
kubectl get deployments hello-world
kubectl describe deployments hello-world

# Display information about your ReplicaSet objects
#
kubectl get replicasets
kubectl describe replicasets

# Create a Service object that exposes the deployment
#
kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

# Display information about the Service
#
kubectl get services my-service

# Display detailed information about the Service
#
kubectl describe services my-service

# To verify these are pod addresses, enter this command:
#
kubectl get pods --output=wide

# Use the external IP address (LoadBalancer Ingress) to access the Hello World application:
#
curl http://<external-ip>:<port>


# To delete the Service, enter this command:
#
kubectl delete services my-service

# To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello
# World application, enter this command:
#
kubectl delete deployment hello-world



CI/CD
-----
Continuous Integration / Continuous Delivery

CI: Continuous building and testing

Continuous Delivery:	Deployment in production manually
Continuous Deployment:	Deployment in production automatically

Red/black deployment: Only one of the prod environments is getting traffic (a.k.a blue/green)
Canary deployment:	  Deploy the change to a small subset of servers

SAST 	Static Application Security Testing 	(“white box" testing)
DAST 	Dynamic Application Security Testing 	(“black box” testing)
	    (vulnerability scanning of application)


GitLab CI pipeline:
- Add .gitlab-ci.yml to your repository’s root directory.
- Configure a Runner.

Runners run the code that is defined in .gitlab-ci.yml

dind image: docker-in-docker image; each job will be isolated in a container.


Common simple pipeline stages:
- Build
- Test
- Deploy

Example more comprehensive pipeline stages:
- Build
- Test (code quality, SAST, unit tests)
- Review
- Dast
- Deploy/Production
- Cleanup


Example of a GitLab CI pipeline file (.gitlab-ci.yml):

	image:

	services:

	variables:
		<KEY>:<VALUE>
		PORT:3000
	before_script:
	  -
      -
	build:
	  stage:
	  script:
	    -
	    -
	    -

+++ NOT executed; #TODO: 8.7 Deploying Applications, LAB +++

	# Register the GitLab runner
	gitlab-runner register

	# Start GitLab runner
	gitlab-runner run -d /tmp



Hosting Applications on Network Devices
---------------------------------------
Cisco Catalyst 9000 Series, IOS XE

Two ways of application hosting
- Native    (Cisco)
- Container (Docker, KVM)

IOx:
- IOs + linuX
- End-to-end application framework.
- Provides application-hosting capabilities for different application types.
- Example is the Cisco Guest Shell, a special container deployment.

IOx facilitates life cycle management of applications:
- Distribution
- Deployment
- Hosting
- Starting
- Stopping
- Monitoring

Cisco IOx application hosting features:
- Hides network heterogenity.
- APIs manage the life cycle of applications hosted on a device.
- Centralized application life cycle management.
- Cloud-based developer experience.


Applications have no access to the internal device flash.
Applications must reside on External storage.
External storage options for containers, applications and local logging:
- USB 3.0	(max 120 GB)
- SATA SSD  (max 1 TB)

Storage encrypted with AES-256 (Advanced Encryption Standard)
USB is password-protected


Hosted applications can be managed through:
- CLI
- Cisco DNA Center API

Application states:
- DEPLOYED	(installed, but resources not committed)
- ACTIVATED	(resources committed; container artifacts generated)
- RUNNING
- STOPPED

Application lifecycle stages:
- Install
- Activate
- Start

app-hosting install appid myapp package usbflash1:myapp.tar
app-hosting activate appid myapp
app-hosting start appid myapp

app-hosting stop appid myapp
app-hosting deactivate appid myapp
app-hosting uninstall appid myapp


Containers can be connected via:
- Management interface Gi0/0
  mgmt int -- mgmt VRF -- mgmt bridge -- container
  mgmt interface and the container are in the same subnet

- Front panel data ports (Gi1/0/1, Gi1/0/2, etc.)
  data port -- L2 switch -- AppGigabitEthernet -- Bridge - Containers


So, two types of networking apps:
- Control plane apps (access the management interface)
- Data plane apps    (access the front-panel ports)


AppGigabitEthernet interface:
- Internal hardware data port
- hardware-switched to the front panel data ports
- Same features as front-panel GigabitEthernet ports (e.g.switchport mode trunk, switchport trunk allowed vlan).
- Avoid using switchport mode access, because it limits the containers to only one VLAN.
- Single app (IOS XE Release 16): Either vlan-access or trunk mode; no mixing of the modes
- Single app (IOS XE Release 17): Mixing allowed.
- Multiple applications: Mixing allowed.


AppGigabitEthernet is only available on:
- Catalyst 9300 (IOS XE 16.12.1)
- Catalyst 9400 (IOS XE 17.1.1)

Assigning an IP address to the container:
- Linux CLI (static)
- IOS XE CLI (static)
- DHCP
Cisco DNA Center workflow supports both DHCP and static options.


! Application Hosting on Front-Panel VLAN Ports
!
interface AppGigabitEthernet 1/0/1
  switchport trunk allowed vlan 10-12,20
  switchport mode trunk
!
app-hosting appid iox_app
  app-vnic AppGigEthernet vlan-access      <<<<<
    vlan 10 guest-interface 2 			   <<<<<
     !
     guest-ipaddress 192.168.0.1 netmask 255.255.255.0


! Application Hosting on Front-Panel Trunk Ports
!
! Note: Containers can have L3 routed access to an IOS SVI when using app-vnic "trunk" mode
!
interface AppGigabitEthernet 3/0/1
  switchport trunk allowed vlan 10-12,20
  switchport mode trunk
!
app-hosting appid iox_app
  app-vnic AppGigEthernet trunk 		<<<<<
    guest-interface 2 					<<<<<


Enabling Application Hosting
-----------------------------
Prerequisites:
- Release >= 16.12
- Cisco certified USB3.0 Flash Drive
- Smart License for the Catalyst 9000 series
- DNA-Advantage license


! Install licenses
!
license boot level network-advantage addon dna-advantage
wr mem
reload
show version

! Enable IOx Application Hosting framework
!
conf t
iox

! Check that CAF, IOxman and Libvirtd services are running
!
show iox-service


CAF:	Cisco Application Hosting Framework



Deploying Application
---------------------
- Download the application on the USB
- Configure the interface (what is meant here: mgmt, data port / AppGigabitEthernet ??)
- Install the application on the networking device


! Generate a Docker tarball:
!
docker pull myapp
docker save myapp:latest -o myapp.tar

! Interface config etc.
!
...

! Copy Docker tar to the switch (usbflash1)
!
! ++ Check this cmd ++
!
copy scp:<path> usbflash1:myapp.tar vrf Mgmt-vrf


! Install the app on the switch
!
app-hosting install appid myapp package usbflash1:myapp.tar
app-hosting activate appid myapp
app-hosting start appid myapp
!
show app-hosting list


! Stop and uninstall
!
app-hosting stop appid myapp
app-hosting deactivate appid myapp
app-hosting uninstall appid myapp


Guest Shell (on IOS-XE)
-----------------------

! Enable IOx
!
conf t
iox

! Check that CAF, IOxman and Libvirtd services are running:
show iox-service


! Enable the communication between IOS XE and the Guest Shell container
! Network settings of IOS XE host
!
conf t
interface VirtualPortGroup0
  ip address 192.168.1.1 255.255.255.0

! Network settings of Guest Shell container  (>= IOS XE 16.7.1):
! - IP address
! - Default gateway
! - DNS server.
!
app-hosting appid guestshell
 vnic gateway1 virtualportgroup 0 guest-interface 0 guest-ipaddress 192.168.1.2 netmask 255.255.255.0 gateway 192.168.1.1 name-server 8.8.8.8


! NAT towards outside world (via outside interface Gig/1)
!
interface VirtualPortGroup0
 ip nat inside
!
interface GigabitEthernet1
 ip nat outside
!
ip access-list extended NAT-ACL
 permit ip 192.168.1.0 0.0.0.255 any
!
ip nat inside source list NAT-ACL interface GigabitEthernet1 overload


! Install/enable Cisco Guest Shell
!
guestshell enable
show app-hosting detail
end

! Access guestshell; yum installed by default
!
guestshell
[guestshell@guestshell ~]$ whoami
[guestshell@guestshell ~]$ pwd
[guestshell@guestshell ~]$ ls /flash/

[guestshell@guestshell ~]$ sudo yum update -y
[guestshell@guestshell ~]$ sudo yum install -y git nano
[guestshell@guestshell ~]$ git --version
[guestshell@guestshell ~]$ nano --version
[guestshell@guestshell ~]$ git clone https://github.com/CiscoDevNet/dnav3-code.git


! Execute IOS XE CLI commands from within the guestshell
!
[guestshell@guestshell ~]$ dohost "show memory statistics"

! Run Guest Shell linux applications directly from IOS XE prompt
!
guestshell run groups
guestshell run cat /etc/centos-release


IOS-XE
- IOx disabled (Application Hosting framework)
- Guest Shell disabled

Nexus, NX-OS:
- Guest Shell is enabled by default on NX-OS
- IOx enabled then implicitly ?


=============================================


Distributed Systems
===================

Principles large scale distributed applications:
- Availability
- Performance
- Transparency
- Scalability


HAProxy frontend example:

	frontend http
	  bind *:80
	  mode http
	  acl url_img path_beg /images
	  use_backend img-backend if url_img	<<< to img-backend servers
	  default_backend web-backend 			<<< to default web-backend servers

HAProxy backend example:

	backend web-backend
	   balance roundrobin [leastconn|source]
	   mode http
	   server web1 web1.example.com:80 check
	   server web2 web2.example.com:80 check


Types of load balancers:
- Layer 4 (Transport layer)
- Layer 7 (Application layer)


HAProxy
Linux Virtual Servers (LVS) (L4)
NGINX


Reasons to use APIs to interact with web services:
- Lightweight
- Flexible
- Scalable
- Platform-agnostic


GET
POST
PUT
PATCH
DELETE


Alternatives to REST: gRPC, RPC, SOAP, CORBA


Cisco DNA Center API:
- Northbound API aka Intent API
- RESTful
- HTTPS: GET, POST, PUT, and DELETE
- JSON
- https://developer.cisco.com/docs/dna-center/api/1-3-1-x/


Generate DNA token:
	POST <cluster-ip>/dna/system/api/v1/auth/token
	with basic authorization header with username and password

	HTTP response code 200 Json token:
		{
		    "Token": "auX0eFe ... O7uMRA-ab“
		}

Subsequent requests:
	GET <cluster-ip>/dna/intent/api/v1/<object>
	with X-Auth-Token header


Example:

	import requests

	base_url = 'https://<hostname>'
	r = requests.post(f'{base_url}/dna/system/api/v1/auth/token',
	                         auth=('<USERNAME>', '<PASSWORD>'))
	r.raise_for_status()
	token = r.json()['Token']

	headers = {'X-Auth-Token': token}
	r = requests.get(f'{base_url}/dna/intent/api/v1/site-health', headers=headers)
	r.raise_for_status()
	sites = r.json()['response']


Example SDK (Cisco Intersight API; requires request signing):

	from intersight.intersight_api_client import IntersightApiClient
	import intersight.apis.network_element_summary_api as summary_api

	client = IntersightApiClient(
	    host='https://intersight.com/api/v1',
	    private_key='<KEYFILE>',
	    api_key_id='<ID>'
	)
	api = summary_api.NetworkElementSummaryApi(client)
	summary = api.network_element_summaries_get()



Event-Driven Architecture (EDA)
-------------------------------
Atomic or simple event. 			(what happened)
Related events or event stream. 	(when something happened)
Behavioral or complex events. 		(why an event happened)

Loosely coupled and well distributed.
Scalability and performance.

Event Emitter
Event Channel
Event Consumer

EDA characteristics:
- Asynchronous
- Real-time
- Fine-grained
- Unicast & Multicast
- Event Ontology

EDA topologies:
- Mediator topology (orchestrates)
- Broker topology

Mediator topology components:
- Event queue
- Event mediator
- Event channel
- Event processor

Message broker topology: collection of event channels (message queue).


Microservices
-------------
Microservices architecture characteristics:
- Independent evolution and deployment
- Independent scalability
- Availability
- Modularity preservation
- Independent technology and language stack


Logging
-------
Aggregate logging techniques:
- Log replication
- Centralized syslog repositories
- Distributed log collection


Distributed logging stages:
- Collection (by collector agents)
- Forwarding (to log aggregator)
- Storage 	 (in object storage)
- Indexing   (into different data sets; optional)
- Alerting	 (based on thresholds and patterns; optional)


ELK stack:
- Elasticsearch
- Logstash
- Kibana
Beats is used as collector agent

Distributed tracing frameworks: OpenTracing and OpenCensus (merged into OpenTelemetry)


ssh root@dev.app.local -p 12005
service config status
service config start

service app status
service app start

Graylog filter: source:front* AND INFO


Cisco AppDynamics
-----------------
- Application Monitoring.
- Stores historic data & calculates baselines.
- Correlate function or web service calls across multiple systems.
- Visualizes the aggregate data.
- Does not rely on logging.
- Uses code instrumentation techniques to "hook into" application code.
- Discovers business transactions (user action, response to an API call, etc.)
- Instrumentation code observes the communication with other systems.
- Creates a map of application components (transaction flow map; application topology).

A flow map consists of:
- Nodes 		(monitored application server)
- Tiers 		(logical grouping of nodes like front-end and back-end)
- Backends 		(black-box; not instrumented component; DB ar MQ)
- Applications 	(components representing a different business application)


AppDynamics Controller
- Processes and analyzes data.
- Host on-prem or use the officially provided cloud instance.

Monitoring Agents collect data and forward to Cisco AppDynamics controller.

Types of monitoring agents:
- Application Server Agent 	(analyzes and monitors code)
- Machine Agent 			(monitors server performance etc.)
- End User Agent 			(monitors performance experienced by users)

Application Server Agents available for Java, .NET, Node.js, Python, PHP, Go.

To instrument a Python application:
- Install Python Agent: 		pip install -U appdynamics\<4.5
- Create configuration file: 	application, node, tier and controller info
- Start Python Agent:			pyagent run -c <config> -- <command>
  (incl. Java agent proxy)


Example configuration file skeleton

	[agent]
	app = <app_name>
	tier = <tier_name>
	node = <node_name>

	[controller]
	host = <controller_host>
	port = <controller_port>
	ssl = on|off
	account = <your AppDynamics controller account name>
	accesskey = <your AppDynamics controller account access key>



CAP Theorem (Brewer's theorem, Eric Brewer)
-------------------------------------------
- Consistency: 			Every read receives the most recent write or an error.
- Availability: 		Every request receives a (non-error) response, without the guarantee that it contains the most recent write.
- Partition tolerance:	The system continues to operate despite messages being dropped (or delayed) by the network between nodes.

In the presence of a network partition, choose between consistency and availability:
- Cancel the operation 		 	(decrease availability, ensure consistency)
- Proceed with the operation 	(provide availability, risk inconsistency)


Challenges/Solutions in Distributed Systems:
- Eventually consistent systems.
- Split-brain scenario.
- Quorum: Majority of the nodes agree.
- Conflict resolution mechanisms. Timestamps, newer values take precedence; "last write wins" scenario (changes might be lost).
- Bulkhead pattern. For example: multiple, individual resource pools; different connection pools.
- Circuit Breaker pattern: Fail fast, retry after a while.
- Service mesh.


=============================================


UCS
---
UCS 	Unified Compute System
MIT:	Management information tree
MIM:	Management information model
DME:	Data Management Engine
DN:		Distinguished Name
RN:		Relative Name

Cisco UCS Manager:
- XML API
- API is transactional (change multiple objects, or none)
- Managed objects (MOs) in hierarchical MIT
- MIT aka MIM (management information model)
- Top of MIT: sys

Data management engine (DME):
- User-level process on fabric interconnects.
- Manages and centrally stores the information model.
- Model-driven framework: First apply change to MIT, then to the managed endpoint.

The API entities:
- Classes: Properties and states of objects.
- Methods: Actions on objects.
- Types:   Object properties that map values to the object state.

dn = "sys/chassis-4/blade-1/adaptor-1/host-eth-1"


UCS API Methods:

- Authentication methods
	- aaaLogin
	- aaaRefresh
	- aaaLogout

- Query methods, examples:
	- configResolveDns 			Retrieves objects by a set of DNs.
	- configResolveClass 		Retrieves objects of a given class.
	- configResolveClasses 		Retrieves objects of multiple classes.
	- configFindDnsByClassId 	Retrieves the DNs of a specified class.
	- configResolveChildren 	Retrieves the child objects of an object.
	- configResolveParent 		Retrieves the parent object of an object.
	- configScope 				Class queries on a DN in the MIT.

- Configuration methods
	- configConfMo: 			Configs a single MO/DN.
	- configConfMos: 			Configs multiple subtrees (for example several DNs).
	- configConfMoGroup: 		Configs multiple subtree structures (DNs) or MOs.

- Event subscription methods
	- eventSubscribe
	- eventUnsubscribe


Most query methods have an argument inHierarchical. If true, it returns all child objects.
	<configResolveDn … inHierarchical="true"></>

Query filter types:
- Simple filters
- Property filters
- Composite Filters
- Modifier Filter

Event channel connection (eventSubscribe) closes after 600 seconds of inactivity.
Keep alive by:
- aaaKeepAlive request (using same event channel session cookie)
- send any XML API method (using same event channel session cookie).


Workflow to retrieve or configure an MO:

	Login Phase
		--- aaaLogin -->
		<-- cookie ---
	Requests:
		--- Request (cookie) -->
		<-- response ---
	Logout Phase
		--- aaaLogout -->


Example configResolveDn query on DN sys/chassis-1/blade-1:

	<configResolveDn
	     dn="sys/chassis-1/blade-1"
	     cookie="<real_cookie>"
	     inHierarchical="false"/>

Response:

	<configResolveDn dn="sys/chassis-1/blade-1"
	     cookie="<real_cookie>"
	     response="yes">
	     <outConfig>
	          <computeItem adminPower="policy"
	          adminState="in-service"
	          assignedToDn=""
	          association="none"
	          availability="available"
	          chassisId="1"
	          checkPoint="discovered"
	          connPath="A"
	          connStatus="A"
	          discovery="complete"
	          dn="sys/chassis-1/blade-1"
	          fltAggr="0"
	          fsmDescr=""
	          fsmFlags=""
	          fsmPrev="DiscoverSuccess"
	          fsmRmtInvErrCode="unspecified"
	          fsmRmtInvErrDescr=""
	          fsmRmtInvRslt=""
	          fsmStageDescr=""
	          fsmStamp="2008-11-24T01:27:10"
	          fsmStatus="nop"
	          fsmTry="0"
	          lc="discovered"
	          managingInst="A"
	          model="Gooding"
	          name=""
	          numOfAdaptors="1"
	          numOfCores="4"
	          numOfEthHostIfs="2"
	          numOfFcHostIfs="2"
	          numOfThreads="0"
	          operPower="off"
	          operState="unassociated"
	          operability="operable"
	          originalUuid="1b4e28ba-2fa1-11d2-0101-b9a761bde3fb"
	          presence="equipped"
	          revision=""
	          serial="1-1"
	          slotId="1"
	          totalMemory="4096"
	          uuid=""
	          vendor="Cisco"/>
	     </outConfig>
	</configResolveDn>


Example Failed Request (Includes XML attributes for errorCode and errorDescr):

	<configConfMo dn="fabric/server"
	     cookie="<real_cookie>"
	     response="yes"
	     errorCode="103"
	     invocationResult="unidentified-fail"
	     errorDescr="can't create; object already exists.">
	</configConfMo>


Example Empty Results
A query for a nonexistent object is not treated as a failure by the DME. It returns a success message with an empty data field (<outConfig> </outConfig>)

	<configResolveDn
	     dn="sys/chassis-1/blade-4711"
	     cookie="<real_cookie>"
	     response="yes">
	     <outConfig>
	     </outConfig>
	</configResolveDn>


Cisco UCS Manager Emulator:
- Emulates the actual UCS system.
- Same code, full MIT.
- CentOS VM (.vmx and .ova image files.


Example Cisco UCS Manager API: see UCS.py


Terraform
---------
https://github.com/CiscoUcs/UCS-Terraform

- Open-source, HashiCorp.
- Infrastructure as Code (IaC).
- Declarative: Describe the desired state/intent.
- HashiCorp Configuration Language (HCL).
- Configuration  in .tf files.
- Configuration files under version control.
- Basic syntax is the block type:

	<block type> "<block label>" ["<block label>"] {
	  <identifier> = <expression>
	}

- Examples of block types:
	- Resource block (infrastructure objects; resource type, resource name; resource arguments)
	- Provider block type (communicates with the infrastructure; deploy the configuration)
- Provides and resources are linked. Provider name is prefix of resource type.

	# For example in main.tf
	#
	provider "ucs" {
	  ip_address  = "1.2.3.4"
	  username    = "john"
	  password    = "supersecret"
	  log_level    = 6
	  log_filename = "terraform.log"
	}

	# For example in profiles.tf
	#
	resource "ucs_service_profile" "master-server" {
	  name                     = “terraserver1"
	  target_org               = “root-org"
	  service_profile_template = “terraformprofiletemplate"
	  #
	  metadata {
	    # Free format. Values must be strings.
	    #
	    role             = "master"
	    ansible_ssh_user = "root"
	    foo              = "bar"
	  }
	}

terraform init 				Initialize a Terraform working directory
terraform plan 				Generate and show an execution plan
terraform apply 			Builds or changes infrastructure

terraform version 			Prints the Terraform version
terraform validate      	Validate Terraform file(s)
terraform show 				Inspect Terraform state or plan

terraform get 				Download and install modules for the configuration
terraform destroy 			Destroy Terraform-managed infrastructure


AWS example:

	provider "aws" {
		access_key = "Your Access Key"
		secret_key = "Your Secret Key"
		region = "ap-southeast-2"
	}

	resource "aws_instance" "MyFirstTerraFormedVM" {
		ami = "ami-da2ac3b8"
		instance_type = "t2.micro"
	}

	resource "aws_vpc" "myvpc" {
		cidr_block = "${var.vpc_cidr}"
		enable_dns_hostnames = true
		tags = {
			Name = "myvpc"
		}
	}

	resource "aws_subnet" "myvpc_public_subnet" {
	    vpc_id = "${aws_vpc.myvpc.id}"
	    cidr_block = "${var.subnet_one_cidr}"
	    availability_zone = "${data.aws_availability_zones.availability_zones.names[0]}"
	    map_public_ip_on_launch = true
	    tags = {
	        Name = "myvpc_public_subnet"
	    }
	}


Orchestration:
- Focus on bootstrapping and initializing resources.
- Provision resources.
- Typical actions: Create a VM. Create virtual network.
- Example: Terraform.

Configuration management:
- Configure resources that exist.
- Manage resources.
- Typical actions: Install software, provide configurations, manage services.
- Examples: Ansible, Puppet.

There is some overlap between tools. They can work together.


Two categories of management configuration and orchestration tools:
- Procedural	(Ansible)
- Declarative 	(Puppet and Terraform)

Ansible is partly hybrid (procedural and declarative).


Ansible 					|	Puppet
----------------------------|--------------------------------------
Procedural 					|	Declarative
Agentless 					|	Agent on managed node; Master node (Proxy agent, if agent not possible on device)
Client only 				|	Client/server
Push model 					|	Pull model
							|
Playbooks (YAML) 			|	Catalogs
Tasks 						|	Manifests (desired state)
Inventory of nodes			|	Agent gathers facts; sends them to master; requests a catalog from master.
							|	Master creates a catalog from the manifests; sends catalog to agent.
							|	Agent checks catalog and applies required changes; sends a report to master.
							|   (Agent requests config every 30 minutes)
							|
Playbook needs to revert	|	Executes periodically: manual changes will be reverted!
 any manual changes 		|



Puppet
------

Puppet Concepts:
- Resource: Describes particular aspect of a system. Unit of configuration.
- Class: 	Collection of resources.
- Manifest: Configuration file describing how to configure resources.
- Catalog: 	Compiled configuration for a node.
- Module: 	Collection of manifests, templates and other files. Allow reusability.
			Write own modules or download from Puppet Forge.

Manifest:
- Main infrastructure configuration.
- Puppet code that describes configuration of resources.
- Single file or directory with multiple manifests. Multiple files treated as one.
- Text file with .pp extension (usually site.pp).
- UTF-8 encoding.
- Configuration in manifests is compiled into a catalog for each individual node.


Built-in resource types:
- exec: 	Executes commands.
- file: 	Manages files.
- service: 	Manages running services.
- user: 	Manages users.

	<RESOURCE TYPE> { '<TITLE>':
		<ATTRIBUTE> => <VALUE>,
		<ATTRIBUTE> => <VALUE>,
		...
	  }


	package { 'openssh-server':
	  ensure => installed,
	}

	file { '/etc/ssh/sshd_config':
	  source  => 'puppet:///modules/sshd/sshd_config',
	  owner   => 'root',
	  group   => 'root',
	  mode    => '0640',
	  notify  => Service['sshd'], # sshd restarts whenever you edit this file.
	  require => Package['openssh-server'],
	}

	service { 'sshd':
	  ensure     => running,
	  enable     => true,
	}

	user { "jane":
	    ensure     => present,
	    uid        => '507',
	    gid        => 'admin',
	    shell      => '/bin/zsh',
	    home       => '/home/jane',
	    managehome => true,
	}


Class specification:

	class save_time (String $file_path='/root/time.log') {

	  file { 'log_file_present':
	    ensure => present,
	    path => $file_path,
	  }

	  exec { 'save_time_cmd':
	    command => "/usr/bin/time > $file_path"
	  }
	}

Ways of declaring a class:
- Include-like 		Most common. Declare multiple times.

						include save_time

- Resource-like		Pass parameters (override defaults)

						class { 'save_time':
						  file_path => '/tmp/temp_time.log'
						}


Example site.pp (in /etc/puppetlabs/code/environments/production/manifests):

	class conf_if (String $ip_address) {
	  file { 'if_config':
	    path => '/etc/sysconfig/network-scripts/ifcfg-enp0s3',
	    content => template("$settings::manifest/network-if.cfg"),
	    ensure => present,
	  }
	  exec { 'if_down': command => '/usr/sbin/ifdown enp0s3',
	    require => File['if_config'],
	  }
	  exec { 'if_up': command => '/usr/sbin/ifup enp0s3',
	    require => Exec['if_down'],
	  }
	}

	class conf_dns {
	  $dns_servers = ['192.168.1.253', '192.168.1.254']
	  file { 'dns_config':
	    path => '/etc/resolv.conf',
	    content => template("$settings::manifest/dns.cfg"),
	    ensure => present,
	  }
	}

	node 'puppeta01' {
	  class { 'conf_if': ip_address => "192.168.1.11" }
	  include conf_dns
	  class { 'ntp': servers => ['192.168.1.250'] }
	}

	node 'puppeta02' {
	  class { 'conf_if': ip_address => "192.168.1.12" }
	  include conf_dns
	  class { 'ntp': servers => ['192.168.1.250'] }
	}


Template file, network-if.cfg:

	TYPE=Ethernet
	BOOTPROTO=none
	DEFROUTE=yes
	IPV4_FAILURE_FATAL=no
	NAME=enp0s3
	UUID=
	DEVICE=enp0s3
	ONBOOT=yes
	IPADDR=<%= @ip_address %>
	PREFIX=24
	GATEWAY=192.168.1.1

Template file, dns.cfg:

	<% [@dns_servers].flatten.each do |server| -%>
	nameserver <%= server %>
	<% end -%


To deploy the configuration:
- Run the pull request manually on the node: puppet agent -t
- Or wait until next poll period.


# Client, server with Puppet agent
#
# Test connectivity or pul the configuration
#
puppet agent -t

# Enable new settings on agent (after change on master)
#
puppet agent



# Puppet Server
#
service puppetserver status
puppetserver start

# Pull configuration every 2 minutes (default is 30 min)
# Or modify /etc/puppetlabs/puppet/puppet.conf
#
puppet config set --section agent runinterval 120

# Check logs on master
#
cat /var/log/puppetlabs/puppetserver/puppetserver.log

# Check certificates
#
puppetserver ca list --all


# Install standard Puppet NTP module
#
puppet module install puppetlabs-ntp
puppet module list


+++ NOT executed, LAB 10.6 Configure Network Parameters Using Puppet +++



Ansible
-------
https://docs.ansible.com/ansible/latest/user_guide/

Manages elements using: SSH, NETCONF, REST APIs, etc.
Ansible uses YAML playbooks and Jinja2 templating language.
Two basic elements Ansible architecture: core engine and modules.

Inventory:
- List of managed nodes ('hostfile').
- INI or YAML format.
- Default: etc/ansible/hosts.
- Groups and nesting for easier scaling.
- Assigning variables to host or group.
- Dynamic inventory: Track hosts from multiple sources, e.g. cloud providers (inventory Plugins and scripts).

Playbook:
- YAML format.
- Instructions to configure infrastructure.
- Playbook consists of plays.
- Play is a collection of tasks.
- Task consist of name and module.

Tasks are executed sequentially.
Specific task is executed against all hosts in play.
Task must be finished on all hosts, before moving to the next one.
If a task fails on a host, the next tasks are not executed on that host.


Create reusable playbooks with:
- Include
- Import
- Roles

Split playbook into smaller playbooks.
Reuse smaller playbooks in other parent playbooks.
Imports are processed at playbook parsing time.
Include are processed when task is encountered.

A role groups tasks, variables, templates, etc.
A role is typically used to configure a well-defined part of the infrastructure (BGP peer, OSPF process).
A role has a directory structure. It consists of multiple files.

Collections pack Ansible content in a reusable format for simple distribution.
A collection may include playbooks, roles, modules and plugins.
Install Ansible collection from a community hub, a hosted server or from a tarball.


Inventory, INI format:

	mail.example.com

	[webservers]
	foo.example.com
	bar.example.com

	[dbservers]
	one.example.com
	two.example.com
	three.example.com


Inventory, YAML format:

	all:
	  hosts:
	    mail.example.com:
	  children:
	    webservers:
	      hosts:
	        foo.example.com:
	        bar.example.com:
	    dbservers:
	      hosts:
	        one.example.com:
	        two.example.com:
	        three.example.com:


Host variables:

	[atlanta]
	host1 http_port=80 maxRequestsPerChild=808
	host2 http_port=303 maxRequestsPerChild=909


Host variables (for non-standard SSH port):

	badwolf.example.com:5309


Inventory alias:

	jumper ansible_port=5555 ansible_host=192.0.2.50


Group variables:

	[atlanta]
	host1
	host2

	[atlanta:vars]
	ntp_server=ntp.atlanta.example.com
	proxy=proxy.atlanta.example.com


host_vars folder:
- Files have same names as hosts in the inventory.
- Files contain host-specific variables.

group_vars folder:
- Files names correspond to group names in inventory.
- Files contain group-specific variables.


Syntax of a play:

	- hosts: <HOST_OR_GROUP_NAME>
	  tasks:
	    - name: <TASK NAME>


Example playbook (verify-apache.yml, one play):

	---
	- hosts: webservers
	  vars:
	    http_port: 80
	    max_clients: 200
	  remote_user: root
	  tasks:
	  - name: ensure apache is at the latest version
	    yum:
	      name: httpd
	      state: latest
	  - name: write the apache config file
	    template:
	      src: /srv/httpd.j2
	      dest: /etc/httpd.conf
	    notify:
	    - restart apache
	  - name: ensure apache is running
	    service:
	      name: httpd
	      state: started
	  handlers:
	    - name: restart apache
	      service:
	        name: httpd
	        state: restarted


Example playbook (multiple plays):

	---
	- hosts: webservers
	  remote_user: root

	  tasks:
	  - name: ensure apache is at the latest version
	    yum:
	      name: httpd
	      state: latest
	  - name: write the apache config file
	    template:
	      src: /srv/httpd.j2
	      dest: /etc/httpd.conf

	- hosts: databases
	  remote_user: root

	  tasks:
	  - name: ensure postgresql is at the latest version
	    yum:
	      name: postgresql
	      state: latest
	  - name: ensure that postgresql is started
	    service:
	      name: postgresql
	      state: started


routes_ntp.yml:

    ---
	- name: Change routes and NTP on all servers
	  hosts: all
	  tasks:

	    - name: Change route
	      shell: /sbin/ip route replace {{ mgmt_subnet }} dev {{ if_name }}

		- name: Create NTP configuration from the template
		  template:
		    src: ntp_config.tmpl
		    dest: /etc/ntp.conf
		    owner: root
		    group: root
		    mode: 644

		- name: Restart NTP service
		  service:
		    name: ntp
		    state: restarted


	group_vars/all:

		mgmt_subnet: 10.60.70.0/24
		if_name: bg0
		ntp_servers:
		  - ntp1.tisipi.nl
		  - ntp1.tisipi.nl

	templates/ntp_config.tmpl:

		{% for server in ntp_servers %}
		server {{ server }} iburst
		{% endfor %}



# Run a playbook
#
ansible-playbook -i hosts configure_servers.yml


# Create a reusable Ansible role
#
ansible-galaxy init roles/ntp


# Ping hosts
#
ansible all -i hosts -m ping


+++ NOT executed, LAB 10.8, Configure Network Parameters Using Ansible +++


Options for single source of truth:
- Configuration files: YAML, JSON, XML, CSV, etc.
- Databases
	Relational: MySQL, PostgreSQL, MariaDB, etc.
	Non-relational: MongoDB, Cassandra, etc.
- Off-the-self solutions: NetBox, Network Source of Truth (NSot)


+++ Add code snippets for reading/writing +++
YAML
JSON
XML
CSV


JSON:
	import json
	json_object = json.loads(response_string)

	json.dumps(json_object, sort_keys=True, indent=4)


	Remarks:
	- JSON strings themselves must be delineated using double quotes " "
	- JSON uses an all-lowercase convention of true and false (Python uppercase).


MySQL:

	import MySQLdb

	db = MySQLdb.connect(host='localhost', user='dbAdmin', password='myDbAdmin123', db='mySimpleDB')
	c = db.cursor()
	c.execute('SELECT * FROM devices')

	devices = c.fetchall()
	for device in devices:
	 print(device)



Firepower Threat Defense
------------------------
https://developer.cisco.com/firepower/threat-defense/  +++ #TODO, 1.5h learning module ++++
https://developer.cisco.com/site/ftd-api-reference/#!authenticating-your-rest-api-client-using-oauth/requesting-a-custom-access-token

FDM 	Firepower Device Manager
FTD 	Firepower Threat Defense

API explorer: https://<hostname>/#/api-explorer

Firepower Threat Defense API:
- REST API
- JSON
- Oauth 2.0, JSON Web Tokens
- Different Access and refresh tokens!
- Access token valid for 1800s (default is 30 min)
- Standard HTTP status codes: 200 (OK) successful GET/PUT/POST call; 204 a successful DELETE call
- Format URLs: https://<hostname>/api/fdm/<version>/object/networks
			   https://<hostname>/api/fdm/<version>/object/urls


JSON object for the password-granted access token grant:

	{
	  "grant_type": "password",
	  "username": "string",
	  "password": "string"
	}


Requesting a Password-Granted Access Token:

	Use POST https://<server>/api/fdm/v4/fdm/token to obtain the access token.

	curl -X POST \
	    --header 'Content-Type: application/json' \
	    --header 'Accept: application/json' \
	    -d '{
	        "grant_type": "password",
	        "username": "admin",
	        "password": "Admin123"
	    }' https://ftd.example.com/api/fdm/v4/fdm/token


	Retrieve tokens from response:
	{
    "access_token": "eyJ..snip..K38",
    "expires_in": 1800,
    "token_type": "Bearer",
    "refresh_token": "eyJ..snip..z_o",
    "refresh_expires_in": 2400
	}


Requesting a Custom Access Token:

curl -X POST \
  --header 'Content-Type: application/json' \
  --header 'Accept: application/json' \
  -d '{
    "grant_type": "custom_token",
    "access_token": "eyJ..snip..K38",
    "desired_expires_in": 2400,
    "desired_refresh_expires_in": 3000,
    "desired_subject": "api-client",
    "desired_refresh_count": 3
  }' https://ftd.example.com/api/fdm/v4/fdm/token



Using an Access Token on API Call, GET /object/networks:

	Include the received TOKEN in the Authorization header
	Authorization: Bearer <access_token>

	curl -k -X GET \
	    -H 'Accept: application/json' \
	    -H 'Authorization: Bearer eyJ..snip..bN8' \
	    https://ftd.example.com/api/fdm/v4/object/networks


Refreshing an Access Token:

	Refresh token using the original refresh token. But note that this supplies a NEW pair of access and refresh tokens (it does NOT extend the life of the old tokens).

	curl -X POST \
	    --header 'Content-Type: application/json' \
	    --header 'Accept: application/json' \
	    -d '{
	       "grant_type": "refresh_token",
	       "refresh_token": "eyJ..snip..z_o",
	     }' https://ftd.example.com/api/fdm/v4/fdm/token



Revoking an Access Token:

	Clean up by revoking a token when the user logs out of your API client.

	{
	    "grant_type": "revoke_token",
	    "access_token": "string",
	    "token_to_revoke": "string",
	    "custom_token_subject_to_revoke": "string"
	}

	You must specify one, and only one, of token_to_revoke and custom_token_subject_to_revoke.

	curl -X POST \
	    --header 'Content-Type: application/json' \
	    --header 'Accept: application/json' \
	    -d '{
	        "grant_type": "revoke_token",
    		"access_token": "eyJ..snip..K38",
	        "custom_token_subject_to_revoke": "api-client"
	    }' https://ftd.example.com/api/fdm/v4/fdm/token

	curl -X POST \
	    --header 'Content-Type: application/json' \
	    --header 'Accept: application/json' \
	    -d '{
	        "grant_type": "revoke_token",
    		"access_token": "eyJ..snip..K38",
    		"token_to_revoke": "eyJ..snip..K38",
	    }' https://ftd.example.com/api/fdm/v4/fdm/token


Deploying Configuration Changes:

	Changes are not immediately active. You must deploy them.
	Use the POST /operational/deploy resource to initiate a deployment.

	curl -X POST \
	    --header 'Content-Type: application/json' \
	    --header 'Accept: application/json' \
	    https://ftd.example.com/api/fdm/v4/operational/deploy


Evaluate the response to verify that the deployment job was queued.

	{
	  "id": "a7a227fb-82ab-11e7-8186-0dc471ff0672",
	  "statusMessage": null,
	  "statusMessages": null,
	  "modifiedObjects": {},
	  "queuedTime": 1502905942150,
	  "startTime": -1,
	  "endTime": -1,
	  "state": "QUEUED",				<<<< CHECK STATUS
	  "links": {
	    "self": "https://ftd.example.com/api/fdm/v4/operational/deploy/a7a227fb-82ab-11e7-8186-0dc471ff0672"
	  }
	}

Use the GET /operational/deploy/{objId} resource to check the status of the job:

	curl -X GET \
	    --header 'Accept: application/json' \
	    https://ftd.example.com/api/fdm/v4/operational/deploy/a7a227fb-82ab-11e7-8186-0dc471ff0672


The state, DEPLOYED, indicates the job completed successfully.
	{
	  "id": "a7a227fb-82ab-11e7-8186-0dc471ff0672",
	  "statusMessage": null,
	  "statusMessages": null,
	  "modifiedObjects": {			   <<<< List of modified objects
	    "NetworkObject": [
	      "new-network"
	    ]
	  },
	  "queuedTime": 1502905942150,
	  "startTime": 1502905942463,
	  "endTime": 1502906010068,
	  "state": "DEPLOYED",				<<<< CHECK STATUS (can also be "DEPLOYING")
	  "links": {
	    "self": "https://ftd.example.com/api/fdm/v4/operational/deploy/a7a227fb-82ab-11e7-8186-0dc471ff0672"
	  }
	}


Read all URL objects:

	You can include query parameters to control the number of objects returned.
	Default is 10 objects from the start of the object list.

	curl -k -X GET \
	    -H 'Accept: application/json' \
	    -H 'Content-Type: application/json' \
	    -H 'Authorization: Bearer eyJ..snip..bN8' \
	    https://ftd.example.com/api/fdm/v4/object/urls


POST: Create a New Object

	curl -X POST \
	   --header 'Content-Type: application/json' \
	   --header 'Accept: application/json' -d '{ \
	   "name": "new_network_object", \
	   "description": "A subnet object created using the REST API.", \
	   "subType": "NETWORK", \
	   "value": "10.100.10.0/24", \
	   "type": "networkobject" \
	 }' 'https://ftd.example.com/api/fdm/latest/object/networks'


PUT: Modify an Existing Object

	Change the attributes of an object. The PUT method replaces the entire object.
	You cannot change one attribute. Ensure that your JSON object includes:
	- the old values that you want to preserve.
	- version (changes every time you modify an object)
	- id

	curl -X PUT \
	--header 'Content-Type: application/json' \
	--header 'Accept: application/json' -d '{ \
	   "version": "f6d8da48-7ed5-11e7-9bfd-d96183b5f5f1", \
	   "name": "new_network_object", \
	   "description": "A subnet object created using the REST API.", \
	   "subType": "NETWORK", \
	   "value": "10.100.11.0/24", \
	   "type": "networkobject" \
	 }' 'https://ftd.example.com/api/fdm/latest/object/networks/f6d8da49-7ed5-11e7-9bfd-27136f5686ad'


DELETE: Remove a User-Created Object

	You cannot delete system-defined objects or objects that are required to exist.
	You cannot delete an object that is used by another object (e.g. network object in an access rule).

	Successful calls (return code 204 “No Content”), send an empty response body.

	curl -X DELETE \
	--header 'Accept: application/json' \
	'https://ftd.example.com/api/fdm/latest/object/networks/f6d8da49-7ed5-11e7-9bfd-27136f5686ad'


+++ NOT executed, LAB 10.11, Synchronize Firepower Device Configuration +++

+++ #TODO, test basic things like: API explorer, get token, etc +++


=============================================


YANG
====
YANG:	Yet Another Next Generation
RFC 6020 YANG, a data modeling language for NETCONF.
More recently, independently of NETCONF and for protocols such as RESTCONF and gRPC.

http://www.yang-central.org/twiki/pub/Main/YangDocuments/Instant_YANG_NB.pdf

Characteristics YANG:
- Data modeling language (RFC 6020).
- Analogous to XML schema and SMI for SNMP.
- Models configuration, operational and RPC data:
	* Models configuration data.
	* Models state data of network elements.
	* Defines the format of event notifications.
	* Define the format of remote procedure calls.
- Provides semantics to better define data.
- Extensible and modular (constraints, reusable structures, built-in and derived types).
- Protocol independent. Can be converted into any encoding format, e.g. XML or JSON.

Note: SNMP SMI, Structure of Management Information. SMI is derived from ASN.1.

Tree-structured
- schema tree
- data tree (instance of schema tree)

Modules. May include multiple submodules.
Submodules. May belong to only one module.
Names of all standard (sub)modules must be unique. They share the same namespace.
A module can import data from other modules and include data from submodules:
- Import statement to reference external modules.
- Include statement to include submodules.

For a (sub)module to reference definitions in an external module, the external module must be imported.
For a module to reference definitions in one of its submodules, the module must include the submodule.
For a submodule to reference definitions in a 2nd submodule of the same module, the 1st submodule must include the 2nd submodule.


Example module header:

	module acme-module  {
	    namespace "http://acme.example.com/acme-module";
	    prefix acm;

	    import "ietf-yang-types"  { prefix yang; }
	    include "acme-system";

	    organization "ACME Inc.";
	    contact joe@acme.example.com;
	    description "Module describing the ACME products";
	    revision 2007-06-09  {
	        description "Initial revision.";
	    }
		<... snip ...>
	}


Example Module:

	module example-sports {

	  namespace "http://example.com/example-sports";
	  prefix sports;

	  import ietf-yang-types { prefix yang; }

	  typedef season {
	    type string;
	    description
	      "The name of a sports season, including the type and the year, e.g,
	       'Champions League 2014/2015'.";
	  }

	  container sports {
	    config true;

	    list person {
	      key name;
	      leaf name { type string; }
	      leaf birthday { type yang:date-and-time; mandatory true; }
	    }

	    list team {
	      key name;
	      leaf name { type string; }
	      list player {
	        key "name season";
	        unique number;
	        leaf name { type leafref { path "/sports/person/name"; }  }
	        leaf season { type season; }
	        leaf number { type uint16; mandatory true; }
	        leaf scores { type uint16; default 0; }
	      }
	    }
	  }
	}


Sources of data types:
- Built-in data types 		(RFC 6020)
- Derived data types 		(defined in module or imported from other module)
- Common Yang Data Types 	(RFC 6991; derived types):
	- "ietf-yang-types"  	(general data types)
	- "ietf-inet-types"  	(definitions for Internet protocol suite)


Built-in Types (RFC 6020):

   +---------------------+-------------------------------------+
   | Name                | Description                         |
   +---------------------+-------------------------------------+
   | binary              | Any binary data                     |
   | bits                | A set of bits or flags              |
   | boolean             | "true" or "false"                   |
   | decimal64           | 64-bit signed decimal number        |
   | empty               | A leaf that does not have any value |
   | enumeration         | Enumerated strings                  |
   | identityref         | A reference to an abstract identity |
   | instance-identifier | References a data tree node         |
   | int8                | 8-bit signed integer                |
   | int16               | 16-bit signed integer               |
   | int32               | 32-bit signed integer               |
   | int64               | 64-bit signed integer               |
   | leafref             | A reference to a leaf instance      |
   | string              | Human-readable string               |
   | uint8               | 8-bit unsigned integer              |
   | uint16              | 16-bit unsigned integer             |
   | uint32              | 32-bit unsigned integer             |
   | uint64              | 64-bit unsigned integer             |
   | union               | Choice of member types              |
   +---------------------+-------------------------------------+

	Reference built-in data types:
		type uint32;


ietf-yang-types (RFC 6991):

    +-----------------------+
    | YANG type             |
    +-----------------------+
    | counter32             |
    | zero-based-counter32  |
    | counter64             |
    | zero-based-counter64  |
    | gauge32               |
    | gauge64               |
    | object-identifier     |
    | object-identifier-128 |
    | yang-identifier       |
    | date-and-time         |
    | timeticks             |
    | timestamp             |
    | phys-address          |
    | mac-address           |
    | xpath1.0              |
    | hex-string            |
    | uuid                  |
    | dotted-quad           |
    +-----------------------+


ietf-inet-types (RFC 6991):

   +----------------------+
   | YANG type            |
   +----------------------+
   | ip-version           |
   | dscp                 |
   | ipv6-flow-label      |
   | port-number          |
   | as-number            |
   |                      |
   | ip-address           |
   | ipv4-address         |
   | ipv6-address         |
   | ip-address-no-zone   |
   | ipv4-address-no-zone |
   | ipv6-address-no-zone |
   | ip-prefix            |
   | ipv4-prefix          |
   | ipv6-prefix          |
   | domain-name          |
   | host                 |
   | uri                  |
   +----------------------+


To use/reference ietf-yang-types:
		import ietf-yang-types { prefix yang; }
		type yang:counter32;

To use/reference ietf-inet-types:
		import ietf-inet-types { prefix inet; }
		type inet:ip-address;


Derived Types (typedef)
-----------------------
YANG can define derived types from base types using the "typedef" statement.
A base type can be either a built-in type or a derived type.

Restrictions of the derived type must be the same as or stricter than the base type.
- Numeric restrictions: range and fraction digits substatements.
- String restrictions: length and pattern substatements.

Four substatements of the type statement:
- Range statement
- Length statement
- Pattern statement
- Fraction-digits statement (must be present in case of subtyping decimal64)

Example typedefs:

	typedef acl-num-type {
	  type int32 {
	    range "1..199 | 1300..2699";
	  }
	}

	typedef std-acl-num-type {
	  type acl-num-type {
	    range "min..99 | 1300..1999";
	  }
	}

	typedef ext-acl-num-type {
	  type acl-num-type {
	    range "100..199 | 2000..max";
	  }
	}

	typedef ext-acl-name-type {
	  type string {
	    length "1..50";
	    pattern "[a-zA-Z][a-zA-Z0-9-_]*";
	  }
	}


Example of fraction-digits substatement.

     typedef my-decimal {
       type decimal64 {
         fraction-digits 2;
         range "1 .. 3.14 | 10 | 20..max";
       }
     }


Example of union built-in type:

     typedef ip-address {
       type union {
         type inet:ipv4-address;
         type inet:ipv6-address;
       }

Example of enumeration built-in type:

    typedef ifType {
      type enumeration {
          enum ethernet;
          enum atm;
      }
    }

Example of union and enumeration types:

	typedef intf-ip-addr-type {
	  type union {
	    type inet:ip-address;
	    type enumeration {
	      enum "none";
	      enum "unnumbered";
	      enum "dhcp";
	    }
	  }
	}



YANG SYNTAX
===========

Yang statements:
- Leaf
- Leaf-list (of the same type)
- Container (contains various other types of statements)
- List
- Leafref   (points to data elsewhere in the data model)

- Grouping


Leaf
----
leaf loopback {
  type int32 {
    range "0..2147483647";
  }
}

Leaf substatements (optional):

config 			Configurable (true)
					Part of the reply to <get-config>, <copy-config> or <edit-config> request
				Operational (false)
					Can be part of reply to a <get>
default 		Default value (implies that leaf is optional)
mandatory 		Mandatory (true); optional (false). True implies a constraint.
must 			XPath constraint
type 			Data type
when 			Conditional leaf (present only if XPath expression is true)
description 	Human-readable description
reference 		Human-readable reference to other element or spec
units 			Human-readable unit (Hz, MBps, ℉, ...)
status 			current, deprecated, obsolete


Leaf-list
---------
Values in a leaf-list must be unique.

leaf-list loopback {
  type int32 {
    range "0..2147483647";
  }
}


Container
---------
Interior data node in the schema tree!

container loopback-ipv4 {
  leaf loopback {
    type int32 {
      range "0..2147483647";
    }
  }
  leaf ip-address {
    type inet:ipv4-address;
  }
}


List
----
Interior data node in the schema tree!

list loopback-ipv4 {
  key loopback;
  unique ip-address;
  leaf loopback {
    type int32 {
      range "0..2147483647";
    }
  }
  leaf ip-address {
    type inet:ipv4-address;
  }
}


Substatements for the list and leaf-list statements:
- Max-elements
- Min-elements
- Ordered-by

List entries are sorted by system or user:
- System: Natural order (numerically, alphabetically, ...).
- User:   Order in which the operator entered them is preserved (DNS server search order, firewall rules).


Leafref
-------
list services {
  key "ip port"
  leaf ip {
    type inet:ipv4-address;
  }
  leaf port {
    type uint16;
  }
}

container app {
  leaf address {
    type leafref {
      path "/services/ip";
    }
  }
  leaf port {
    type leafref {
      path "/services[ip=current()/../address]/port";
    }
  }
}

Note: Using "deref" might be simpler for complexer scenarios.


Choice
------
Set of alternatives. Only one may exist in the data tree.

choice route-distinguisher {
 case ip-address-based {
	 leaf ip-address {
	 	type ipv4-address;
	 }
	 leaf ip-address-number {
	 	type uint16;
	 }
 }
 case asn32-based {
	 leaf asn32 {
	 	type uint32;
	 }
	 leaf two-byte-number {
	 	type uint16;
	 }
 }
}


Grouping
--------
A reusable collection of nodes.
Enables 'code' reuse.
Reuse by "uses" keyword.


grouping app {
  leaf address {
    type inet:ip-address;
  }
  leaf port {
    type inet:port-number;
  }
}

container http-server {
  uses app {
    refine port {
      default 80;
    }
  }
}

container smtp-server {
  uses app {
    refine port {
      default 25;
    }
  }
}


Backward compatibility
----------------------
Two concepts:
- Revision control in module ("revision"). Import/include with "revision-date".
- Status statement (current, deprecated, obsolete)



XPath - Query Language
-----------------------
XML Path Language, defined by W3C.
YANG uses older version: XPath 1.0 (W3C recommens XPath 3.1, 2017, ...)

Query language:
- For selecting nodes from an XML document
- Compute values (strings, numbers, booleans) from the content of an XML document.

Results of path expressions:
- Node set (multiple elements from the XML document)
- String
- Number
- Boolean

XPath uses path expressions to select nodes in an XML document.
The node is selected by following a path or steps.

Path expressions:
	nodename	Selects all nodes with the name "nodename"
	/			Selects from the root node
	//			Selects nodes from the current node that match the selection no matter where they are
	.			Selects the current node
	..			Selects the parent of the current node
	*			Selects any element at a given level
	@			Selects attributes


Example XPath query:

	<Loopback>
	  <name>0</name>
	  <ip>
	    <address>
	      <primary>
	        <address>10.11.11.11</address>
	        <mask>255.255.255.255</mask>
	      </primary>
	    </address>
	  </ip>
	</Loopback>

	<GigabitEthernet>
	  <name>0/1</name>
	  <ip>
	    <address>
	      <primary>
	        <address>10.22.2.1</address>
	        <mask>255.255.255.252</mask>
	      </primary>
	    </address>
	  </ip>
	</GigabitEthernet>
	...

Select all IP addresses:

	//primary/address
			or
	/*/ip/address/primary/address

Select only GigabitEthernet IP addresses:

	//GigabitEthernet//primary/address
	//GigabitEthernet/ip/address/primary/address
	/GigabitEthernet/ip/address/primary/address 	<<< valid?

Select IP addresses of line card 0 interfaces:
	//GigabitEthernet[starts-with(name,'0')]//primary/address

Select IP addresses of first GigabitEthernet interfaces:
	//GigabitEthernet[1]//primary/address
		or
	//GigabitEthernet[position()=1]//primary/address


Examples for bookstore with books (https://www.w3schools.com/xml/xpath_examples.asp):

	Select all the titles				/bookstore/book/title
	Select the title of the first book 	/bookstore/book[1]/title
	Select all the prices 				/bookstore/book/price[text()]
	Select price nodes with price>35 	/bookstore/book[price>35]/price
	Select title nodes with price>35 	/bookstore/book[price>35]/title


XPath Functions (Node):
* count(node-set)				The number of nodes
* position()					The context position index in the node set
* last()						The index of the last node in the node set
* id(node)						The node by name

XPath Functions (String):
* string(node-set)				The value of the first node as string
* starts-with(string, prefix)	True if the string starts with the prefix
* contains(string, substring)	True if the string contains the substring
* string-length(string)			The length of string

XPath Functions (Number):
* number(node-set)				The value of the first node as number
* sum(node-set)					Sum of the numeric values of each node in the node-set.



Cisco IOS XE Data Model
-----------------------
https://github.com/YangModels/yang/tree/master/vendor/cisco/xe
https://github.com/YangModels/yang/tree/master/vendor/cisco/xe/1691	(version 16.9.1)


URI for the Cisco IOS XE full device configuration:
 https://<ip address switch>:443/restconf/data/native
or
 https://<ip address switch>:443/restconf/data/Cisco-IOS-XE-native:native 		<<< Module name is optional


NETCONF
-------
The IANA-assigned port for NETCONF-over-SSH sessions is 830.
Python client: ncclient


RESTCONF
-------
RESTCONF: HTTP-based Representational State Transfer Configuration Protocol

RESTCONF ~ NETCONF/YANG + HTTP(S)

Subset of NETCONF
Exposes YANG models via a REST API
HTTP(S) transport; XML or JSON encoding

RESTCONF uses same utilities and tools as REST, e.g.:
- curl
- Postman
- Firefox RESTClient
- Python requests module

Uses common HTTP verbs in REST APIs:
GET			Read 	(NOT used for Operations!)
PATCH		Update
PUT			Create or Replace
POST		Create or Operations (reload, default)
DELETE		Deletes the targeted resource
HEAD		Header metadata (no response body)

Note:
- PATCH adds configuration.
- PUT replaces configuration (declarative)


RESTCONF URI for config:		/restconf/data/<module name>:interfaces
RESTCONF URI for operations:	/restconf/operations/<module name>:flapinterface


Examples - RESTCONF
-------------------
https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/prog/configuration/168/b_168_programmability_cg/RESTCONF.html


RESTCONF Requests (HTTPS Verbs). RESTCONF request that shows the HTTPS verbs allowed on a targeted resource (logging monitor):

curl -i -k -X "OPTIONS" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native/logging/monitor/severity" \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin'

	HTTP/1.1 200 OK
	Server: nginx
	Date: Mon, 23 Apr 2018 15:27:57 GMT
	Content-Type: text/html
	Content-Length: 0
	Connection: keep-alive
	Allow: DELETE, GET, HEAD, PATCH, POST, PUT, OPTIONS    					>>>> Allowed methods
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Accept-Patch: application/yang-data+xml, application/yang-data+json
	Pragma: no-cache



POST (Create) Request. The POST operation creates a configuration (logging monitor alerts)

curl -i -k -X "POST" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native/logging/monitor" \
     -H 'Content-Type: application/yang-data+json' \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin' \
     -d $'{"severity": "alerts"}'

	HTTP/1.1 201 Created
	Server: nginx
	Date: Mon, 23 Apr 2018 14:53:51 GMT
	Content-Type: text/html
	Content-Length: 0
	Location: https://10.85.116.30/restconf/data/Cisco-IOS-XE-native:native/logging/monitor/severity 		<<< NEW RESOURCE !
	Connection: keep-alive
	Last-Modified: Mon, 23 Apr 2018 14:53:51 GMT
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Etag: 1524-495231-97239
	Pragma: no-cache


PUT: (Create or Replace) Request.   Command: logging monitor warnings
- If not present, the request creates it.
- If present, the request replaces it.

curl -i -k -X "PUT" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native/logging/monitor/severity" \
     -H 'Content-Type: application/yang-data+json' \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin' \
     -d $'{"severity": "warnings"}
     '
	HTTP/1.1 204 No Content
	Server: nginx
	Date: Mon, 23 Apr 2018 14:58:36 GMT
	Content-Type: text/html
	Content-Length: 0
	Connection: keep-alive
	Last-Modified: Mon, 23 Apr 2018 14:57:46 GMT
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Etag: 1524-495466-326956
	Pragma: no-cache


PATCH: (Update) Request.  Command: logging monitor informational

curl -i -k -X "PATCH" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native" \
     -H 'Content-Type: application/yang-data+json' \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin' \
     -d $'{"native": {
    		    "logging": {
    		      "monitor": {
    		        "severity": "informational"
    		      }
    		    }
    		  }
    		}'

	HTTP/1.1 204 No Content
	Server: nginx
	Date: Mon, 23 Apr 2018 15:07:56 GMT
	Content-Type: text/html
	Content-Length: 0
	Connection: keep-alive
	Last-Modified: Mon, 23 Apr 2018 15:07:56 GMT
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Etag: 1524-496076-273016
	Pragma: no-cache


GET Request (To Read). Command: logging monitor informational

curl -i -k -X "GET" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native/logging/monitor/severity" \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin'

	HTTP/1.1 200 OK
	Server: nginx
	Date: Mon, 23 Apr 2018 15:10:59 GMT
	Content-Type: application/yang-data+json
	Transfer-Encoding: chunked
	Connection: keep-alive
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Pragma: no-cache

	{
	  "Cisco-IOS-XE-native:severity": "informational"
	}


DELETE Request (To Delete the Configuration)
curl -i -k -X "DELETE" "https://10.85.116.30:443/restconf/data/Cisco-IOS-XE-native:native/logging/monitor/severity" \
     -H 'Content-Type: application/yang-data+json' \
     -H 'Accept: application/yang-data+json' \
     -u 'admin:admin'

	HTTP/1.1 204 No Content
	Server: nginx
	Date: Mon, 23 Apr 2018 15:26:05 GMT
	Content-Type: text/html
	Content-Length: 0
	Connection: keep-alive
	Last-Modified: Mon, 23 Apr 2018 15:26:05 GMT
	Cache-Control: private, no-cache, must-revalidate, proxy-revalidate
	Etag: 1524-497165-473206
	Pragma: no-cache


RESTCONF Responses
------------------
200 OK			 				Success. Response body contains representation of the resource.
201 Created				 		Resource was created; new resource URI is in the "Location" header.
204 No Content			 		The request was successfully completed, but no response body is returned.

400 Bad Request			 		Missing or invalid information.
401 Unauthorized			 	The request requires user authentication.
403 Forbidden			 		Access denied, due to authorization rules.
404 Not Found			 		The requested resource does not exist.
405 Method Not Allowed			The HTTP method is not supported for this resource.
406 Not Acceptable			 	The resource can not generate the requested representation specified in the "Accept" header.
409 Conflict				 	Request tries to create a resource that exists.
415 Unsupported Media Type		The format of the request is not supported.

500 Internal Error			 	The server encountered an unexpected condition.
501 Not Implemented				The server does not support the functionality.
503 Unavailable					The server is currently unable to handle the request.



More IOS XE RESTCONF examples
-----------------------------
Full device configuration:
	GET https://172.16.10.66:443/restconf/data/native/

List of GigabitEthernet interfaces:
	GET https://172.16.10.66:443/restconf/data/native/interface/GigabitEthernet?fields=name

Get interface attributes of GigabitEthernet0/0/2 (note the %2F for /):
	GET https://172.16.10.66:443/restconf/data/native/interface/GigabitEthernet=0%2F0%2F2

Get the primary IP address of the GigabitEthernet interfaces:
	GET https://172.16.10.66:443/restconf/data/native/interface/GigabitEthernet?fields=ip/address/primary



RESTCONF lab (Cisco CSR1000v)
-----------------------------

! Enable RESTCONF interface.
!
conf t
restconf

! Enable HTTPS server
!
ip http secure-server

---

Postman:

# Get the full configuration
#
https://10.0.0.20:443/restconf/data/native

# Set the authorization to basic (username and password)
#
Authorization: Basic ....

# Set the Header key Accept to "application/yang-data+json"
#
Accept: application/yang-data+json


# Get the configurations of the interfaces.
#
https://10.0.0.20:443/restconf/data/native/interface


# Get the configurations of the 3rd GigabitEthernet interfaces.
#
https://10.0.0.20:443/restconf/data/native/interface/GigabitEthernet=3


# Get the IP address of the first GigabitEthernet interface
#
https://10.0.0.20:443/restconf/data/native/interface/GigabitEthernet=1/ip/address

---

# Change the IP address of the 3rd GigabitEthernet interface


# Set the Header key Accept to "application/yang-data+json"
#
Accept: application/yang-data+json
Content-Type: application/yang-data+json

Json Raw format:
{
   "address": "192.168.0.99",
   "mask": "255.255.255.0"
}

Post to:
https://10.0.0.20:443/restconf/data/native/interface/GigabitEthernet=3/ip/address/primary

---

Refer restconf.py for examples with python requests library. Some basics:

	response = requests.get(url_gig1, auth=(USER, PASS), headers=headers, verify=False)
	response.text


	headers = {'Content-Type': 'application/yang-data+json',
               'Accept': 'application/yang-data+json'}

    payload = {
        "Cisco-IOS-XE-native:address": {
            "primary": {
                "address": "192.168.0.66",
                "mask": "255.255.255.0"
            }
        }
    }

    response = requests.put(url_gig2, auth=(USER, PASS), headers=headers, verify=False, data=json.dumps(payload))
	response.status_code



Model-Driven Telemetry
----------------------
MDT: Model-Driven Telemetry
Subscription model (subscribe to YANG datastore)
Streaming data

Session:	 	Specifies destinations to collect streamed data.
				Two Modes:
				- Dial-in: Subscribed receiver dials in to the router. Router is server; receiver is client. Dynamic subscription terminates when the receiver cancels the subscription or when the session terminates.
				- Dial-out: Router dials out to the subscribed receivers (default mode). Router is client; receiver is server. Dial-out mode is persistent. If a session terminates, the router continually attempts to re-establish every 30 seconds.
Sensor path:	YANG path to data definitions.
Subscription:	Links sensor-paths and destinations. Specifies streaming criteria.
Transport:		Protocols:
				- TCP/UDP			(dial-out)
				- NETCONF/YANG push	(dial-out)
				- gNMI				(dial-in and dial-out)
				- gRPC				(dial-in and dial-out)
Encoding:		- JSON
				- GPB (Google Protocol Buffers) (compiled .proto files metadata; describe the GPB message format)
				  - Compact GPB encoding
				  - KV-GPB encoding (Key-value or self-describing”)


Streaming Telemetry with gNMI
-----------------------------
gNMI:	gRPC Network Management Interface
gRPC:	Google remote procedure call

gNMI:
- Management protocol for streaming telemetry and configuration management.
- Protocol for configuration manipulation and state retrieval.
- Opensource: OpenConfig community.
- Transport protocol is gRPC, built on top of HTTP/2.
- Alternative to NETCONF/RESTCONF.
- Functional subset of NETCONF. But: improved security and support for bidirectional streaming.
- Data can be modeled using YANG (but, no requirement).


gNMI RPCs:
- Capability request/response. Handshake to exchange capabilities (models, encoding, version, extensions).
- GET request/response. Retrieve data on the target.
- SET request/response. Modify state of the target (all changes are considered as a part of one transaction).
- Subscribe Request/Response. Subscribe to data on the target:
	- Dedicated stream to return once-off data (ONCE).
	- Periodically a set of data (POLL).
	- Long-lived stream of data.

Cisco-specific gRPC operations for gNMI:
- GetConfig
- MergeConfig
- DeleteConfig
- ReplaceConfig
- CommitConfig
- ConfigDiscardChanges
- CliConfig 				Merge configuration data in CLI format.
- GetOper 					Get operational data.
- ShowCmdTextOutput 		Show CLI command output data.


=============================================


Databases
=========
ERM 	Entity relationship modeling
ERD 	entity relationship diagram
LDM		logical data model
PDM		physical data model

Atomicity, Consistency, Isolation, Durability (ACID Compliance; e.g. Relational DB).
Consistency, Availability, Partition (CAP) theorem.

Entity, Attributes, Relationship
weak entities (foreign key)
cardinality, modality

Crow’s Foot Notation
->O 	zero or more
->|		1 or more
-||		1 and only 1 (exactly one)
-|0		zero or 1


DB Types
- Relational
- Key-value database
- Document-based database
- Graph-based database
- Column-based database
- Time-series database


Non-relational DBs:
- Scalability and flexibility.
- 3 Vs: volume, velocity, and variety.
- Denormalization, Aggregation, Data distribution (sharding and replication).


Key-value DB:
- Simplicity
- Speed
- Scalability

- Values commonly stored as BLOB (Binary Large Object).
- Search by key.
- You cannot search by values.
- Range-based queries are not supported.


Document-based DB (document store):
- Subclass of key-value databases
- Semi-structured data
- Collections of documents.
- Documents similar to programming objects.
- Document is a member of only one collection.
- Embedded documents vs. referenced documents.
- Quick iterations and frequent code pushes (agile sprints)
- Fast inserts
- Easy indexing
- Loose schema
- Evolving very fast
- Examples: MongoDB, Amazon DynamoDB
- Query operators allow for functionalities like SQL: $gt, $lt, $all, $size, $elemMatch

Cons:
- Less structured data.
- Queries cannot leverage relations and joins.
- Not ACID compliant (by default, can be changed).
- Less explored and documented than relational databases.

Example:
	db.createCollection( “Persons“ )
	db.Persons.insertOne( { name: "Eric", location: "NL", age: 24 } )
	db.Persons.insertMany( [ { name: "Erica", location: "NL", age: 36 },
							 { name: "Jeromeke", location: "BE", age: 17 }
						 ] )
	db.Persons.find( { location: "NL" } )
	db.Person.find( { “children”: { $size: 3 } } )

	db.Inventory.updateMany( { type: ”GPU” }, { $set: { stock: true } } )

	db.Inventory.deleteMany( { material: [ “Plastic”, “Iron” ] } )


Analysing/processing big amounts of data:
- Elasticsearch: Scalable, distributed, full-text search and analytics engine (open-source).
- MapReduce: Programming model and implementation. Uses parallelism and load distribution.
- Both use inverted index concept. Moreover: data sharding and replication.
- MapReduce stages: split, map, shuffle, and reduce.



Graph-based DB:
- nodes and edges (both having properties)
- example: Neo4j

Useful for:
- Social graph
- Intent graph
- Consumption graph
- Interest graph
- Mobile graph

Not suitable for:
- Processing high volumes of transactions
- Data warehouses
- Large volume analytical queries
- Mass analytics across all relations



OLTP	Online transaction processing
OLTA	Online transaction analysis


Columnar-based DB:
- Range-based queries
- Aggregate operations (AVG, SUM, MIN, MAX)

OLTP	row-based database
OLTA	colums-based database

Data warehouses
Business intelligence (BI)
Big data
Library card catalog
Ad-hoc query systems

Examples: Cassandra, HBase


Time-series DB (TSDB):
- Essentially key-value DB.
- Key represents time/date.
- Value is a metric (a measurement), system state, or something more complex.
- Single value or a collection of values.
- Exceptionally fast and scalable.
- ACID compliance is not enforced (measurement can be lost).
- Simple insert commands at regular time intervals.
- Data points are immutable.
- Update statements should not be used.
- Smoothing techniques create new representations of time series (original data is preserved).
- Time series data is usually very compressed.
- Used in:
	- System monitoring
	- Finances
	- IoT
	- Asset management
	- Machine learning
- Round Robin Databases + Consolidation
- Implemented either as a relational database (TimescaleDB) or as a NoSQL database (InfluxDB).



Example TimescaleDB
-------------------
Time-series DB with PostgreSQL engine

SELECT COUNT(*) FROM rides;

EXPLAIN ANALYSE SELECT date_trunc('hour', pickup_datetime) AS timeframe, COUNT(*) FROM rides GROUP BY timeframe ORDER BY timeframe;

	EXPLAIN ANALYSE
	SELECT date_trunc('hour', pickup_datetime) as timeframe,

	COUNT(*) FROM rides
	GROUP BY timeframe
	ORDER BY timeframe;


Same query using time bucket aggregation tool (10% faster):

EXPLAIN ANALYSE SELECT time_bucket('5 minutes', pickup_datetime) AS timeframe, COUNT(*) FROM rides GROUP BY timeframe ORDER BY timeframe;

	EXPLAIN ANALYSE
	SELECT time_bucket('5 minutes', pickup_datetime) AS timeframe,

	COUNT(*) FROM rides
	GROUP BY timeframe
	ORDER BY timeframe;



MongoDB Exercise
----------------
# Select the DEVNET database
use DEVNET

# List the collections
db.getCollectionNames()

# Display a document from the collection
db.zips.findOne()

# List the number of documents
db.zips.find().count()

# Now count the number of documents, having non-null values
db.zips.find({$and:[{city:{$exists:true}},{loc:{$exists:true}},{pop:{$exists:true}},{state:{$exists:true}}]}).count().

	db.zips.find(
		{$and:
			[
				{city:{$exists:true}},
				{loc:{$exists:true}},
				{pop:{$exists:true}},
				{state:{$exists:true}}
			]
		}
	).count().


# Get the total population of each state.
#
db.zips.aggregate([{$group:{"_id":"$state", "total_pop":{"$sum":"$pop"}}}])

	db.zips.aggregate(
		[
			{
				$group:{"_id":"$state", "total_pop":{"$sum":"$pop"}}
			}
		]
	)

# Iterate through the rest of the output.
it


# Display population of the western part of the USA (left of the 100th meridian)
# Sort results alphabetically
#
db.zips.aggregate([{"$match":{"loc.0":{"$lt": -100}}}, {"$group":{"_id":"$state", "total_pop":{"$sum":"$pop"}}}, {"$sort": {"_id": 1}}])

	db.zips.aggregate(
		[
			{"$match":{"loc.0":{"$lt": -100}}},
			{"$group":{"_id":"$state", "total_pop":{"$sum":"$pop"}}},
			{"$sort": {"_id": 1}}
		]
	)


db.zips.mapReduce(function(){if (this.loc[0] < -100{emit(this.state, this.pop);})},function(state, pop){return Array.sum(pop);}, {out: "Total_Population_West"})

	db.zips.mapReduce(
		function() {
			if (
				this.loc[0] < -100 {
					emit(this.state, this.pop);
				}
			)
		},
		function(state, pop){
			return Array.sum(pop);
		},
		{out: "Total_Population_West"}
	).


Neo4j Graph Database
--------------------
MATCH (n) RETURN n

Display the Bond actors:
- with the movies they played in
- who played in the most Bond movies

MATCH (p:People)-[:AS_BOND_IN]->(m:Film)
RETURN p.Name AS ActorName, count(p.Name) AS BondMovies ORDER BY BondMovies DESC


Display the most commonly used car brand that Sean Connery drove.

MATCH (p:People)-[:AS_BOND_IN]->(m:Film)-[:HAS_VEHICLE]->(v:Vehicle)
WHERE p.Name='Sean Connery'
RETURN p.Name as Actor, v.Brand as Car, COUNT(p.Name) as Count
ORDER BY COUNT(p.Name) DESC LIMIT 1)



+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-



Setting up your Linux/Ubuntu workstation
----------------------------------------
# Basic Linux tools and utilities
# wget is already installed
#
sudo apt-get install curl


# Postman
# Install via snap. If not installed yet: sudo apt install snapd
sudo snap install postman


# Developer tools and utilities:
# - Equivalent to openssl-dev on other distributions
# - For example the gcc C compiler
#
sudo apt-get install libssl-dev
sudo apt-get install build-essential


# Git
#
sudo apt-get install git
git --version


# Python
#
sudo apt-get install python3
sudo apt-get install python3-pip


# Python3-venv
sudo apt-get install python3-venv

# Create a virtual environment
# python3 -m venv <virtualenv>
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate
python -V

# Deactivate virtual environment
deactivate


# Install Nodejs
sudo apt-get install nodejs
# Install NPM (Node Package Manager)
sudo apt-get install npm
nodejs -v
npm -v


# ngrok
cd /opt
sudo wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
sudo unzip ngrok-stable-linux-amd64.zip
sudo mv ngrok /usr/local/bin

# Start an ngrok tunnel.
ngrok http 6767


# Google Chrome
installed via Software installer...


# install OpenConnect (open source VPN client)
sudo apt-get install openconnect
openconnect -b <ipAddress>

sudo ps -ax | grep openconnect
sudo kill <openconnect PID>



# Docker
#
# Install HTTPS plugins
#
sudo apt install apt-transport-https ca-certificates curl software-properties-common

# Ddd the GPG key for the official Docker repository to the system
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

# Add the Docker repository to APT sources to always check for the latest version.
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
	or
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable edge test"

# Update the package database with the Docker packages from the newly added repo
# Install Docker Community Edition.
#
sudo apt update
sudo apt install docker-ce

# Set up group permissions so you don’t need sudo for every docker command.
# Add your user to the docker group.
#
sudo usermod -aG docker <user>

# Verify Docker
sudo systemctl status docker
docker run busybox
docker ps -a


Git
---
git clone https://github.com/CiscoDevNet/dnav3-code

git status
git diff


# Create your own branch locally (called mycode)
git checkout -b mycode

# Refresh your local repository from the one on GitHub.
# Update your local repository with the remote updates.
git fetch


# Rollback all changes in a specific file.
# Checkout the last version of the file.
git checkout <changed file>

# Revert all files.
# Reset your working directory to the last commit.
git reset --hard

# Delete a branch.
git branch --delete --force <branch name>


# Set global settings
git config --global user.name "Tisipi"
git config --global user.email "Tisipi@tisipi.nl"

# List global configured settings
git config --list


# Add a specific file to staging area
git add <file>

# Add files from current directory to staging area
git add .


# Commit files in staging area
git commit -m "Initial commit"


# Combined add and commit
git commit -am "msg"



API
---
HTTP Verb	Typical Action (CRUD)
POST		Create
GET			Read
PUT			Update
PATCH		Update
DELETE		Delete

Status Code	Status Message	Meaning
200	OK						All looks good
201	Created					New resource created
400	Bad Request				Request was invalid
401	Unauthorized			Authentication missing or incorrect
403	Forbidden				Request was understood but not allowed
404	Not Found				Resource not found
500	Internal Server Error	Something wrong with the server
503	Service Unavailable		Server is unable to complete request


curl -X		followed by a request verb such as GET, PUT, POST, PATCH, or DELETE.
curl -H 	followed by a header such as a token to send with the requests.


curl https://deckofcardsapi.com/api/deck/new/ | python -m json.tool
curl https://deckofcardsapi.com/api/deck/new/ | python3 -m json.tool

curl https://deckofcardsapi.com/api/deck/i5esttp06r1z/shuffle/
curl https://deckofcardsapi.com/api/deck/i5esttp06r1z/draw/?count=3 | python -m json.tool
curl https://deckofcardsapi.com/api/deck/9ujz1q9z2hnk/draw/?count=3 | python -m json.tool

https://deckofcardsapi.com/api/deck/{{deck_id}}/draw/?count=3


curl https://api.ciscospark.com/v1/people/me

curl https://api.ciscospark.com/v1/messages -X POST -H "Authorization:Bearer NDc0OWI3NWMtNjc3ZS00ZDM4LWE3MzktNWYyYmMxYWY2YjIxMWQ0ZjI5ZjAtM2Yz_PF84_e1a492ed-2b3b-4ab5-b37e-e7a31a6a4d57" --data "toPersonEmail=myEmail@email.nl" --data "text=Hi%20from%20DevNet"

curl https://api.ciscospark.com/v1/teams -X GET -H "Authorization:Bearer M2RiZWQyMDItMDdhYi00NTA1LTgwOTQtMjUzMTc4YThhZDI4YjU4NmJmNmMtZWQ3_PF84_e1a492ed-2b3b-4ab5-b37e-e7a31a6a4d57"

curl -X GET https://api.ciscospark.com/v1/teams -H "Authorization:Bearer M2RiZWQyMDItMDdhYi00NTA1LTgwOTQtMjUzMTc4YThhZDI4YjU4NmJmNmMtZWQ3_PF84_e1a492ed-2b3b-4ab5-b37e-e7a31a6a4d57"



DNA - Digital Network Architecture
----------------------------------
Cisco SDN controller for the enterprise

Cisco DNA Center FAQ:
	https://www.cisco.com/c/en/us/products/collateral/cloud-systems-management/dna-center/nb-06-dna-center-faq-cte-en.html
	https://developer.cisco.com/dnacenter/
	https://developer.cisco.com/docs/dna-center/#!cisco-dna-center-platform-overview

Cisco DNA Center workflow:
- Design
- Policy (Business and application intent)
- Provision
- Assure (via Network telemetry and sensor capabilities)

SDA??


DNA lab
-------
git clone https://github.com/CiscoDevNet/dnav3-code
cd dnav3-code
python3 -m venv venv
source venv/bin/activate
cd intro-dnac/
pip install -r requirements.txt


YANG
----
# Validate a YANG module
pyang ietf-interfaces.yang

# Generate tree view of Yang model.
pyang -f tree ietf-interfaces.yang


Linux
-----
# IP in IP tunnel
ip link add name <if_name> type ipip local <local_ip> remote <remote_ip>
ip link set <if_name> up

ip addr show dev <if_name>
ip addr add <ip_address> dev <if_name>
ip addr del <ip_address> dev <if_name>

ip route add <subnet> dev <if_name>
ip route replace <subnet> dev <if_name>
route -n

ping -c 5 10.10.10.1


service ntp status
cat /etc/ntp.conf


# Generate SSH keys (private and public)
ssh-keygen -t rsa

# Copy public keys to clients
#
ssh-copy-id -p <port> <username>@<server IP>
