
Python
======

python --version
python -V
python2 --version
python3 --version


shebang line examples
#!/usr/bin/env python3
#!/usr/bin/python


Ubuntu:
ls /usr/lib/python3.8/dist-packages
Manjaro:
ls /usr/lib/python3.8


__init__.py


https://pypi.org/


pip3 install --upgrade pip
pip3 --version

brew update
brew upgrade
brew install python
brew upgrade python

pip install robotframework
pip check robotframework
pip show robotframework
robot --version

intellibot lte2000 (IntelliBot #patched) <<<

(Robocorp and Robot Framework Language Server (RFLS) plugin)

robot .



Python 3 Module of the Week
---------------------------
https://pymotw.com/3/



Basic I/O
---------
print("Hello")
print("Hello {}".format(name))

name = input("Please type your name: ")


Remark
- In Python 2 raw_input should be used to input a string
- input(x) is eval(raw_input(x)).


# Linux env variables
env
env | grep PATH
echo $PATH
export PATH=/bin:/usr/bin
# Expand PATH var
export PATH=$PATH:/bin/eric
# Define env var
export TEST_VAR="Yes Yes"

os.environ["PATH"]
os.environ.get("PATH")
os.environ.get("PATH", "")

my_env = os.environ.copy()
my_env['PATH'] = os.pathsep.join(["/opt/niceapp", my_env['PATH']])


sys.exit(0)		default exit code python script
sys.exit(1)		exit code error


FYI:
/etc/profile 	This file sets the environment variables at startup of the Bash shell.
/etc/profile.d  This directory contains application-specific startup files executed at startup of the shell.



sys
---
import sys

sys.stdin

sys.argv
sys.argv[0], sys.argv[1], etc.


./myCommands.py simple_commands.txt

	import sys
	import subprocess

	cmdfile = sys.argv[1]
	with open(cmdfile) as file:
	    for line in file:
	        cmd = line.strip()
	        subprocess.run([cmd])



subprocess
----------
https://docs.python.org/3/library/subprocess.html

subprocess.run(["ls"])
subprocess.run(["ps"])
subprocess.run(["date"])
subprocess.run(["sleep", "5"])
subprocess.run(["ping", "8.8.8.8"])
subprocess.run(["echo", old_file_name, new_file_name])
subprocess.run(["mv", old_file_name, new_file_name])


result = subprocess.run(["ls"])
result.returncode

result = subprocess.run(["ls"], capture_output=True)
result.stdout
result.stdout.decode().split('\n')
result.stderr


Misc
----
split()
strip()
word.lower()
word.isalpha()
replace()

punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
new_file_contents = ''.join(char for char in file_contents if char not in punctuations)


department_list.count("IT")



read/write files
----------------
file = open('test.txt')
file.close()

with open('test.txt') as file:
    for line in file:
        print(line.strip())

line = file.readline()
next_line = file.readline()
rest_of_file = file.read()

lines = file.readlines()


modes:
'r'	open for reading (default)
'w'	open for writing, truncating the file first !!!
'x'	open for exclusive creation, failing if the file already exists
'a'	open for writing, appending to the end of the file if it exists
'b'	binary mode
't'	text mode (default)
'+'	open for updating (reading and writing)

--

Note: Byte Order Mark or BOM is used in UTF-16 to discriminate between Little-endian and Big-endian.
If your file is in UTF-8 you don't need BOM markers. To remove them use UTF-8-sig:

	with open(file_path, encoding='utf-8-sig') as csv_file:

https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/


+++
To be checked:
https://realpython.com/python-concurrency/
https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32
https://stackoverflow.com/questions/33047452/definitive-list-of-common-reasons-for-segmentation-faults
https://sites.google.com/a/case.edu/hpcc/home/important-notes-for-new-users/debugging-segmentation-faults

Readable Python code on GitHub:
https://github.com/fogleman/Minecraft
https://github.com/cherrypy/cherrypy
https://github.com/pallets/flask
https://github.com/tornadoweb/tornado
https://github.com/gleitz/howdoi
https://github.com/bottlepy/bottle/blob/master/bottle.py
https://github.com/sqlalchemy/sqlalchemy

https://blog.rescuetime.com/how-to-prioritize/

https://landing.google.com/sre/sre-book/chapters/effective-troubleshooting/

+++


os
--
https://docs.python.org/3/library/os.html
https://docs.python.org/3/library/os.path.html

os.getcwd()
os.mkdir('temp_dir')
os.chdir('temp_dir')
os.rmdir('temp_dir')
os.listdir('/')
os.listdir(path='.')

os.scandir(path='.')
	Returns an iterator

	Using scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information


	with os.scandir(path) as it:
	    for entry in it:
	        if not entry.name.startswith('.') and entry.is_file():
	            print(entry.name)


os.remove('my.txt')
os.rename('draft.txt', 'final.txt')
os.path.exists('draft.txt')

os.path.abspath('test_math.py')
os.path.isfile(file)
os.path.isdir(file)
os.path.join(dir, name)
os.path.dirname(path)
os.path.expanduser('~')

file, ext = os.path.splitext(infile)

os.stat('somefile.txt')


# File size in bytes
os.path.getsize('draft.txt')

# Last modified time (unix time)
os.path.getmtime('draft.txt')

# Return the time of last access of path
os.path.getatime(path)


os.environ
os.environ['HOME']
os.environ['PATH']

os.getenv('HOME')
os.getenv('PATH')
os.putenv(key, value)


os.getpid()
os.getppid()
os.uname()

os.sep
os.linesep
os.pathsep


csv
---
https://docs.python.org/3/library/csv.html
https://realpython.com/python-csv/

Note: Files below should be opened with newline=''
		with open('person.csv', newline='') as csvfile:


with open('person.csv', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        name, gender, age = row

persons = [['Jan', 'Man', 32], ['Piet', 'Man' ,45], ['Joke' ,'Vrouw', 34]]
with open('my_persons.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerows(persons)


with open('personHeader.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(row['Name'], row['Age'])


persons = [{'Name': 'Jan', 'Gender': 'Man', 'Age': 27},
           {'Name': 'Piet', 'Gender': 'Man' ,'Age': 45},
           {'Name': 'Joke' ,'Gender': 'Vrouw', 'Age':34}]
with open('my_persons_header.csv', 'w') as f:
    keys = ['Name', 'Gender', 'Age']
    writer = csv.DictWriter(f, fieldnames=keys)
    writer.writeheader()
    writer.writerows(persons)


csv.register_dialect('myDialect', skipinitialspace=True, strict=True)
csv.DictReader(f, dialect = 'myDialect')



math
----
math.pi


datetime
--------
unixtime = os.path.getmtime('test_math.py')
datetime.datetime.fromtimestamp(unixtime)
#datetime.datetime(2020, 10, 9, 17, 30, 52, 145723)


shutil
------
du = shutil.disk_usage("/")
du.free / du.total
du.used / du.total


psutil
------
psutil.cpu_percent()
psutil.cpu_percent(0.1)

psutil.disk_io_counters()
psutil.net_io_counters()


requests
--------
# sudo apt install python3-requests
request = requests.get("http://www.google.com")
request.status_code == 200


socket
------
localhost = socket.gethostbyname('localhost') # Returns '127.0.0.1'


re
--
https://www.regex101.com
https://regexcrossword.com/
https://docs.python.org/3/howto/regex.html
https://docs.python.org/3/library/re.html

Metacharacters: . ^ $ * + ? { } [ ] \ | ( )
Metacharacters must be escaped.

.		Any char except a newline character (in alternate mode (re.DOTALL) it matches even a newline)
?		Optional char (0 or 1 char)
*		0 or more instances
+		1 or more instances
^		Matches at the beginning of string/line (re.MULTILINE flag)
$		Matches at the end of a string/line
[]		Character class (a set of characters). For example:
			[a-z]
			[A-Z]
			[0-9]
			[a-zA-Z]
			[^a-zA-Z]
			[a-zA-Z0-9]
			[\s,.] 		match any whitespace character or ',' or '.'.

|		Alternation, “or” operator, e.g. [man|woman]
{n}		Repetition, n times
{n,m}	Repetition, n to m times (incl.)
{n,}	Repetition, n or more times
{,m}	Repetition, zero up to m times.


\d		Matches any decimal digit; this is equivalent to the class [0-9].
\D		Matches any non-digit character; this is equivalent to the class [^0-9].
\s		Matches any whitespace character; this is equivalent to the class [ \t\n\r\f\v].
\S		Matches any non-whitespace character; this is equivalent to the class [^ \t\n\r\f\v].
\w		Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].
\W		Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].

\b		Word boundary. Matches only at the beginning or end of a word.


match()		Determine if the RE matches at the beginning of the string.
search()	Scan through a string, looking for any location where this RE matches.
findall()	Find all substrings where the RE matches, and returns them as a list.
finditer()	Find all substrings where the RE matches, and returns them as an iterator.

split()		Split the string into a list, splitting it wherever the RE matches.
sub()		Find all substrings where the RE matches, and replace them with a different string.
subn()		Does the same thing as sub(), but returns the new string and the number of replacements.


re.search(r"man", "mankind")
re.search(r"man", "Mankind", re.IGNORECASE)
re.search(r"\.nl", "tisipi.nl")

re.findall(r"man|woman", "Both man and woman")

pattern = re.compile(r"man|woman")
pattern.findall("Both man and woman")

mo = re.search(r"\[(\d+)\]", "This is a PID [1234] maestro")
mo.group()
mo.group(0)
mo.groups()


result = re.search(r" () () ", <string>)
result[1]
result[2]


re.split(r"[.!]", "Split my sentence. This one too! Is it working?")
re.split(r"([.!])", "Split my sentence. This one too! Is it working?")

p = re.compile('(blue|white|red)')
p.sub('colour', 'blue socks and red shoes')
p.sub('colour', 'blue socks and red shoes', count=1)

re.sub(r"\d+.\d+.\d+.\d+", "[HIDDEN]", "The customers IP address 11.6.6.7 must not be visible")


unittest
--------
https://docs.python.org/3/library/unittest.html#basic-example
https://docs.python.org/3/library/unittest.html#command-line-interface
https://docs.python.org/3/library/unittest.html#organizing-test-code
https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertRaises


Create module called <module>_test.py:

	import unittest

	# Test Class
	class TestWhatever(unittest.TestCase):

		# Test methods
	    def setUp(self):
	        self.widget = Widget('The widget')

	    def tearDown(self):
	        self.widget.dispose()

	    def test_whatever1(self):
	    	self.assertEqual(..., ...)

	    def test_whatever2(self):

	if __name__ == '__main__':
	    unittest.main()


assertEqual(a, b)			a == b
assertNotEqual(a, b)		a != b
assertTrue(x)				bool(x) is True
assertFalse(x)				bool(x) is False
assertIs(a, b)				a is b
assertIsNot(a, b)			a is not b
assertIsNone(x)				x is None
assertIsNotNone(x)			x is not None
assertIn(a, b)				a in b
assertNotIn(a, b)			a not in b
assertIsInstance(a, b)		isinstance(a, b)
assertNotIsInstance(a, b)	not isinstance(a, b)

assertRaises(exc, fun, *args, **kwds)			fun(*args, **kwds) raises exc
assertRaisesRegex(exc, r, fun, *args, **kwds)	fun(*args, **kwds) raises exc and the message matches regex r
assertWarns(warn, fun, *args, **kwds)			fun(*args, **kwds) raises warn
assertWarnsRegex(warn, r, fun, *args, **kwds)	fun(*args, **kwds) raises warn and the message matches regex r
assertLogs(logger, level)						The with block logs on logger with minimum level

assertAlmostEqual(a, b)		round(a-b, 7) == 0
assertNotAlmostEqual(a, b)	round(a-b, 7) != 0
assertGreater(a, b)			a > b
assertGreaterEqual(a, b)	a >= b
assertLess(a, b)			a < b
assertLessEqual(a, b)		a <= b
assertRegex(s, r)			r.search(s)
assertNotRegex(s, r)		not r.search(s)
assertCountEqual(a, b)		a and b have the same elements in the same number, regardless of their order.

assertMultiLineEqual(a, b)	strings
assertSequenceEqual(a, b)	sequences
assertListEqual(a, b)		lists
assertTupleEqual(a, b)		tuples
assertSetEqual(a, b)		sets or frozensets
assertDictEqual(a, b)		dicts


assert isinstance(item, str), "Must be a string"


Command line examples:
	python -m unittest -h
	python -m unittest test_module
	python -m unittest -v test_module
	python -m unittest test_module1 test_module2
	python -m unittest test_module.TestClass
	python -m unittest test_module.TestClass.test_method


Exceptions
----------
raise ValueError
raise NameError('This is an error')


Built-in Exception hierarchy:

	BaseException
	 +-- SystemExit
	 +-- KeyboardInterrupt
	 +-- GeneratorExit
	 +-- Exception
	      +-- StopIteration
	      +-- StopAsyncIteration
	      +-- ArithmeticError
	      |    +-- FloatingPointError
	      |    +-- OverflowError
	      |    +-- ZeroDivisionError
	      +-- AssertionError
	      +-- AttributeError
	      +-- BufferError
	      +-- EOFError
	      +-- ImportError
	      |    +-- ModuleNotFoundError
	      +-- LookupError
	      |    +-- IndexError
	      |    +-- KeyError
	      +-- MemoryError
	      +-- NameError
	      |    +-- UnboundLocalError
	      +-- OSError
	      |    +-- BlockingIOError
	      |    +-- ChildProcessError
	      |    +-- ConnectionError
	      |    |    +-- BrokenPipeError
	      |    |    +-- ConnectionAbortedError
	      |    |    +-- ConnectionRefusedError
	      |    |    +-- ConnectionResetError
	      |    +-- FileExistsError
	      |    +-- FileNotFoundError
	      |    +-- InterruptedError
	      |    +-- IsADirectoryError
	      |    +-- NotADirectoryError
	      |    +-- PermissionError
	      |    +-- ProcessLookupError
	      |    +-- TimeoutError
	      +-- ReferenceError
	      +-- RuntimeError
	      |    +-- NotImplementedError
	      |    +-- RecursionError
	      +-- SyntaxError
	      |    +-- IndentationError
	      |         +-- TabError
	      +-- SystemError
	      +-- TypeError
	      +-- ValueError
	      |    +-- UnicodeError
	      |         +-- UnicodeDecodeError
	      |         +-- UnicodeEncodeError
	      |         +-- UnicodeTranslateError
	      +-- Warning
	           +-- DeprecationWarning
	           +-- PendingDeprecationWarning
	           +-- RuntimeWarning
	           +-- SyntaxWarning
	           +-- UserWarning
	           +-- FutureWarning
	           +-- ImportWarning
	           +-- UnicodeWarning
	           +-- BytesWarning
	           +-- ResourceWarning


Python Try Except
-----------------
https://www.w3schools.com/python/python_try_except.asp:

	- The try block lets you test a block of code for errors.
	- The except block lets you handle the error.
	- You can use the else keyword to define a block of code to be executed if no errors were raised
	- The finally block lets you execute code, regardless of the result of the try- and except blocks.


https://realpython.com/python-exceptions/

	- In the try clause, all statements are executed until an exception is encountered.
	- except is used to catch and handle the exception(s) that are encountered in the try clause.
	- else lets you code sections that should run only when no exceptions are encountered in the try clause.
	- finally enables you to execute sections of code that should always run, with or without any previously encountered exceptions.
	- raise allows you to throw an exception at any time.
	- assert enables you to verify if a certain condition is met and throw an exception if it isn’t.

try:
  print(x)
except:
  print("An exception occurred")

try:
    with open('file.log') as file:
        read_data = file.read()
except FileNotFoundError as fnf_error:
    print(fnf_error)


try:
  print(x)
except NameError:
  print("Variable x is not defined")
except:
  print("Something else went wrong")


try:
  print("Hello")
except:
  print("Something went wrong")
else:
  print("Nothing went wrong")

try:
    linux_interaction()
except AssertionError as error:
    print(error)
else:
    print('Executing the else clause.')


try:
  print(x)
except:
  print("Something went wrong")
finally:
  print("The 'try except' is finished")

try:
  f = open("demofile.txt")
  f.write("Lorum Ipsum")
except:
  print("Something went wrong when writing to the file")
finally:
  f.close()


PIL
---
Python Imaging Library (Python 2)
Manipulation of images

Recent Python 3 fork: Pillow
- https://pypi.org/project/Pillow/
- Packaged as pillow.
- Module name is still PIL.

Install Pillow
- sudo apt install python3-pil (if present as native package on Linux
- pip3 install pillow


from PIL import Image

im = Image.open("hopper.ppm")

# The file format of the source file, for example JPEG
print(im.format)

# Image size, in pixels. Returns a 2-tuple (width, height).
print(im.size)

# The pixel format. Typical values are "1", "L", "RGB", or "CMYK."
print(im.mode)

# Display image
im.show()

# Rotate image
im.rotate(45).show()
im.rotate(90).resize((128, 128)).convert('RGB')


# Geometry transforms
out = im.resize((128, 128))
out = im.rotate(45) # degrees counter-clockwise

# Transposing an image
out = im.transpose(Image.FLIP_LEFT_RIGHT)
out = im.transpose(Image.FLIP_TOP_BOTTOM)
out = im.transpose(Image.ROTATE_90)
out = im.transpose(Image.ROTATE_180)
out = im.transpose(Image.ROTATE_270)


# Crop
box = (100, 100, 400, 400)
region = im.crop(box)

# The region is defined by a 4-tuple , where coordinates are (left, upper, right, lower).
# (0, 0) in the upper left corner.

region = region.transpose(Image.ROTATE_180)
im.paste(region, box)



new_im = im.resize((1600,1000)).transpose(Image.FLIP_LEFT_RIGHT)
new_im.save("my_new_image.jpg")



# Save as jpg
outfile = f + ".jpg"
with Image.open(infile) as im:
	im.save(outfile)


# Thumbnail and jpg
with Image.open(infile) as im:
	im.thumbnail(size)
	im.save(outfile, "JPEG")




Data Serialization
------------------
JSON is used for example for transmitting data between web services.
YAML is mainly used for storing configuration.

JSON 				https://www.json.org/json-en.html
					https://docs.python.org/3/library/json.html#py-to-json-table
YAML 				https://yaml.org/
XML 				https://www.w3.org/XML/
Pickle 				https://docs.python.org/3/library/pickle.html
protocol-buffers	https://developers.google.com/protocol-buffers


Remarks:
- JSON strings themselves must be delineated using double quotes " "
- JSON uses an all-lowercase convention of true and false (Python uppercase).
- json.loads expects a string; use json.load() to load for example a file.


json_dict = {
  "employees": [
    {
      "name": "Pete",
      "department": "Security",
    },
    {
      "name": "Sandy",
      "department": "Cloud",
    }
  ]
}

# Serialize Python object, e.g. dictionary, into a JSON file
#
import json

with open('my_file.json', 'w') as file:
    json.dump(json_dict, file, indent=2)


# Serialize Python object, e.g. dictionary, into a JSON string
#
data = json.dumps(json_dict)


# Deserialize JSON from a file into a Python object.

with open('my_file.json', 'r') as file:
    json_dict = json.load(file)


# Deserialize a JSON string into a Python object.
json_dict = json.loads(data)




# Serialize Python object, e.g. dictionary, into a YAML file
#
import yaml

with open('my_file.yaml', 'w') as file:
    yaml.safe_dump(json_dict, file)



Requests
--------
https://docs.python-requests.org/en/master/
https://docs.python-requests.org/en/master/user/quickstart/
https://html.spec.whatwg.org/multipage/

import requests

resp = requests.get('https://www.github.com')
resp.text[:50]

resp = requests.get('https://www.github.com', stream=True)
resp.raw.read()[:50]


requests.post('url', data={'key':'value'})
requests.put('url', data={'key':'value'})
requests.delete('url')
requests.head('url')
requests.patch('url', data={'key':'value'})
requests.options('url')


# Basic Authentication header
r = requests.get(url, auth=(USER, PASS), verify=False)


# Basic Authentication header plus other headers
r = requests.get(url, auth=(USER, PASS), headers=headers, verify=False)


# Parameters in URL (url/get?key1=value1&key2=value2)
params = {
    "key1": "value1",
    "key2": "value2"
}
r = requests.get(url, params=params)


# Data in body
r = requests.post(url, data = {'key':'value'})
r = requests.put(url, auth=(USER, PASS), headers=headers, verify=False, data=json.dumps(my_payload))
r = requests.put(url, auth=(USER, PASS), headers=headers, verify=False, json=data)


# Data as a dictionary (automatically form-encoded)
r = requests.post('https://httpbin.org/post', data={'key':'value'})

# Data as a list of tuples (automatically form-encoded)
r = requests.post('https://httpbin.org/post', data=[('key', 'value')])

# Use json parameter for JSON data (requests serializes your data and adds Content-Type application/json)
r = requests.post('https://httpbin.org/post', json={'key':'value'})


r.text 						Response (with educated guesses about the encoding of the response)
r.json() 					Response with JSON decoded by builtin JSON decoder
r.status_code
r.raise_for_status()		Raises an HTTPError exception if the response wasn’t successful
r.headers
r.headers['Content-Type']
r.headers['Content-Encoding']
r.headers.get('content-type')
r.request.headers 			Headers sent to the server
							r.request.headers['Accept-Encoding']
r.request.url
r.request.body
r.url 						Gives the submitter URL and its parameters
r.content 					Binary Response Content (response body as bytes)
r.encoding 					Check or set the encoding used

if r: 						Successful response (evaluates to True if the status code was between 200 and 400)
if r.ok: 					Successful response
if r.status_code == 200:



params = {'key1': 'value1', 'key2': 'value2'}

headers = {
    "Authorization": "Bearer ACCESS_TOKEN",
    "Content-Type": "application/json"
}

my_payload = {
    "Cisco-IOS-XE-native:address": {
        "primary": {
            "address": "192.168.0.66",
            "mask": "255.255.255.0"
        }
    }
}

Web frameworks:
 Django		https://www.djangoproject.com/
 Flask 		https://www.fullstackpython.com/flask.html
 Bottle 	https://bottlepy.org/docs/dev/
 CherryPy 	https://cherrypy.org/
 CubicWeb 	https://www.cubicweb.org/



Email & SMTP
------------
Valid MIME types	https://www.iana.org/assignments/media-types/media-types.xhtml

# Guess the mimetype of a file
# Note: This only check the filename extension
#
import mimetypes
filename = '/home/tisipi/Downloads/TheCuisBook.pdf'
mime_type, _ = mimetypes.guess_type(filename)
print(mime_type)  # application/pdf


# Better, check the mimetype of a file by reading it contents
# pip install python-magic
#
import magic
mime = magic.Magic(mime=True)
mime.from_file('TheCuisBook')



# Send an email with an attachment
#
from email.message import EmailMessage

# From, To, Subject, Body
#
FROM = "he@example.com"
TO = "she@example.com"
SUBJECT = 'A nice email'
BODY = "Hi! This email is sent using Python!"

# Attachment
#
import mimetypes
import os.path

attachment_path = '/home/eric/Downloads/simple.txt'
attachment_filename = os.path.basename(attachment_path)
mime_type, _ = mimetypes.guess_type(attachment_path)
# For example text/plain, application/pdf
mime_type, mime_subtype = mime_type.split('/',1)
print(mime_type)
print(mime_subtype)

# Create message
msg = EmailMessage()
msg['From'] = FROM
msg['To'] = TO
msg['Subject'] = SUBJECT
msg.set_content(BODY)
with open(attachment_path, 'rb') as ap:
    msg.add_attachment(ap.read(), maintype=mime_type, subtype=mime_subtype, filename=attachment_filename)
print(msg)


# Send message using SMTP
#
import smtplib
mail_server = smtplib.SMTP('localhost')
mail_server = smtplib.SMTP_SSL('smtp.ziggo.nl')
mail_server.set_debuglevel(1)
#
import getpass
mail_password = getpass.getpass('Please give your email password: ')
#
try:
    mail_server.login(sender, mail_pass)
except smtplib.SMTPAuthenticationError:
	print('SMTP Authentication Error my friend')
#
mail_server.send_message(msg)
mail_server.quit()


+++++++++++
server: smtp.ziggo.nl
serverpoort: 587
type versleuteling: STARTTLS, TLS of SSL

    import smtplib
    import ssl
    context = ssl.SSLContext(ssl.PROTOCOL_TLS)
    connection = smtplib.SMTP('smtp-mail.outlook.com', 587)
    connection.ehlo()
    connection.starttls(context=context)
    connection.ehlo()
    connection.login('now_your_real_login_data@outlook.com', 'otherwise_SMTPServerDisconnect')
+++++++++++




ReportLab
---------
Generating PDF
https://www.reportlab.com/dev/opensource/
https://www.reportlab.com/docs/reportlab-userguide.pdf

pip install reportlab

PLATYPUS	Page Layout and Typography Using Scripts


# The canvas should be thought of as a sheet of white paper with points on the sheet
# identified using Cartesian (X,Y) coordinates
# (0,0) origin at the lower left corner of the page.
# x goes to the right, y goes up, by default.


# Hello world PDF
#
from reportlab.pdfgen import canvas
from reportlab.lib.units import cm

c = canvas.Canvas("hello.pdf")
c.drawString(9*cm, 22*cm, "Hello World!")
c.showPage()
c.save()


# Generate a PDF report using PLATYPUS
#
from reportlab.platypus import SimpleDocTemplate
from reportlab.platypus import Paragraph, Spacer, Table, Image
from reportlab.lib.styles import getSampleStyleSheet

report = SimpleDocTemplate("report.pdf")

# Use default style
styles = getSampleStyleSheet()
report_title = Paragraph("A Complete Inventory of My Cars", styles["h1"])

report.build([report_title])


# Add a table to the report using a Table object:
# - Create a Table object, using a list-of-lists (a two-dimensional array).
# - Add the table object to the report.
# - Generate the PDF again.

# Convert a dictionary into a list-of-lists
#
cars = {
  "Opel": 1,
  "VW": 1,
  "Ferrari": 2,
  "BMW": 2,
  "Mercedes": 3,
  "Renault": 2
}

# Generate the list-of-lists
#
table_data = []
for key, value in cars.items():
  table_data.append([key, value])
print(table_data)

# Add the table object to the report
#
report_table = Table(data=table_data)
report.build([report_title, report_table])

# Add a border around all of the cells in our table
# Align to the left

from reportlab.lib import colors
table_style = [('GRID', (0,0), (-1,-1), 1, colors.red)]
report_table = Table(data=table_data, style=table_style, hAlign="LEFT")
report.build([report_title, report_table])


# Create a Pie chart. Pie charts needs
# - list of data,
# - list oflabels.
#
from reportlab.lib.units import cm
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.piecharts import Pie

report_pie = Pie(width=7*cm, height=7*cm)
report_pie.data = []
report_pie.labels = []
for car in sorted(cars):
  report_pie.data.append(cars[car])
  report_pie.labels.append(car)
print(report_pie.data)
print(report_pie.labels)

# Place the Pie object a Flowable Drawing.
report_chart = Drawing()
report_chart.add(report_pie)

# Add the Drawing to the report
report.build([report_title, report_table, report_chart])



Jupyter Notebooks
-----------------
Jupyter Notebook Tutorial			https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook
How to use Jupyter Notebooks		https://www.codecademy.com/articles/how-to-use-jupyter-notebooks
Teaching and Learning with Jupyter 	https://jupyter4edu.github.io/jupyter-edu-book/


unittest.main(argv = ['ignore-this-dummy-argument'], exit = False)
	unittest.main( ) looks at sys.argv. In Jupyter the first parameter of sys.argv is what started the Jupyter kernel.
	Passing an explicit list to unittest.main( ) prevents it from looking at sys.argv.
	exit = False prevents unittest.main( ) from shutting down the kernel process.



Semantic Versioning
-------------------
https://semver.org/#summary



Bash Scripting
--------------
Bash = Bourne Again shell (Steve Bourne, Bell Labs Research)

https://ryanstutorials.net/bash-scripting-tutorial/
https://linuxconfig.org/bash-scripting-tutorial-for-beginners
https://www.shellscript.sh
https://wiki.bash-hackers.org/commands/classictest

echo $SHELL


STDIN	0
STDOUT	1
STDERR	2

>	redirect STDOUT output to file
>>	redirect STDOUT output to file (but append, iso overwrite)
2>	redirect STDERR output to file

<	redirect STDIN input from file


cat pluralsight.txt | less
cat pluralsight.txt | tr ' ' '\n'
cat pluralsight.txt | tr ' ' '\n' | sort
cat pluralsight.txt | tr ' ' '\n' | sort | uniq
cat pluralsight.txt | tr ' ' '\n' | sort | uniq -c
cat pluralsight.txt | tr ' ' '\n' | sort | uniq -c | sort -n
cat pluralsight.txt | tr ' ' '\n' | sort | uniq -c | sort -nr
cat pluralsight.txt | tr ' ' '\n' | sort | uniq -c | sort -nr | head


ps ax | grep <search>

ctrl-c	SIGINT 		Terminates process cleanly
ctrl-z	SIGSTOP		Stops process (restart/continue with "fg")

kill <pid>	default is SIGTERM


ps			list processes executing in current terminal for current user
ps ax		list all processes executing for all users
ps e		shows the environment for the processes listed
fg			move stopped or background job to the foreground
bg			move stopped job to the background
jobs		lists jobs currently running or stopped
top			list top CPU processes ("q" to quit)


cd <directory>					Change directory
pwd								Print the current working directory
ls								List current directory
ls <directory>					List directory
ls -l							List long info of current directory
ls -a							List current directory (incl. hidden files)
ls -la
mkdir <directory>				Create/make directory
rmdir <directory>				Remove directory (if empty)
rm -rf <directory>				Remove directory (if not empty)
cp <old> <new>      			Copy
mv <old> <new>      			Move
touch <file>					Create an empty file (or update modification time if existing)
chmod <modifiers> <file>		Change permissions
chown <user> <file>				Changes the owner
chgrp <group> <file>			Changes the group

								File permissions:
								- Special file permissions
								- Owner permissions
								- Group permissions
								- Others permissions

								For each group, the permissions refer to the possibility of reading, writing and executing the file:
								- 4 for read, 2 for write and 1 for execute
								- For example:
									- Permission of 6 means read and write
									- Permission of 5 means read and execute
									- Permission of 7 means read, write and execute.

cat  <file>						Shows the content of the file on standard output
wc   <file>						Word Count (characters, words, lines)
file <file>						Print the file type
head <file>						Head of file (first 10 lines)
tail <file>						Tail of file (last 10 lines)
less <file>						Scroll through file ("q" to quit)
sort <file>						Sort the file alphabetically
cut OPTION... [FILE]...			Cut specific fields/character from file
cut -d' ' -f 6					Cut 6th field only (using space delimiter)
cut -d' ' -f6-					Cut starting from 6th field when using space delimiter
								e.g.: journalctl | head | cut -d' ' -f6-
basename NAME [SUFFIX]			Strip directory and suffix from filenames


echo "message"					Print message to standard output
date							Print current date
who								Print logged in users
man <command>					Manual pages
uptime							Uptime system
free							Free and used memory


# Status code last Linux command (0 is success/ok)
echo $?


collect.sh:
	#!/bin/bash
	echo "START TIME $(date)"; echo
	echo "YOU ARE $(who)"; echo
	echo "UPTIME IS"; uptime; echo

Variables:
	# Note: no spaces!
	name=Tisipi
	echo $name

	# Store results of command in variable
	files=$(ls)
	echo $files


Globs like * and ?:
	echo *.sh
	echo c*
	echo P?r*

Create an empty file:
	>test.txt

Append to file:
	echo ja zeker >> test.txt

Read a line from standard input:
	echo What is your name?
	read NAME
	echo "Hello $NAME - How are you?"

Conditional expression:
	#!/bin/bash
	if grep "127.0.0.1" /etc/hosts
	then
		echo "Loopback present"
	else
		echo "No Loopback"
	fi

	#!/bin/bash
	if grep "127.0.0.1" /etc/hosts; then
		echo "Loopback present"
	else
		echo "No Loopback"
	fi


	# https://wiki.bash-hackers.org/commands/classictest
	#
	# Bash test command
	#
	# file exists: test -e

	if test -e /etc/passwd; then
	  echo "Alright man..." >&2
	else
	  echo "Yuck! Where is it??" >&2
	  exit 1
	fi


	# Alternative test command ("[" is a test command too; "]" is an argument)

	if [ -e /etc/passwd ]; then
	  echo "Alright man..." >&2
	else
	  echo "Mmh! Where is it??" >&2
	  exit 1
	fi


	# Test if string is not empty

	if test -n "$PATH" ; then echo "Path is not empty"; fi
	if [ -n "$PATH" ]; then echo "Path is not empty"; fi


File tests:

	-e <FILE>			Tests if <FILE> exists.
	-f <FILE>			Tests if <FILE> exists and is a regular file.
	-d <FILE>			Tests if <FILE> exists and is a directory.
	-S <FILE>			Tests if <FILE> exists and is a socket file.
	-L <FILE>			Tests if <FILE> exists and is a symbolic link.
	-r <FILE>			Tests if <FILE> exists and is readable.
	-w <FILE>			Tests if <FILE> exists and is writable.
	-x <FILE>			Tests if <FILE> exists and is executable.
	-s <FILE>			Tests if <FILE> exists and has size bigger than 0 (not empty).
	<FILE1> -nt <FILE2>	Tests if <FILE1> is newer than <FILE2> (mtime).
	<FILE1> -ot <FILE2>	Tests if <FILE1> is older than <FILE2> (mtime).


String tests:

	-z <STRING>				Tests if <STRING> is empty/zero.
	-n <STRING>				Tests if <STRING> is not empty (default operation).
	<STRING1> = <STRING2>	Tests if strings are equal.
	<STRING1> != <STRING2>	Tests if strings are not equal.
	<STRING1> < <STRING2>	Tests if <STRING1> sorts before <STRING2> lexicographically (pure ASCII, not current locale!). Remember to escape! Use \<
	<STRING1> > <STRING2>	Tests if <STRING1> sorts after <STRING2> lexicographically (pure ASCII, not current locale!). Remember to escape! Use \>

Arithmetic tests:

	<INTEGER1> -eq <INTEGER2>	Tests if the integers are equal.
	<INTEGER1> -ne <INTEGER2>	Tests if the integers are NOT equal.
	<INTEGER1> -le <INTEGER2>	Tests if the first integer is less than or equal second one.
	<INTEGER1> -ge <INTEGER2>	Tests if the first integer is greater than or equal second one.
	<INTEGER1> -lt <INTEGER2>	Tests if the first integer is less than second one.
	<INTEGER1> -gt <INTEGER2>	Tests if the first integer is greater than second one.



While loop:

	i=1
	while [ $i -le 10 ]; do
	  echo "$i"
	  ((i+=1))
	done

	operation=$1
	while ! $operation && [ $i -le 10 ]; do
	  ...
	  ((i=i+1))
	done


For loop:

	for i in 1 2 3 4 5; do
	  echo "$i"
	done

	for name in John Peter Tim Steve; do
	  echo "$name"
	done

	for file in *.txt; do
	  name=$(basename "$file" .txt)
	  mv "$file" "$name.doc"
	done



operator
--------
import operator

# Sort dictionary based on its keys.
sorted(cars.items())
sorted(cars.items(), key=operator.itemgetter(0))

# Sort dictionary based on its values.
sorted(cars.items(), key=operator.itemgetter(1))

# Sort dictionary based on its values - Descending
sorted(cars.items(), key=operator.itemgetter(1), reverse=True)



Diff & Patch
------------
http://man7.org/linux/man-pages/man1/diff.1.html
http://man7.org/linux/man-pages/man1/patch.1.html


# Differences between text files
diff file1 file2

# Differences between text files using context
# The -u (unified) option tells diff to also list some of the unmodified
# text lines from before and after each of the changed sections.
diff -u file1 file2
diff -u file1 file2 > diffs.txt


# Patch proposed diffs to my file
patch my_file < diffs.txt

+++ patch -p1 +++


# Difference between directories
# Use -q to report only when files differ.
diff -q dir1 dir2

# Recursive diff
diff -rq dir1 dir2


# Display word differences between text files
wdiff [ option ... ] file1 file2

# GUI Diff Tools
meld
KDiff3
vimdiff



GIT
---
Pro Git book 			https://git-scm.com/book/en/v2
Git tutorial			https://git-scm.com/docs/gittutorial
Github Git Cheat Sheet	https://training.github.com/downloads/github-git-cheat-sheet.pdf

https://git-scm.com/doc
https://git-scm.com


sudo apt update
sudo apt install git
git --version

git config --global user.name "Tisipi"
git config --global user.email "Tisipi@tisipi.com"
git config -l

git init
git status
git add temp.py
git commit -m 'first commit'

git commit -am "Express commit: Combined add and commit"

git log
git log -2
git log --stat
git show <id>


# Git log with patches (diffs)
git log -p
git log -p -2


# Check diffs between working directory and local repository
git diff

# Check diffs between working directory and staging area
git diff --staged



# Show patches add asks if you want to add file to staging area.
git add -p


# Remove file
git rm <file>

# Rename file
git mv <file>


# Create a .gitignore file, to indicate files that shouldn't be tracked
# Helpful templates for .gitignore files at github.com/github/gitignore.
touch .gitignore


# Cloning a remote repository
git clone <url> <local dir>
git clone <url> .


# Rollback all changes in a specific file.
# Checkout the last version of the file.
# This discard changes in working directory
#
git checkout <changed file>


# Reset staged file; remove file from staging area
git reset HEAD <file>
 or
git reset <file>
 or
git restore --staged <file>

# Reset staged files interactively
git reset -p


# Change last commit message.
# This changes the commit history!
git commit --amend -m "This is the correct message"

# Forgot to include file in last commit.
# This changes the commit history!
git add <file>
git commit --amend


# Revert commit.
git revert HEAD


# Revert a specific commit.
# Does not discard any commits that came after that one.
# It creates a new revision that reverts the effects of a specified commit.
#
git revert <hash>


# Create a local branch called my_test_branch
git branch my_test_branch

# List the branches
git branch
git branch -a
git branch --all

# Jump to branch (git put files in your working directory).
# Switch branches. Checkout the branch.
#
git checkout my_test_branch


# Create and checkout a branch in one step.
git checkout -b my_test_branch


# Delete a branch.
git branch -d <branch name>
git branch --delete  <branch name>


# Delete a branch, forced (in case of unmerged changes)
#
git branch -D <branch name>
git branch -d --force <branch name>
git branch --delete --force <branch name>



# Merge my_test_branch with master
git checkout master
git merge my_test_branch

# In case of conflicts:
# - Edit the file
# - Git add the updated file
# - Git commit the updated file
nano <conflicting file>
git add <conflicting file>
git commit

# To see merge history graphically
git log --graph
git log --graph --oneline

# Abort the merge, for example in case of too many conflicts.
#
git merge --abort



# Clone a remote repository into your local workspace
git clone URL3

# Push commits from your local to a remote repository
git push

# Fetch the newest updates from a remote repository
git pull


# Caching your GitHub credentials (use the credential memory cache)
git config --global credential.helper cache

# Change the default password cache timeout (in seconds)
git config --global credential.helper 'cache --timeout=3600'


# View information about remote repository
git remote -v
git remote show origin

# Check remote branches that git is tracking locally
git branch -r


# Refresh your local repository from the one on GitHub.
# Update your local repository with the remote updates.
#
git fetch

# Check log of the remote master branch (that is tracked locally)
git log origin/master

# Merge changes of remote master branch in local master
git merge origin/master


# Git pull = Combined git fetch and git merge
# - Fetch from remote (into local repository)
# - Perform a git merge into working directory
#
git pull


# Get the contents of a remote branch without automatically merging.
git remote update


# Check log of all local and remote branches.
git log --graph --oneline --all



# Push a new local branch to the remote server
# - By default Git names the remote server 'origin' ('origin' is just an alias for the remote server).
# - Option -u sets the tracking relationship between the local branch and the new remote branch.
#
# git push -u <remote> <branch name>
#
git push -u origin my_new_branch



+++
# Push a new repository to a remote server
# - Define an alias for the remote server with: git remote add
# - Push repo to remote server with: git push -u <remote> <branch name>
#   Note that 'origin' is just an alias for the remote server; by default Git names the server 'origin'.
#
# Remark: Option -u sets the tracking relationship between the local branch and the new remote branch.
#
git remote add origin https://github.com/Tisipi/Help-Files.git
git push -u origin master
+++



# Rebase replays changes in a branch onto for example master.
# Note: This forges history
#
# https://git-scm.com/book/en/v2/Git-Branching-Rebasing
#
# - Checkout branch that you want to replay on master
# - Define in the rebase command the branch the changes should be replayed/rebased on.
# - Checkout master
# - Merge your branch into the master
#
git checkout <feature branch>
git rebase master
git checkout master
git merge <feature branch>

# Delete the rebased branch (remotely)
git push --delete origin <feature branch>
# Delete the rebased branch (locally)
git branch -d <feature branch>
# Push all changes to the remote repo
git push


# Another workflow (keeps history linear)
# - Fetches remote changes into origin/master
# - Rebase your own master changes on top of it.
#
git fetch
git rebase origin/master



# Example of a typical Github pull request
#
- Github repo > Fork;  creates a fork under your username
- Clone the fork (under your username)
- Create and checkout a new branch for your changes:
  git checkout -b my_new_branch
- Create your files
- Stage and commit your files:
  git add .
  git commit -m "New feature"
- Push your new local branch to the remote server
  git push -u origin my_new_branch
- Test/validate your changes
- Open a Pull request:
  > Your Github project repo > Pull request
    > Write description of your pull request
    > Double check the changes files and diffs
    > Create pull request
- In case of comments to your pull request:
  - Update your files
  - Stage and commit your files
    git add .
    git commit -m "Processed project comments and added them to new feature files"
  - Push your files again (will be added to your pull request automatically)
    git push

# Squashing changes via interactive rebase (in case of comment to your pull request)
#
- Do an interactive rebase:
  git rebase -i master
- This opens texteditor with a list of commits. The first word of each line contains a command:
    pick		Use this commit (default command)
    squash 		Use commit, but meld into previous commit
    fixup		Like squash, but discard this commit's log message
- Adapt the commands. Save file.
- Brings you in commit message editor. Adapt the commit message(s). Save file.
- Check the latest commit and status:
  - git show
  - git status
  - git log --graph --oneline --all
- Push the changes to the remote repo.
  If you use git push, this will fail, because the number of commits differs.
  Instead use a forced push to replace the remote commits by the squashed commit:
  - git push -file


# More info about merging pull requests
#
https://help.github.com/en/articles/about-pull-request-merges


# Code reviews
#
http://google.github.io/styleguide/
https://help.github.com/en/articles/about-pull-request-reviews
https://medium.com/osedea/the-perfect-code-review-process-845e6ba5c31
https://smartbear.com/learn/code-review/what-is-code-review/



Troubleshooting
===============

# Monitoring Tools
#
Process Monitor 				https://docs.microsoft.com/en-us/sysinternals/downloads/procmon
Perf analysis in 60s 			http://www.brendangregg.com/linuxperf.html
The USE Method 					http://brendangregg.com/usemethod.html
Activity Monitor in Mac: 		https://support.apple.com/nl-nl/guide/activity-monitor/welcome/mac
Performance Monitor on Windows 	https://www.windowscentral.com/how-use-performance-monitor-windows-10
Resource Monitor in Windows 	https://www.digitalcitizen.life/how-use-resource-monitor-windows-7
Process Explorer Windows 		https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer
Linux nice levels				https://www.reddit.com/r/linux/comments/d7hx2c/why_nice_levels_are_a_placebo_and_have_been_for_a/


# Logfiles Linux
/var/log/syslog
/var/log/syslog/.xsession-errors

# Logfiles MacOS
/Library/Logs

# Logfiles Windows
Via Event Viewer tool


# Performance MacOS
Activity monitor

# Performance Windows
Resource monitor
Performance monitor


# Trace system calls
# https://www.howtoforge.com/linux-strace-command/
#
strace ./myAmazingApp.py
strace -o output.strace ./myAmazingApp.py
	less output.strace
	shift-g 	# movees to end of file


# Trace system calls MacOS
dtruss


# Trace library calls
ltrace <executable ELF>


# Top processes
top
htop

# Kill (e.g. hanging) process (gracefully)
kill -STOP <pid>

# Kill all httpd processes (gracefully)
killall -STOP httpd


# Top I/O processes
sudo iotop

# Statistics on I/O operations
iostat

# Statistics on virtual memory operations
vmstat


# Network usage on interfaces
iftop


# Apache benchmark to check slow web server
ab -n <#requests> <url>
ab -n 500 www.google.com



# Sync the contents of dir1 to dir2 on the same system, type:
# Source: https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories
#
# This is recursive option.
# Note the /
rsync -r dir1/ dir2

# Archive option. Syncs recursively and preserves symbolic links, special and device files,
# and file attributes. More commonly used than -r and is usually what you want to use.
# Note the /
rsync -a dir1/ dir2

# Always do a dry-run when using rsync by passing the -n or --dry-run options.
# The -v flag (for verbose) is also necessary to get the appropriate output
rsync -anv dir1/ dir2


# Syncing to a remote system
# (if you have SSH access to the remote machine and rsync installed on both sides)
# Sync/push the dir1 folder to a remote host (we want to transfer the actual directory, so we omit the trailing slash):
#
rsync -a ~/dir1 username@remote_host:destination_directory

This is called a “push” operation because it pushes a directory from the local system
to a remote system. The opposite operation is

# Sync/pull” a remote directory to the local system.
rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine


# Limiting rsync bandwidth
rsync -bwlimit ...

# Trickle is a network bandwidth shaper tool
# For example limiting the download speed to 1024Kbps:
trickle -d 1024 wget -c <url>



# Set or get process I/O scheduling class and priority
#
# Sets process with PID 89 as an idle I/O process.
ionice -c 3 -p 89
# Runs 'bash' as a best-effort program with highest priority.
ionice -c 2 -n 0 bash
# Prints the class and priority of the processes with PID 89 and 91.
ionice -p 89 91


# Alter priority of running processes
# range -20 to 19
#    0: 	base scheduling priority
#   19: 	lowest, run only when nothing else wants to
#   20:		highes prio (to make things go very fast).
#
renice 19 <pid>

# Find the process ID of a running program
pidof bash
pidof sshd

for pid in $(pidof httpd); do renice -6 $pid; done


# Find location (directory) of a file
#
locate <file>
locate help-htdp.txt


Linux performance analysis in 60 secs (brendangregg):
- uptime
- sudo dmesg | tail
- vmstat 1
- mpstat -P all
- pidstat 1
- iostat -xz 1
- free -m
- sar -n DEV 1
- sar -n TCP,ETCP 1
- top


Profilers
C 		gprof
Python 	cProfile, pprofile3, memory_profiler




# Check real, user space and system time of an executed program
time ./myProgram.py


# Use top to show all running processes and their memory usage.
# VIRT, RES, SHR, MEM
# Shift-m to sort on memory columns
top


# pprofile3 outputting Callgrind Profile Format
pprofile -f callgrind -o cachegrind.out.results ./myProgram.py
pprofile --format callgrind --out cachegrind.out.results ./myProgram.py

# Callgrind format is implicitly enabled if --out basename starts with cachegrind.out.,
# so command can be simplified to:
pprofile --out cachegrind.out.results demo/threads.py

# Check results with graphical kcachegrind tool:
kcachegrind cachegrind.out.results



# Profile Size of Individual Objects
import sys
sys.getsizeof({})
sys.getsizeof([])
sys.getsizeof(set())



memory_profiler
---------------
https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python

# Profile a Single Function or Method?
#
# Puttthe @profile decorator around any function or method
#
python -m memory_profiler <yourscript.py>



guppy - python2
---------------
# Profile an entire Application.
#
# To use guppy, drop something like this in your code.
#
from guppy import hpy

h = hpy()
print h.heap()


guppy - python3
---------------
https://github.com/zhuyifei1999/guppy3

# Create the session context:
from guppy import hpy
h=hpy()

#Show the reachable objects in the heap:
h.heap()

# Show the shortest paths from the root to the single largest object:
h.heap().byid[0].sp

# How to create and show a set of objects:
h.iso(1,[],{})


mprof
-----
# Show you memory usage over the lifetime of your application.
# Creates a graph of your script’s memory usage which you can view by running mprof plot (requires matplotlib)
#
mprof run <script> <script_args>



Threading and asyncio modules
-----------------------------
https://realpython.com/python-concurrency/
https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32


Threading - concurrent.futures
------------------------------
- Multiprocess (ProcessPoolExecutor). Spawn multiple processes with its own Python interpreter. Bypasses Python’s Global Interpreter Lock (GIL). Best for CPU-bound tasks.
- Multithread (ThreadPoolExecutor). Uses multiple threads in the same process. Threads share the same interpreter and memory space. Best for I/O-heavy tasks. Threads are not interruptible/killable like processes are.


# https://www.digitalocean.com/community/tutorials/how-to-use-threadpoolexecutor-in-python-3
#
import requests
import concurrent.futures

def get_wiki_page_existence(wiki_page_url, timeout=10):
    response = requests.get(url=wiki_page_url, timeout=timeout)

    page_status = "unknown"
    if response.status_code == 200:
        page_status = "exists"
    elif response.status_code == 404:
        page_status = "does not exist"

    return wiki_page_url + " - " + page_status

wiki_page_urls = [
    "https://en.wikipedia.org/wiki/Ocean",
    "https://en.wikipedia.org/wiki/Island",
    "https://en.wikipedia.org/wiki/this_page_does_not_exist",
    "https://en.wikipedia.org/wiki/Shark",
]
with concurrent.futures.ThreadPoolExecutor() as executor:
    futures = []
    for url in wiki_page_urls:
        futures.append(
            executor.submit(
                get_wiki_page_existence, wiki_page_url=url, timeout=0.00001
            )
        )
    for future in concurrent.futures.as_completed(futures):
        try:
            print(future.result())
        except requests.ConnectTimeout:
            print("ConnectTimeout.")


# Alternative
executor = concurrent.futures.ThreadPoolExecutor()
executor.submit(get_wiki_page_existence, wiki_page_url=url, timeout=0.00001)
print("Wait for all threads to finish")
executor.shutdown()

# Alternative with ProcessPoolExecutor
executor = concurrent.futures.ProcessPoolExecutor()
executor.submit(get_wiki_page_existence, wiki_page_url=url, timeout=0.00001)
print("Wait for all processes to finish")
executor.shutdown()


Python concurrency:	https://realpython.com/python-concurrency/
Python asyncio: 	https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32


---

#!/usr/bin/env python3
from multiprocessing import Pool

def run(task):
  # Do something with task here
    print("Handling {}".format(task))

if __name__ == "__main__":
  tasks = ['task1', 'task2', 'task3']
  # Create a pool of specific number of CPUs
  p = Pool(len(tasks))
  # Start each task within the pool
  p.map(run, tasks)


---


#!/usr/bin/env python3

import os
from multiprocessing import Pool
import subprocess


def run_rsync_dir(dir):
    print("Syncing {}".format(dir))
    src = '/data/prod/'
    dst = '/data/prod_backup'
    # subprocess.run(["rsync", "-av", "--dry-run", src, dst])
    subprocess.run(["rsync", "-arq", src, dst])


if __name__ == "__main__":

    path = '/data/prod'
    sync_dirs = []
    for (root,dir,files) in os.walk(path, topdown=True):
    	sync_dirs.append(root)

    p = Pool(len(sync_dirs))
    p.map(run_rsync_dir, sync_dirs)


sudo netstat -nlp | grep :80
/etc/nginx
/etc/nginx/sites.enabled
/etc/uwsgi
/etc/uwsgi/apps-enabled
sudo servuce uwsgi reload

System Log Linux 			https://www.fosslinux.com/8984/how-to-check-system-logs-on-linux-complete-usage-guide.htm
System Log on Windows 10 	https://www.digitalmastersmag.com/magazine/tip-of-the-day-how-to-find-crash-logs-on-windows-10/
System Log on a MacOS 		https://www.howtogeek.com/356942/how-to-view-the-system-log-on-a-mac/
System calls MacOS 			https://etcnotes.com/posts/system-call/



memtest86 and memtest86+		Test and stress test the random access memory of an x86 architecture system for errors
Valgrind 	(Linux, MacOS) 		Memory debugger, memory leak detection and profiling.
Dr. Memory 	(Linux, Windows) 	Memory debugger
pdb3 							Python Debugger

# Run Python debugger
pdb3 my_program <args>
	next
	continue
	print(var)

# Enabling Linux core dumps
ulimit -c unlimited
ulimit -S -c unlimited

# Disable core dumps
ulimit -c 0
ulimit -S -c 0

# Check core dump settings
ulimit -a


# Load core file in gdb debugger
ls -l core
gdb -c core <program>

	# Stacktrace/backtrace
	backtrace
	# Move to the calling function
	up
	# List more lines of code
	list
	# Print value of variable
	print i


# Dump files in octal and other formats
# /dev/random, /dev/urandom and /dev/arandom are special files that serve as pseudorandom number generators.
# -c characters
# -x hex
#
od -cx /dev/urandom


+++ check git bisect +++




Puppet
======
https://puppet.com/docs/puppet/latest/lang_resources.html
https://puppet.com/blog/deploy-packages-across-your-windows-estate-with-bolt-and-chocolatey/

Puppet language style guide:	https://puppet.com/docs/puppet/7.4/style_guide.html
Puppet Lint 					http://puppet-lint.com/
	Use puppet-lint and metadata-json-lint to check your module for compliance with the style guide.


Every resource declaration has a resource type, a title, and a set of attributes:
<resource type> { '<title>':
	<attribute> => <value>,
	<attribute> => <value>,
	...
}


Built-in resource types:
- package:
- file: 	Manages files.
- service: 	Manages running services.
- exec: 	Executes commands.
- cron:
- group:
- user: 	Manages users.
- notify:


File resource type:

	file { '/etc/passwd':
	  ensure => file,
	  owner  => 'root',
	  group  => 'root',
	  mode   => '0600',
	}


Note, these are all the same:


	file { "/tmp/foo":
	  ensure => file,
	}

	File { "/tmp/foo":
	  ensure => file,
	}

	Resource[File] { "/tmp/foo":
	  ensure => file,
	}


Resource declaration default attributes:

	file {
	  default:
	    ensure => file,
	    owner  => "root",
	    group  => "wheel",
	    mode   => "0600",
	  ;
	  ['ssh_host_dsa_key', 'ssh_host_key', 'ssh_host_rsa_key']:
	    # use all defaults
	  ;
	  ['ssh_config', 'ssh_host_dsa_key.pub', 'ssh_host_key.pub', 'ssh_host_rsa_key.pub', 'sshd_config']:
	    # override mode
	    mode => "0644",
	  ;
	}

Setting attributes from a hash by using the splat (*) attribute:

	$file_ownership = {
	  "owner" => "root",
	  "group" => "wheel",
	  "mode"  => "0644",
	}

	file { "/etc/passwd":
	  ensure => file,
	  *      => $file_ownership,
	}


Arrays of titles let you create multiple resources with the same set of attributes:

	$rc_dirs = [
	  '/etc/rc.d',       '/etc/rc.d/init.d','/etc/rc.d/rc0.d',
	  '/etc/rc.d/rc1.d', '/etc/rc.d/rc2.d', '/etc/rc.d/rc3.d',
	  '/etc/rc.d/rc4.d', '/etc/rc.d/rc5.d', '/etc/rc.d/rc6.d',
	]

	file { $rc_dirs:
	  ensure => directory,
	  owner  => 'root',
	  group  => 'root',
	  mode   => '0755',
	}


Facts:
- Puppet Facter tool collects system information, called facts.
- Facts are assigned as values to variables.
- Puppet also sets built-in variables, which behave a lot like facts.
- Access facts in two ways:
	- $fact_name
	- $facts[fact_name].


if $facts['is_virtual'] {
  ...
} else {
  ...
}


exec is not idempotent; to make it idempotent use "onlyif":

exec { 'move file':
  command => 'mv /Downloads/myfile.txt /Desktop',
  onlyif  => 'test -e myfile.txt',
}


# Puppet manifests directory
cd /etc/puppet/code/environments/production/modules/profile/manifests
cat myfile.pp

# Trigger a manual run of the Puppet agent
sudo puppet agent -v --test


# Install puppet
sudo apt install puppet-master


tools.pp:
package { 'htop':
  ensure => present	,
}

sudo puppet apply -v tools.pp


# Install Apache module
sudo apt install puppet-module-puppetlabs-apache
cd /usr/share/puppet/modules.available/puppetlabs-apache
	manifests
	files
	templates
	lib
	metadata.json file

webserver.pp:
	# include global module
	include ::apache



Node definitions:
- A node definition or node statement allows you to assign specific configurations to specific nodes.
- Typically stored in site.pp

	# <ENVIRONMENTS DIRECTORY>/<ENVIRONMENT>/manifests/site.pp
	node 'www1.example.com' {
	  include common
	  include apache
	  include squid
	}
	node 'db1.example.com' {
	  include common
	  include mysql
	}


	node 'www1.example.com', 'www2.example.com', 'www3.example.com' {
	  include common
	  include apache, squid
	}


# Install puppet agent
sudo apt install puppet
# Configure its pupper master
sudo puppet config set server my.master.com

# Test connectivty to master
puppet agent -t
puppet agent --test
puppet agent -v --test



# Main manifest or site manifest.
# /etc/puppet/code/environments/production/manifests/site.pp
# /etc/puppetlabs/code/environments/production/manifests/site.pp
#
site.pp:
	node my.sillyweb.com {
	  class {'apache':}
	}
	node default {}

puppet apply /etc/puppetlabs/code/environments/production/manifests/site.pp



# Enable puppet service to enable agent automatically when it boots
sudo systemctl enable puppet
# Start the agent
sudo systemctl start puppet
# Check the status of the agent
sudo systemctl status puppet


Puppet SSL explained: 	https://www.masterzen.fr/2010/11/14/puppet-ssl-explained/



Puppet Templates
----------------
Two templating languages:
- Embedded Ruby (ERB).
- Embedded Puppet (EPP). Puppet version ≥ 4.0.


Example
-------
Use module "packages" to install packages on the servers in the fleet.

cd /etc/puppet/code/environments/production/modules/packages
cat manifests/init.pp

sudo chmod 646 manifests/init.pp

class packages {
   package { 'python-requests':
       ensure => installed,
   }
   if $facts[os][family] == "Debian" {
     package { 'golang':
       ensure => installed,
     }
  }
   if $facts[os][family] == "RedHat" {
     package { 'nodejs':
       ensure => installed,
     }
  }
}

---

cd /etc/puppet/code/environments/production/modules/machine_info
cat manifests/init.pp
sudo chmod 646 manifests/init.pp

class machine_info {
  if $facts[kernel] == "windows" {
       $info_path = "C:\Windows\Temp\Machine_Info.txt"
   } else {
       $info_path = "/tmp/machine_info.txt"
   }
 file { 'machine_info':
       path => $info_path,
       content => template('machine_info/info.erb'),
   }
}

sudo chmod 646 templates/info.erb
cat templates/info.erb

	Server Information
	------------------
	Disks: <%= @disks %>
	Memory: <%= @memory %>
	Processors: <%= @processors %>
	Network Interfaces: <%= @interfaces %>
	}

---

cd /etc/puppet/code/environments/production/modules/reboot/manifests
cat reboot/manifests/init.pp:

class reboot {
  if $facts[kernel] == "windows" {
    $cmd = "shutdown /r"
  } elsif $facts[kernel] == "Darwin" {
    $cmd = "shutdown -r now"
  } else {
    $cmd = "reboot"
  }
  if $facts[uptime_days] > 30 {
    exec { 'reboot':
      command => $cmd,
     }
   }
}


To get a module executed, make sure to include it in the site.pp file:
/etc/puppet/code/environments/production/manifests/site.pp

node default {
  class { 'packages': }
  class { 'machine_info': }
  class { 'reboot': }
}



Puppet Lint
-----------
http://puppet-lint.com/
http://puppet-lint.com/checks/

Checks syntax and style of manifests.


# Install it
#
package { 'puppet-lint':
  ensure   => '1.1.0',
  provider => 'gem',
}

# Run/check it
puppet-lint /etc/puppet/modules

# Fix it
puppet-lint --fix /etc/puppet/modules



RSpec
-----
https://rspec-puppet.com/tutorial/

Use RSpec to test your Puppet modules

Example:

	require 'spec_helper'

	describe 'logrotate::rule' do
	  let(:title) { 'nginx' }

	  it { is_expected.to contain_class('logrotate::setup') }

	  it do
	    is_expected.to contain_file('/etc/logrotate.d/nginx').with({
	      'ensure' => 'present',
	      'owner'  => 'root',
	      'group'  => 'root',
	      'mode'   => '0444',
	    })
	  end

	  context 'with compress => true' do
	    let(:params) { {'compress' => true} }

	    it do
	      is_expected.to contain_file('/etc/logrotate.d/nginx') \
	        .with_content(/^\s*compress$/)
	    end
	  end

	  context 'with compress => false' do
	    let(:params) { {'compress' => false} }

	    it do
	      is_expected.to contain_file('/etc/logrotate.d/nginx') \
	        .with_content(/^\s*nocompress$/)
	    end
	  end

	  context 'with compress => foo' do
	    let(:params) { {'compress' => 'foo'} }

	    it do
	      expect {
	        is_expected.to contain_file('/etc/logrotate.d/nginx')
	      }.to raise_error(Puppet::Error, /compress must be true or false/)
	    end
	  end
	end



Bolt
----
https://puppet.com/docs/bolt/latest/bolt.html

Bolt is an open source orchestration tool that automates the manual work it takes to maintain your infrastructure. Bolt can be installed on your local workstation and connects directly to remote targets with SSH or WinRM, so you are not required to install any agent software.



Google Cloud Platform (GCP)
---------------------------
https://cloud.google.com/sdk/docs
Google Cloud Load Balancer 	https://cloud.google.com/load-balancing/docs/https/

--

# Installing the latest Cloud SDK version (334.0.0)
#
# https://cloud.google.com/sdk/docs/quickstart
# Download the Linux 64-bit archive file from your command-line
#
curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-334.0.0-linux-x86_64.tar.gz

# Extract the contents of the file to any location on your file system (preferably your Home directory).
#tar -xvf google-cloud-sdk-334.0.0-linux-x86_64.tar.gz
tar -xvf google-cloud-sdk-334.0.0-linux-x86_64.tar.gz

# Optional. Use the install script to add Cloud SDK tools to your PATH.
#
./google-cloud-sdk/install.sh

# Run gcloud init to initialize the SDK:
#
./google-cloud-sdk/bin/gcloud init

--

# Initialize or reinitialize gcloud; authenticates against cloud
gcloud init

# Create three server instances from instance template
gcloud compute instances create --source-instance-template=SOURCE_INSTANCE_TEMPLATE server1 server2 server3

gcloud compute instances create example-instance --source-instance-template example-instance \
    --machine-type e2-standard-2 --image-family debian-8 --image-project debian-cloud \
    --metadata bread=butter --disk=boot=no,name=my-override-disk

--

# Check project-wide quotas
gcloud compute project-info describe --project PROJECT_ID

# List quotas in a region, run:
gcloud compute regions describe REGION


Example GCP instances
---------------------

# Linux systemd service files location
/etc/systemd/system


sudo cp hello_cloud.py /usr/local/bin/
sudo cp hello_cloud.service /etc/systemd/system
sudo systemctl enable hello_cloud.service

gcloud compute instances create --zone us-west1-b --source-instance-template vm1-template vm2 vm3 vm4 vm5 vm6 vm7 vm8
gcloud compute instances list


hello_cloud.service:
    [Unit]
    After=network.target

    [Service]
    ExecStart=/usr/local/bin/hello_cloud.py 80

    [Install]
    WantedBy=default.target


hello_cloud.py:

    #!/usr/bin/env python3
    # Copyright 2019 Google LLC
    #

    """A simple Hello World type app which can serve on port 8000.
    Optionally, a different port can be passed.

    The code was inspired by:
    https://gist.github.com/davidbgk/b10113c3779b8388e96e6d0c44e03a74
    """
    import http
    import http.server
    import socket
    import socketserver
    import sys

    # TCP port for listening to connections, if no port is received
    DEFAULT_PORT=8000

    class Handler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            self.send_response(http.HTTPStatus.OK)
            self.end_headers()
            # Hello message
            self.wfile.write(b'Hello Cloud')
            # Now get the hostname and IP and print that as well.
            hostname = socket.gethostname()
            host_ip = socket.gethostbyname(hostname)
            self.wfile.write(
                '\n\nHostname: {} \nIP Address: {}'.format(
                    hostname, host_ip).encode())


    def main(argv):
        port = DEFAULT_PORT
        if len(argv) > 1:
            port = int(argv[1])

        web_server = socketserver.TCPServer(('', port), Handler)
        print("Listening for connections on port {}".format(port))
        web_server.serve_forever()


    if __name__ == "__main__":
        main(sys.argv)



GCP with Terraform
------------------
https://cloud.google.com/community/tutorials/getting-started-on-gcp-with-terraform

Create a Google Cloud project
	Saved the project ID
Create a service account
	Select JSON as the key type
Download the JSON file with the credentials
	Store file in a secure place

Set up Terraform
	Create a new directory for the project
	Create a main.tf file for the Terraform config

		// Configure the Google Cloud provider
		provider "google" {
		 credentials = file("CREDENTIALS_FILE.json")
		 project     = "project ID"
		 region      = "us-west1"
		}

	Run terraform init to download the latest version of the provider.
	This build the .terraform directory.

		terraform init

	Create a single Compute Engine instance running Debian.
	Add this google_compute_instance resource to the main.tf:


		// Terraform plugin for creating random ids
		resource "random_id" "instance_id" {
		 byte_length = 8
		}

		// A single Compute Engine instance
		resource "google_compute_instance" "default" {
		 name         = "flask-vm-${random_id.instance_id.hex}"
		 machine_type = "f1-micro"
		 zone         = "us-west1-a"

		 boot_disk {
		   initialize_params {
		     image = "debian-cloud/debian-9"
		   }
		 }

		// Make sure flask is installed on all new instances for later steps
		 metadata_startup_script = "sudo apt-get update; sudo apt-get install -yq build-essential python-pip rsync; pip install flask"

		 network_interface {
		   network = "default"

		   access_config {
		     // Include this section to give the VM an external ip address
		   }
		 }
		}

	The random_id Terraform plugin allows you to create a random instance name. This requires an additional plugin. Download and install the plugin by runing terraform init again:

		terraform init

	Validate the work done so far (syntax, existence credentials, preview of what will be created):

		terraform plan

	Create oogle Cloud resources by running terraform apply:

		terraform apply


	Check the Google Cloud VM Instances page.


	Add a public SSH key to the Compute Engine instance to access and manage it.

		Caution: Managing SSH keys in metadata is only for advanced users who are unable to use other tools such as OS Login to manually manage SSH keys

		resource "google_compute_instance" "default" {
		 ...
		metadata = {
		   ssh-keys = "INSERT_USERNAME:${file("~/.ssh/id_rsa.pub")}"
		 }
		}

	Validate your changes:
		terraform plan

	Apply your changes:
		terraform apply


	Use a Terraform output variable to retrieve the instance's IP address:

		// A variable for extracting the external IP address of the instance
		output "ip" {
		 value = google_compute_instance.default.network_interface.0.access_config.0.nat_ip
		}

	terraform plan
	terraform apply

	Note: The default network's default-allow-ssh firewall rule needs to be in place before you can use SSH to connect to the instance.


	SSH to the instance:

		ssh `terraform output ip`


Build  a Flask app for your Web server:
	Inside the VM instance, create file app.py:

		from flask import Flask
		app = Flask(__name__)

		@app.route('/')
		def hello_cloud():
		   return 'Hello Cloud!'

		app.run(host='0.0.0.0')

	Run the app:
		python app.py


Flask serves traffic on localhost:5000 by default.
Open port 5000 on the instance:

	resource "google_compute_firewall" "default" {
	 name    = "flask-app-firewall"
	 network = "default"

	 allow {
	   protocol = "tcp"
	   ports    = ["5000"]
	 }
	}

Test the Webserver:

	curl http://0.0.0.0:5000


Cleanup: Remove all the resources with terraform:

	terraform destroy


Misc
----

Caching:
- Varnish
- nginx
- Cloudfare
- Fastly
- Memcached (also DB caching)
- Redis (also DB caching)


geoDNS
geoIP

CI:
Jenkins
Travis CI (on Github)


Debugging Problems on the Cloud:
- https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-instances
- https://docs.microsoft.com/en-us/azure/virtual-machines/troubleshooting/
- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-troubleshoot.htm


--

HTTP response status codes:
- Informational responses (100–199)
- Successful responses (200–299)
- Redirects (300–399)
- Client errors (400–499)
- Server errors (500–599)


Check the status of the web server:
  sudo systemctl status apache2
  sudo journalctl -xe

Restart the service:
  sudo systemctl restart apache2

Start the service:
  sudo systemctl start apache2


Check which process is using a particular port:
  sudo netstat -nlp

Check which python3 program this is:
  ps -ax | grep python3

Kill the process:
  sudo kill <pid>


Check for any service with the keywords "python":
  sudo systemctl --type=service | grep python

Stop and disable a service:
  sudo systemctl stop <svc>
  sudo systemctl disable <svc>



SSH Access with PEM file
------------------------
download PEM key file
chmod 600 <keyfile>.pem
ssh -i <keyfile>.pem username@<IP Address>

